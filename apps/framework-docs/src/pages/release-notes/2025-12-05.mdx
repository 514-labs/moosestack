---
title: December 5, 2025
description: Release notes for December 5, 2025
---

import { Callout } from "@/components";

# December 5, 2025

Flexible Streaming Ingest, Web App Integration, and Infra Visibility Upgrades. This release pushes MooseStack forward on three fronts: flexible high-volume ingestion, developer experience for full-stack apps, and operational visibility. Moose gained the ability to handle arbitrary JSON ingest, Kafka-backed streaming, and powerful modeling tools like materialized columns and per-column codecs. On the app side, new Fastify and Next.js patterns make it easier to build type-safe Moose-backed web experiences. Meanwhile, Boreal added infra maps and storage visualizations so teams can see how their systems are wired and how their data grows.

<Callout type="info" title="Highlights">
* **New:** [Arbitrary JSON fields in ingest APIs](#arbitrary-json-fields-in-ingest-apis) for flexible schema evolution
* **New:** [Kafka engine support](#kafka-engine-support-for-streaming-ingestion) for real-time ClickHouse ingestion
* **New:** [Fastify web app template](#fastify-web-app-template) with automatic async initialization
* **New:** [Infrastructure map visualization](#infrastructure-map-visualization) in Boreal dashboard
</Callout>

---

## Arbitrary JSON fields in ingest APIs

Accept payloads with extra fields beyond your defined schema while still validating known fields. Extra fields automatically pass through to streaming functions and land in a JSON column.

**Why it matters:** Schema flexibility without chaos. Teams can ship events even when they don't fully agree on every property yet. Known fields still get enforceable validation, while "extra" properties land in a JSON column for later exploration. This is ideal for product analytics, logging, and gradual schema evolution.

```typescript filename="app/ingest/indexSignatureTests.ts" copy
import {
  IngestApi,
  Stream,
  OlapTable,
  Key,
  DateTime,
} from "@514labs/moose-lib";

// Input type with index signature - accepts arbitrary additional fields
export type UserEventInput = {
  timestamp: DateTime;
  eventName: string;
  userId: Key<string>;
  orgId?: string;
  // Index signature allows any additional properties
  [key: string]: any;
};

// Output type with fixed schema for OlapTable storage
export interface UserEventOutput {
  timestamp: DateTime;
  eventName: string;
  userId: Key<string>;
  orgId?: string;
  // JSON column stores extra fields from index signature
  properties: Record<string, any>;
}

// Create streams and tables
export const userEventInputStream = new Stream<UserEventInput>("UserEventInput");
export const userEventIngestApi = new IngestApi<UserEventInput>("userEventIngestApi", {
  destination: userEventInputStream,
});
export const userEventOutputTable = new OlapTable<UserEventOutput>("UserEventOutput", {
  orderByFields: ["userId", "timestamp"],
});
export const userEventOutputStream = new Stream<UserEventOutput>("UserEventOutput", {
  destination: userEventOutputTable,
});

// Transform receives ALL fields including extras from index signature
userEventInputStream.addTransform(
  userEventOutputStream,
  (input: UserEventInput): UserEventOutput => {
    const { timestamp, eventName, userId, orgId, ...extraFields } = input;
    return {
      timestamp,
      eventName,
      userId,
      orgId,
      properties: extraFields, // Store extra fields in JSON column
    };
  },
);
```

In Python, use `extra='allow'` in your Pydantic models to achieve the same flexibility.

PR: [#3047](https://github.com/514-labs/moosestack/pull/3047)

---

## Kafka engine support for streaming ingestion

Define tables with Kafka engine to consume streaming data directly from Kafka topics into ClickHouse, with materialized views to transform and route the data.

**Why it matters:** First-class streaming from Kafka. Kafka engine tables + materialized views mean you can go directly from Kafka → ClickHouse with Moose orchestrating the flow and transformations. This reduces glue code, keeps ingestion real-time, and standardizes how streaming pipelines are defined.

```typescript filename="app/ingest/kafkaTests.ts" copy
import {
  Stream,
  IngestApi,
  OlapTable,
  MaterializedView,
  ClickHouseEngines,
  Key,
  sql,
} from "@514labs/moose-lib";

// Define the event schema
export interface KafkaTestEvent {
  eventId: Key<string>;
  userId: string;
  eventType: string;
  amount: number;
  timestamp: number;
}

const KAFKA_TOPIC_NAME = "KafkaTestInput";

// Create a stream and ingest API to push data to Kafka
export const kafkaTestInputStream = new Stream<KafkaTestEvent>(KAFKA_TOPIC_NAME);
export const kafkaTestIngestApi = new IngestApi<KafkaTestEvent>("kafka-test", {
  destination: kafkaTestInputStream,
});

// Create a Kafka engine table that consumes from the topic
export const KafkaTestSourceTable = new OlapTable<KafkaTestEvent>(
  "KafkaTestSource",
  {
    engine: ClickHouseEngines.Kafka,
    brokerList: "redpanda:9092",
    topicList: KAFKA_TOPIC_NAME,
    groupName: "moose_kafka_consumer",
    format: "JSONEachRow",
    settings: {
      kafka_num_consumers: "1",
    },
  },
);

// Create a MaterializedView to transform and persist data
const kafkaSourceColumns = KafkaTestSourceTable.columns;

export const KafkaTestMV = new MaterializedView<KafkaTestEvent>({
  tableName: "KafkaTestDest",
  materializedViewName: "KafkaTestDest_MV",
  orderByFields: ["eventId", "timestamp"],
  selectStatement: sql`
    SELECT
      ${kafkaSourceColumns.eventId} as eventId,
      ${kafkaSourceColumns.userId} as userId,
      ${kafkaSourceColumns.eventType} as eventType,
      ${kafkaSourceColumns.amount} as amount,
      ${kafkaSourceColumns.timestamp} as timestamp
    FROM ${KafkaTestSourceTable}
  `,
  selectTables: [KafkaTestSourceTable],
});
```

PR: [#3066](https://github.com/514-labs/moosestack/pull/3066)

---

## Materialized columns in data models

Define materialized columns to automatically compute derived values at ingestion time—date extractions, hash functions, JSON transformations—without separate aggregations.

**Why it matters:** Cheaper, faster queries at scale. Materialized columns move expensive expressions into ingestion time so queries become simple scans over precomputed fields. This directly impacts infrastructure cost and query latency for time-series, metrics, and logs.

```typescript filename="app/ingest/models.ts" copy
import {
  IngestPipeline,
  Key,
  DateTime,
  UInt64,
  ClickHouseMaterialized,
  ClickHouseCodec,
} from "@514labs/moose-lib";
import typia from "typia";

// Use ClickHouseMaterialized<"expression"> intersection type
export interface MaterializedTest {
  id: Key<string>;
  timestamp: DateTime;
  userId: string;
  // Materialized column - computed automatically at ingestion time
  eventDate: string &
    typia.tags.Format<"date"> &
    ClickHouseMaterialized<"toDate(timestamp)">;
  // Hash for partitioning/anonymization
  userHash: UInt64 & ClickHouseMaterialized<"cityHash64(userId)">;
  // JSON blob with compression
  log_blob: Record<string, any> & ClickHouseCodec<"ZSTD(3)">;
  // Complex expression with both materialized and codec
  combinationHash: UInt64[] &
    ClickHouseMaterialized<"arrayMap(kv -> cityHash64(kv.1, kv.2), JSONExtractKeysAndValuesRaw(toString(log_blob)))"> &
    ClickHouseCodec<"ZSTD(1)">;
}

export const MaterializedTestPipeline = new IngestPipeline<MaterializedTest>(
  "MaterializedTest",
  { table: true, stream: true, ingestApi: true },
);
```

PR: [#3051](https://github.com/514-labs/moosestack/pull/3051)

---

## Per-column compression codecs

Specify compression codecs (ZSTD, LZ4, Delta, Gorilla) on individual columns to optimize storage and query performance for different data characteristics.

**Why it matters:** Per-column codecs let you tune storage/performance per field—Delta/Gorilla for time-series, higher ZSTD for logs. This directly impacts infrastructure cost and query latency.

```typescript filename="app/ingest/models.ts" copy
import {
  IngestPipeline,
  Key,
  DateTime,
  UInt64,
  ClickHouseCodec,
} from "@514labs/moose-lib";

// Use ClickHouseCodec<"codec"> intersection type to specify compression
export interface CodecTest {
  id: Key<string>;
  // Time-series data with Delta + LZ4 for optimal timestamp storage
  timestamp: DateTime & ClickHouseCodec<"Delta, LZ4">;
  // JSON logs with high compression for storage efficiency
  log_blob: Record<string, any> & ClickHouseCodec<"ZSTD(3)">;
  // Floating-point time series with Gorilla codec
  temperature: number & ClickHouseCodec<"Gorilla, ZSTD(3)">;
  // Counter metrics with DoubleDelta
  request_count: number & ClickHouseCodec<"DoubleDelta, LZ4">;
  // Status code without compression (frequently queried)
  status_code: number;
}

export const CodecTestPipeline = new IngestPipeline<CodecTest>("CodecTest", {
  table: true,
  stream: true,
  ingestApi: true,
});
```

PR: [#3035](https://github.com/514-labs/moosestack/pull/3035)

---

## Fastify web app template

New project template demonstrating Fastify framework with Moose, plus fixed WebApp support for Fastify's async initialization.

**Why it matters:** Fast path to production web APIs. The Fastify template + WebApp fix give a working, idiomatic example for building Moose-backed services. Fastify's performance and schema support pair nicely with Moose's data models, lowering the barrier for new services.

```typescript filename="app/apis/bar.ts" copy
import Fastify from "fastify";
import { WebApp, getMooseClients, sql } from "@514labs/moose-lib";

const app = Fastify({ logger: true });

app.get("/health", async (_request, _reply) => {
  return {
    status: "ok",
    timestamp: new Date().toISOString(),
    service: "analytics-api",
  };
});

app.get("/query", async (request, reply) => {
  const { client } = await getMooseClients();
  const limit = parseInt((request.query as any).limit as string) || 10;

  const result = await client.query.execute(sql`
    SELECT * FROM analytics_table LIMIT ${limit}
  `);
  return result;
});

// Export as WebApp - Moose handles Fastify's ready() automatically
export const analyticsApp = new WebApp("analyticsApi", app, {
  mountPath: "/analytics",
  metadata: { description: "Analytics API with Fastify" },
});
```

PR: [#3068](https://github.com/514-labs/moosestack/pull/3068), [#3061](https://github.com/514-labs/moosestack/pull/3061) | Docs: [Fastify integration](/moose/app-api-frameworks/typescript-fastify)

---

## Next.js client-only mode

Enable `MOOSE_CLIENT_ONLY` environment variable so Next.js apps can import Moose data models without running the full runtime or fighting HMR errors.

**Why it matters:** Frontend type safety without runtime headaches. This solves "already exists" errors during Hot Module Replacement and supports a pattern where the frontend gets type-safe definitions while the heavy Moose runtime stays server-side where it belongs.

```javascript filename="next.config.js" copy
/** @type {import('next').NextConfig} */
const nextConfig = {
  env: {
    MOOSE_CLIENT_ONLY: 'true', // Prevents duplicate registration errors during HMR
  },
}

module.exports = nextConfig
```

```typescript filename="app/models/UserActivity.ts" copy
import { OlapTable, Key, DateTime } from '@514labs/moose-lib'

export interface UserActivity {
  userId: Key<string>
  action: string
  timestamp: DateTime
}

// Safe to import in Next.js - duplicates silently overwrite instead of throwing
export const UserActivityTable = new OlapTable<UserActivity>('UserActivity', {
  orderByFields: ['timestamp', 'userId'],
})
```

PR: [#3057](https://github.com/514-labs/moosestack/pull/3057)

---

## Web apps in `moose ls`

The `moose ls` command now displays web applications alongside tables, streams, and APIs, making them first-class infrastructure citizens.

**Why it matters:** Web apps as first-class infra. Discover what UIs exist (admin dashboards, portals, analytics UI) and integrate this into tooling, scripts, or dashboards.

```bash filename="Terminal" copy
# List all project components including web apps
moose ls

# Filter to show only web applications
moose ls --type web_apps

# View web apps in JSON format for programmatic use
moose ls --type web_apps --json
```

PR: [#3054](https://github.com/514-labs/moosestack/pull/3054)

---

## Other Moose improvements

- **Lifecycle inheritance in IngestPipeline** – Top-level lifecycle settings automatically propagate to table, stream, and deadLetterQueue components, reducing config duplication. PR [#3088](https://github.com/514-labs/moosestack/pull/3088)
- **MCP query result compression** – Results compressed using toon format for better IDE/AI integration performance. PR [#3033](https://github.com/514-labs/moosestack/pull/3033)
- **API key auth in MCP template** – Optional PBKDF2-based authentication for secure MCP server deployments. PR [#3053](https://github.com/514-labs/moosestack/pull/3053)
- **No-ANSI output option** – Disable colored CLI output for CI/CD systems and log files. PR [#3044](https://github.com/514-labs/moosestack/pull/3044)
- **Faster CLI logging** – Optimized tracing implementation reduces memory allocations. PR [#3040](https://github.com/514-labs/moosestack/pull/3040), [#3041](https://github.com/514-labs/moosestack/pull/3041)
- **Documentation updates** – Improved quickstart, ClickHouse migration guide, and simplified installation command. PR [#3079](https://github.com/514-labs/moosestack/pull/3079), [#3073](https://github.com/514-labs/moosestack/pull/3073)
- **Hash-token output cleanup** – Cleaner, more concise `moose generate hash-token` output. PR [#3060](https://github.com/514-labs/moosestack/pull/3060)

## Moose bug fixes

- Next.js security updates in templates (15.4.7 → 16.0.7, React 19.0.0 → 19.0.1) – [#3089](https://github.com/514-labs/moosestack/pull/3089)
- MCP template .npmrc fixes for Docker builds and npm install – [#3084](https://github.com/514-labs/moosestack/pull/3084), [#3082](https://github.com/514-labs/moosestack/pull/3082), [#3081](https://github.com/514-labs/moosestack/pull/3081)
- MCP SDK compilation errors and Zod-based runtime validation – [#3075](https://github.com/514-labs/moosestack/pull/3075)
- MCP template auth setup instructions clarified – [#3058](https://github.com/514-labs/moosestack/pull/3058)
- ORDER BY extraction when tables have projections – [#3052](https://github.com/514-labs/moosestack/pull/3052)

---

## Infrastructure map visualization

View your deployed MooseStack architecture directly in the Boreal dashboard—services, data flows, and components in one place.

**Why it matters:** See your architecture, don't guess it. The infrastructure map helps with onboarding new teammates, debugging mis-wired components, and explaining the system to stakeholders.

PR: [#1176](https://github.com/514-labs/commercial/pull/1176)

---

## Database storage visualization (experimental)

New experimental page showing table storage usage over time with interactive charts.

**Why it matters:** Watch storage growth over time. Time-series charts of table sizes are crucial for capacity planning, catching runaway growth, and understanding which workloads drive storage cost.

PR: [#1125](https://github.com/514-labs/commercial/pull/1125)

---

## Boreal bug fixes

- **GitHub authentication** – Fixed issues preventing repository connections and operations. PR [#1136](https://github.com/514-labs/commercial/pull/1136)
- **Security updates** – React 19.2.0 → 19.2.1, Next.js 16.0.1 → 16.0.7. PR [#1130](https://github.com/514-labs/commercial/pull/1130)
- **Function names display** – Fixed missing function names in metrics table. PR [#1126](https://github.com/514-labs/commercial/pull/1126)
