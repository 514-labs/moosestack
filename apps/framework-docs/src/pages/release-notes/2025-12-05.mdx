---
title: December 5, 2025
description: Release notes for December 5, 2025
---

import { Callout } from "@/components";

# December 5, 2025

<Callout type="info" title="Highlights">
* **New:** Enable arbitrary JSON fields in ingest APIs
* **New:** Infrastructure map visualization for deployed applications
</Callout>



## ü¶å Moose

### New Features
- **Enable arbitrary JSON fields in ingest APIs**
  Ingest APIs now accept payloads with extra fields beyond the defined data model schema. In TypeScript, use index signatures to allow arbitrary fields. In Python, use extra='allow' in your Pydantic models. Extra fields are automatically passed through to streaming functions and stored in a JSON column, enabling flexible data ingestion while maintaining schema validation for known fields.

  **TypeScript Index Signatures for Flexible Data Ingestion**
  ```typescript
  // datamodels/UserEvent.ts
  export interface UserEvent {
    timestamp: string;
    eventName: string;
    userId: string;
    orgId?: string;
    // Index signature allows arbitrary extra fields
    [key: string]: any;
  }
  
  // streaming/userEventProcessor.ts
  import { UserEvent } from "../datamodels/UserEvent";
  
  export default function processUserEvent(event: UserEvent) {
    // Extract known fields
    const { timestamp, eventName, userId, orgId, ...extraFields } = event;
    
    // Extra fields are automatically available and can be processed
    const customProperties = {
      pageUrl: extraFields.pageUrl,
      sessionDuration: extraFields.sessionDuration,
      customMetrics: extraFields.customMetrics,
      // Any other arbitrary fields sent in the payload
      ...extraFields
    };
  
    return {
      timestamp,
      eventName,
      userId,
      orgId,
      // Store extra fields in a JSON column for flexible querying
      properties: customProperties
    };
  }
  ```
  
  This example shows how to use index signatures in TypeScript data models to accept arbitrary JSON fields in ingest APIs. The streaming function receives all fields and can store extra properties in a JSON column for flexible data ingestion.

- **Add Kafka engine support for streaming data ingestion**
  Users can now define tables with Kafka engine to consume streaming data directly from Kafka topics. This enables real-time data ingestion from Kafka brokers into ClickHouse tables, with support for materialized views to transform and route the data. Includes credential management and configuration options for Kafka connections.

  **Kafka Streaming Data Pipeline with Real-time Transformation**
  ```typescript
  // datamodels/KafkaEvents.ts
  export interface KafkaTestSource {
    eventId: string;
    userId: string;
    eventType: string;
    amount: number;
    timestamp: number;
  }
  
  // datamodels/ProcessedEvents.ts  
  export interface KafkaTestDest {
    eventId: string;
    userId: string;
    eventType: string;
    amount: number;
    timestamp: number;
    processedAt: number;
  }
  
  // streaming/kafka-consumer.ts
  import { Streaming } from "@514labs/moose-lib";
  
  export default Streaming.fromKafka<KafkaTestSource, KafkaTestDest>({
    // Source table with Kafka engine - consumes from Kafka topic
    source: {
      table: "KafkaTestSource",
      engine: {
        type: "Kafka",
        broker: "localhost:9092",
        topic: "user-events",
        group_id: "moose-consumer-group",
        format: "JSONEachRow"
      },
      // Kafka credentials via table settings
      settings: {
        kafka_username: "MOOSE_ENV::KAFKA_USERNAME",
        kafka_password: "MOOSE_ENV::KAFKA_PASSWORD"
      }
    },
    
    // Destination table with MergeTree engine for analytics
    destination: "KafkaTestDest",
    
    // Transform streaming data as it flows from Kafka to ClickHouse
    select: `
      SELECT 
        eventId,
        userId,
        eventType,
        amount,
        timestamp,
        now() as processedAt
      FROM KafkaTestSource
      WHERE amount > 0
    `
  });
  ```
  
  This example shows how to set up real-time data ingestion from Kafka topics using the new Kafka engine support. It defines source and destination data models, configures a Kafka consumer table with credentials, and creates a materialized view to transform streaming data as it flows into ClickHouse.

- **Add TypeScript Fastify template for web applications**
  Added a new project template that demonstrates how to build web applications using Fastify framework with Moose. The template includes example APIs, data models, views, and workflows, making it easier for developers to get started with Fastify-based web apps in their Moose projects. Also improved the WebApp integration to handle Fastify's async initialization automatically.

  **Fastify WebApp with automatic async initialization**
  ```typescript
  import Fastify from "fastify";
  import { WebApp } from "@514labs/moose-lib";
  
  // Create a Fastify app for handling user analytics data
  const app = Fastify({ logger: true });
  
  // Health check endpoint
  app.get("/health", async (request, reply) => {
    return { status: "healthy", service: "user-analytics-api" };
  });
  
  // Query endpoint with schema validation
  app.get<{
    Querystring: { limit?: number; userId?: string };
  }>("/users", {
    schema: {
      querystring: {
        type: "object",
        properties: {
          limit: { type: "integer", minimum: 1, maximum: 100 },
          userId: { type: "string" }
        }
      }
    }
  }, async (request, reply) => {
    const { limit = 10, userId } = request.query;
    
    // Query your Moose data models here
    const users = await getUserAnalytics({ limit, userId });
    
    return { users, count: users.length };
  });
  
  // POST endpoint for complex queries
  app.post<{
    Body: { dateRange: { start: string; end: string }; metrics: string[] };
  }>("/analytics", async (request, reply) => {
    const { dateRange, metrics } = request.body;
    
    // Process analytics request using Moose utilities
    const results = await processAnalytics(dateRange, metrics);
    
    return { success: true, data: results };
  });
  
  // Export as WebApp - Moose handles Fastify's async initialization automatically
  export const userAnalyticsApp = new WebApp("userAnalytics", app, {
    mountPath: "/analytics",
    metadata: { description: "User Analytics API with Fastify" }
  });
  ```
  
  Shows how to create a Fastify-based web application in Moose with schema validation, multiple endpoints, and automatic async initialization. The WebApp integration now handles Fastify's ready() method automatically, eliminating the need for manual setup.

- **Add client-only mode for Next.js integration with OlapTable, View, and MaterializedView**
  Introduces MOOSE_CLIENT_ONLY environment variable that enables Next.js applications to import Moose data model definitions for type-safe queries without running the full Moose runtime. This prevents "already exists" errors during Hot Module Replacement (HMR) by allowing duplicate resource registrations to silently overwrite instead of throwing errors. Particularly useful for frontend applications that need type definitions but don't need the full data pipeline functionality.

  **Next.js integration with client-only mode**
  ```typescript
  // next.config.js - Enable client-only mode for Next.js
  /** @type {import('next').NextConfig} */
  const nextConfig = {
    env: {
      MOOSE_CLIENT_ONLY: 'true', // Prevents duplicate registration errors during HMR
    },
  }
  
  module.exports = nextConfig
  
  // app/models/UserActivity.ts - Define your data models
  import { OlapTable } from '@514labs/moose-lib'
  
  export interface UserActivity {
    userId: string
    action: string
    timestamp: Date
    metadata?: Record<string, any>
  }
  
  // This table definition can be safely imported in Next.js
  // without running the full Moose runtime
  export const UserActivityTable = new OlapTable<UserActivity>('UserActivity', {
    orderByFields: ['timestamp', 'userId'],
    primaryKey: ['userId', 'timestamp'],
  })
  
  // app/api/analytics/route.ts - Use type-safe queries in your API routes
  import { UserActivityTable } from '../../models/UserActivity'
  import { NextResponse } from 'next/server'
  
  export async function GET() {
    // Get type-safe query builder from the table definition
    const query = UserActivityTable
      .select(['userId', 'action', 'timestamp'])
      .where('timestamp > ?', [new Date(Date.now() - 24 * 60 * 60 * 1000)])
      .orderBy('timestamp', 'DESC')
      .limit(100)
  
    // Execute query against your ClickHouse instance
    const results = await query.execute()
    
    return NextResponse.json(results)
  }
  ```
  
  Shows how to enable MOOSE_CLIENT_ONLY mode in a Next.js application to import Moose data model definitions for type-safe queries without running the full Moose runtime. This prevents "already exists" errors during Hot Module Replacement while still providing type safety for frontend queries.

- **Add web_apps support to `moose ls` command**
  The `moose ls` command now displays web applications in your project. You can filter specifically for web apps using `--type web_apps` or see them in the complete project listing. This makes it easier to discover and manage web applications alongside other Moose infrastructure components like tables, streams, and APIs.

  **Using moose ls to discover web applications**
  ```bash
  # List all project components including web apps
  moose ls
  
  # Filter to show only web applications
  moose ls --type web_apps
  
  # Search for specific web apps by name
  moose ls --name dashboard
  
  # View web apps in JSON format for programmatic use
  moose ls --type web_apps --json
  
  # Example output when listing web apps:
  # Web Apps
  # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  # ‚îÇ name            ‚îÇ mount_path       ‚îÇ
  # ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  # ‚îÇ admin-dashboard ‚îÇ /admin           ‚îÇ
  # ‚îÇ user-portal     ‚îÇ /portal          ‚îÇ
  # ‚îÇ analytics-ui    ‚îÇ /analytics       ‚îÇ
  # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ```
  
  This example shows how to use the enhanced `moose ls` command to discover and manage web applications in your MooseStack project. You can view all components together or filter specifically for web apps to see their names and mount paths.

- **Add materialized column support for data models**
  Users can now define materialized columns in their data models to automatically compute derived values at ingestion time. This enables efficient pre-computation of expressions like date extractions, hash functions, and complex JSON transformations without requiring separate aggregations. Materialized columns are computed once during data ingestion and stored physically, improving query performance for frequently accessed derived data.

  **Materialized columns for efficient data transformations**
  ```typescript
  // models/UserActivity.ts
  export interface UserActivity {
    id: string;
    timestamp: Date;
    userId: string;
    eventType: string;
    metadata: Record<string, any>;
    
    // Materialized columns - computed automatically at ingestion time
    eventDate: Date; // @materialized("toDate(timestamp)")
    userHash: number; // @materialized("cityHash64(userId)")
    eventMonth: string; // @materialized("formatDateTime(timestamp, '%Y-%m')")
    metadataKeys: string[]; // @materialized("JSONExtractKeys(toString(metadata))")
  }
  
  // The framework automatically generates these computed columns:
  // - eventDate: Extracts just the date portion for efficient date-based queries
  // - userHash: Creates a hash of userId for anonymization and partitioning
  // - eventMonth: Pre-computes month strings for time-series aggregations
  // - metadataKeys: Extracts JSON keys for metadata analysis
  
  // Usage in aggregations - materialized columns improve query performance
  export default {
    select: `
      eventDate,
      eventMonth,
      COUNT(*) as daily_events,
      COUNT(DISTINCT userHash) as unique_users
    `,
    orderBy: 'eventDate'
  }
  ```
  
  This example shows how to define materialized columns in a MooseStack data model using TypeScript decorators. The materialized columns are automatically computed during data ingestion, storing derived values like date extractions, hash functions, and JSON transformations for improved query performance.

- **Add no_ansi configuration option to disable colored terminal output**
  Added a new configuration option to disable ANSI color codes and formatting in CLI output. This is useful for environments that don't support colored output, CI/CD systems, or when redirecting output to files. When enabled, all terminal styling (colors, bold text) is stripped while preserving the same information and formatting structure.

- **Add compression support for MCP query results**
  MCP Server now compresses query results using the toon format, improving performance when working with large datasets in IDE integrations. This reduces memory usage and speeds up data transfer between the MCP server and AI assistants when querying OLAP databases.

- **Add API key authentication to TypeScript MCP template**
  The TypeScript MCP template now supports optional API key authentication using PBKDF2. Users can generate API key pairs using the Moose CLI and configure both server and client authentication. This provides a security layer for MCP server deployments while maintaining backward compatibility with development mode.

- **Add per-column compression codec support for data models**
  Users can now specify compression codecs (like ZSTD, LZ4, Delta, Gorilla) on individual columns in their data models to optimize storage and query performance. This allows fine-grained control over how different data types are compressed in ClickHouse tables, enabling better performance for time-series data, logs, and other specialized use cases.

  **Per-column compression codecs in data models**
  ```typescript
  // datamodels/MetricsData.ts
  import { Key } from "@514labs/moose-lib";
  
  export interface MetricsData {
    @Key
    id: string;
    
    // Time-series data with Delta + LZ4 compression for optimal timestamp storage
    timestamp: Date; // codec: "Delta, LZ4"
    
    // Temperature sensor data with Gorilla codec for floating-point time series
    temperature: number; // codec: "Gorilla, ZSTD(1)"
    
    // Request counts with DoubleDelta for counter-like metrics
    request_count: number; // codec: "DoubleDelta, LZ4"
    
    // JSON logs with high compression for storage efficiency
    log_data: any; // codec: "ZSTD(3)"
    
    // User agent strings with medium compression
    user_agent: string; // codec: "ZSTD(2)"
    
    // Tag arrays with basic LZ4 compression
    tags: string[]; // codec: "LZ4"
    
    // Hash arrays with light compression
    event_hashes: number[]; // codec: "ZSTD(1)"
    
    // Status code without compression (frequently queried)
    status_code: number;
  }
  ```
  
  This example shows how to specify compression codecs on individual columns in a MooseStack data model. Different codecs are chosen based on data characteristics: Delta for timestamps, Gorilla for floating-point time series, DoubleDelta for counters, and various ZSTD levels for different compression needs.

### Improvements
- Improved lifecycle configuration inheritance in IngestPipeline: IngestPipeline now automatically propagates top-level lifecycle settings to table, stream, and deadLetterQueue components when they don't specify their own lifecycle configuration. This simplifies configuration by allowing users to set lifecycle once at the pipeline level instead of repeating it for each component, while still supporting component-specific overrides when needed.
- Improved quickstart and ClickHouse migration documentation: Enhanced the getting started documentation with clearer instructions, better formatting, and improved troubleshooting sections. Updated the ClickHouse migration guide with more detailed explanations of CDC table handling and streamlined the development workflow steps.
- Simplified installation command in documentation: Updated the installation command in the documentation to only install moose, removing the sloan component. This simplifies the getting started experience for new users.
- Improved hash-token command output formatting: The `moose generate hash-token` command now displays cleaner, more concise output. Removed redundant text and simplified the formatting to make the generated tokens easier to read and use.
- Improved CLI logging performance through optimized tracing implementation: Optimized the internal logging system to reduce memory allocations and improve performance. Users should experience faster CLI command execution, especially for commands that generate significant log output.

### Bug Fixes
- Update Next.js to fix security vulnerabilities in frontend templates: Updated Next.js from version 15.4.7 to 16.0.7 and React from 19.0.0 to 19.0.1 in frontend templates to address security vulnerabilities. This ensures new projects created with Moose templates use secure, up-to-date dependencies.
- Fix TypeScript MCP template by removing empty .npmrc file: Removed an empty .npmrc file from the TypeScript MCP template that was causing issues. This ensures the template works correctly when users create new MCP server projects.
- Fix Docker builds failing when .npmrc file is missing: Fixed an issue where Docker packaging would fail if the .npmrc file was not present in the project. The build process now handles missing .npmrc files gracefully, preventing build failures for projects that don't require custom npm configuration.
- Fix TypeScript MCP template missing .npmrc file: Fixed an issue where the TypeScript MCP template was missing a .npmrc configuration file, which could cause npm package installation issues when creating new MCP projects.
- Fix MCP SDK compilation errors and improve type safety: Fixed compilation errors in the TypeScript MCP template that were preventing successful builds. Updated MCP SDK integration to use the latest API patterns and added proper runtime validation with Zod schemas for ClickHouse query results. This ensures the MCP server template works correctly for IDE integrations and AI assistant interactions.
- Fix authentication setup instructions in TypeScript MCP template: Improved the setup instructions for the TypeScript MCP template to make authentication configuration clearer and more streamlined. The updated instructions now provide step-by-step guidance for generating API keys, copying environment files, and setting up authentication tokens correctly. This fixes confusion around the authentication setup process that users were experiencing when initializing new MCP projects.
- Fix Fastify framework support in WebApp: Fixed an issue where Fastify applications weren't properly supported in WebApp. The routing property is now correctly handled as a function rather than an object with a handle method, allowing Fastify apps to work seamlessly with Moose's WebApp functionality.
- Fix ORDER BY extraction for tables with projections: Fixed a bug where ORDER BY clauses were incorrectly extracted from ClickHouse CREATE TABLE statements when the table contained projections with their own ORDER BY clauses. This ensures proper table schema parsing and prevents potential data pipeline issues.

## üêª Boreal

### New Features
- **Infrastructure map visualization for deployed applications**
  Added infrastructure map generation and visualization that shows the architecture and components of deployed MooseStack applications. Users can now view detailed infrastructure diagrams of their deployments directly in the Boreal dashboard, providing better visibility into their application structure and data flow.
  
  üì∏ *[Screenshot needed: Added infrastructure map generation and visualization that shows the architecture and components of deployed MooseStack applications. Users can now view detailed infrastructure diagrams of their deployments directly in the Boreal dashboard, providing better visibility into their application structure and data flow.]*

- **Add experimental database storage visualization page**
  Added a new experimental database page that shows table storage usage over time with interactive charts. Users can now visualize how their database storage grows and changes, helping them monitor and optimize their data usage patterns. This feature is currently behind an experimental flag.
  
  üì∏ *[Screenshot needed: Added a new experimental database page that shows table storage usage over time with interactive charts. Users can now visualize how their database storage grows and changes, helping them monitor and optimize their data usage patterns. This feature is currently behind an experimental flag.]*

### Bug Fixes
- Fixed GitHub authentication issues preventing repository operations: Resolved authentication problems that could prevent users from connecting to GitHub repositories, creating new repositories, or managing GitHub integrations. Users should now experience more reliable GitHub connectivity and clearer error messages when reconnection is needed.
- Updated React and Next.js versions to address security vulnerability: Updated React from 19.2.0 to 19.2.1 and Next.js from 16.0.1 to 16.0.7 to fix a security vulnerability. This ensures the platform remains secure and up-to-date with the latest patches.
- Fixed missing function names in functions page table: Fixed an issue where the functions page table was displaying "Route" instead of "Function Name" in the column header and wasn't showing the actual function names. Users can now properly see function names in the metrics table.


