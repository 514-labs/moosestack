---
title: "Migrating to ClickHouse with AI (without shipping AI slop)"
description: "A practical narrative on fast feedback loops, everything-as-code, and opinionated patterns for OLAP migrations"
previewVariant: "dashboards"
languages: ["typescript"]
---

import { Callout, BulletPointsCard, CTACards, CTACard } from "@/components/mdx";

# Migrating to ClickHouse with AI (without shipping AI slop)

<Callout type="info" title="TL;DR">
* CDC is mostly solved. The hard part is making OLAP useful in your product.
* AI helps a lot with SQL translation, but it falls apart on modeling, MVs, and end-to-end integration.
* You can fix that with three things: fast feedback loops, everything-as-code, and opinionated patterns.
</Callout>

## Why AI stalls after CDC

Migrating to ClickHouse can be intimidating, but not because CDC is hard. CDC is pretty solved: Postgres to ClickHouse pipelines are a known path, with robust tooling (ClickPipes, Redpanda, Debezium, etc.).

The real problem shows up one layer later: you now have OLAP tables in ClickHouse, and you need to build your product on top of them.

That work looks like this:

1. Map your OLTP queries to OLAP queries
2. Model data for analytics, not transactions
3. Create and optimize materialized views
4. Update and test APIs
5. Validate the end-to-end frontend experience

Generic copilots are decent at Step 1. The rest is where they drift. You get SQL that “works,” but the system isn’t coherent: schemas drift, views break, APIs mismatch, and the frontend silently goes stale. That’s how you end up with AI slop.

So the question isn’t “can AI write SQL?” It’s: **can you structure the environment so AI can iterate toward correctness?**

This post is a practical framework for that. It’s not a tool comparison. It’s a pattern for building and iterating on OLAP migrations with AI, without sacrificing correctness.

## Act 1: Fast feedback loops and safe experiments

Boyd’s Law applies here: speed of iteration beats quality of iteration.

That’s especially true for OLTP to OLAP migrations. They’re multi-step and require coordination across multiple codependent layers of your stack: CDC landing tables, materialized views/serving tables, API queries, and dashboard UI components. If you try to tackle all of these layers in one shot, it's hard to guarantee correctness. An error in one layer can cause cascading failures in others, and you end up debugging a whole graph of changes at once.

The reliable alternative is to shrink the surface area of each change and validate it immediately. That only works if experimentation is cheap and data is realistic. If a change breaks, fixing it should be as simple as undoing it and trying again. If you have to wait for a deployment and backfill, the loop is too slow to matter.

The best version of this is local, hot-reloading infrastructure with real sample data.

In MooseStack, that starts with `moose dev`. This command spins up ClickHouse locally with your table schemas and applies schema-change DDL as you save.

```shell
moose dev
```

Hot reload is useless if you can’t track progress and know when a change is applied. The `moose dev` server streams status to your terminal so you can time tests as soon as a change completes. When a change fails, you need runtime errors in your terminal and the same logs surfaced to AI agents so they can learn from failures and retry. With `moose dev`, you get both via logging and an embedded MCP server.

Logs are only as good as the data behind them. Without realistic data, tests “pass” on toy datasets and miss the edge cases you’ll hit in production.
MooseStack is designed to let you mimic your production environment locally, not just with your table schemas, but with your production data as well. You can seed your local dev database with a simple `moose seed clickhouse` command, so you can validate your OLAP queries and transformations against the same data powering your existing production dashboards. You can also run queries against your local database to validate your changes immediately after they're applied with the `moose query` command.

When the development feedback loop is this tight and cheap, you contain risk. Mistakes are small, visible and easy to fix before they cascade.

## Act 2: Everything as code

The hard part of migrating dashboards to ClickHouse isn’t translating Postgres SQL to ClickHouse SQL. It’s translating OLTP query patterns into OLAP models and integrating them safely across your codebase. That requires keeping your whole stack coherent as it changes, so AI can iterate across models, APIs, and UI components. 

You get that by unifying all of these layers in one codebase, in one language, with shared objects and types.

Then the OLTP→OLAP modeling changes you make (new columns, rollups, serving tables) ripple through the same compiler and editor, showing a type error before you can even run your code. That’s a big upgrade from the common OLAP migration setup: raw DDL in a `/migrations` folder, raw SQL strings in API handlers, and a schema that only exists inside the database. Schema drift hides in that gap. The dependency graph lives in people’s heads and ad‑hoc DDL history, so you can change a source table and not discover it broke an API query until dashboards are already stale.

When the dependency graph lives in code, breaking changes show up where and when you introduced them, not hours later in production.

With MooseStack, all your ClickHouse tables and Materialized Views are declared in code. Materialized Views are the core OLAP pattern you adopt when you translate OLTP queries into OLAP models: pre‑aggregations, serving tables, and rollups that make dashboards fast. Materialized Views reference the source tables they read from and the destination table they write to. That makes the dependency chain explicit and setup and teardown deterministic. When you change a source or destination schema during an OLTP→OLAP rewrite, the correct order of drops, creates, and backfills is derived from the code every time. The AI doesn’t have to guess dependency order; the code defines it.

```ts filename="BasicUsage.ts" copy
import { MaterializedView, OlapTable, ClickHouseEngines, sql } from "@514labs/moose-lib";
import { sourceTable } from "path/to/SourceTable"; // or a view

// Define the schema of the transformed rows-- this is static and it must match the results of your SELECT. It also represents the schema of your entire destination table.
interface TargetSchema {
  id: string;
  average_rating: number;
  num_reviews: number;
}

export const targetTable = new OlapTable<TargetSchema>("target_table", {
  orderByFields: ["id"],
});

export const mvToTargetTable = new MaterializedView<TargetSchema>({
  // The transformation to run on the source table
  selectStatement: sql`
  SELECT
    ${sourceTable.columns.id},
    avg(${sourceTable.columns.rating}) AS average_rating, 
    count(*) AS num_reviews 
    FROM ${sourceTable}
    GROUP BY ${sourceTable.columns.id}
  `,
  // Reference to the source table(s) that the SELECT reads from
  selectTables: [sourceTable], 

  // Creates a new OlapTable named "target_table" where the transformed rows are written to.
  targetTable: targetTable,
  // The name of the materialized view in ClickHouse
  materializedViewName: "mv_to_target_table", 
});
```

Moose Migrate ensures that lifecycle is safe in dev. Whenever you or your AI copilot tweak a Materialized View query, all DDL and backfills are ordered based on the code.

That same “everything is code” model carries into your API and UI. In the `nextjs-moose` reference app, your ClickHouse schema and queries live in a shared `moose/` workspace package, and your Next.js app imports typed helpers directly. If a schema change breaks a query or UI, TypeScript tells you (and your AI copilot) as you type, not after you ship. That’s the compiler enforcing the same dependency graph across the UI and the database.

```ts title="moose/src/models.ts"
import { OlapTable } from "@514labs/moose-lib";

export interface EventModel {
  id: string;
  amount: number;
  event_time: Date;
  status: 'completed' | 'active' | 'inactive';
}

export const Events = new OlapTable<EventModel>("events", {
  orderByFields: ["event_time"],
});
```

```ts title="moose/src/queries.ts"
import { sql } from "@514labs/moose-lib";
import { Events, EventModel } from "./models";
import { executeQuery } from "./client";

export async function getEvents(limit: number = 10): Promise<EventModel[]> {
  return await executeQuery<EventModel>(
    sql`SELECT * FROM ${Events} ORDER BY ${Events.columns.event_time} DESC LIMIT ${limit}`,
  );
}
```

```ts title="app/analytics/page.tsx"
import { getEvents } from "moose";

export default async function AnalyticsPage() {
  const events = await getEvents(10);

  return (
    <div>
      <h1>Recent Events</h1>
      <ul>
        {events.map((event) => (
          <li key={event.id}>
            {event.id}: {event.amount} ({event.status})
          </li>
        ))}
      </ul>
    </div>
  );
}
```

This does a few things that make AI development viable:

- The schema and the view are explicitly connected in code
- Dependency ordering is handled by the migration planner
- Backfills and hot‑reloads happen automatically in dev
- Your API and UI can import typed outputs instead of guessing at columns

When all of that is true, the LLM’s job narrows to what it does best: translating logic and iterating on query shapes. The framework handles the operational correctness.

## Act 3: Opinions, expertise, and patterns

ClickHouse is not Postgres. OLAP is not OLTP. The modeling choices that look like minor details in generic SQL—types, engines, sorting keys—are the core performance levers in ClickHouse. Get them wrong and you don’t just slow down queries; you miss the whole reason you migrated.

That’s why AI needs explicit opinions and patterns here. Without them, it will default to “works in SQL” habits that ignore ClickHouse’s cost model. You can ship code that runs and still end up with slow, expensive queries and a migration that never pays off.

So give the LLM context and opinions:

- A language server that validates ClickHouse SQL as you type
- A metrics layer or modeling guide that defines “how we do it here”
- A local runtime that surfaces errors when the model drifts

This doesn’t make AI “smart.” It makes the system **coherent**. And coherence is what makes migration work scale.

## What this looks like in practice

If you want a reference implementation, the `nextjs-moose` demo app is a clean baseline. It shows a Next.js app and a sibling `moose/` package living in the same repo. That’s the minimal version of “everything as code,” and it’s the easiest environment for an LLM to help you change and test things quickly.

## Closing: From local to production

A local, code‑first workflow doesn’t replace production deployment. It just makes it safe to iterate quickly before you ship.

Once the migration is correct, you still need the boring, essential pieces: CI/CD, migration plans, environment management, and a reliable deployment path. Fiveonefour’s managed stack exists for that reason: it’s a direct path from the local loop to production ClickHouse without re‑architecting your workflow.

If you want AI to help on OLAP migrations, don’t start with prompt engineering. Start with the environment. Give it fast feedback, give it code, and give it opinionated patterns. That’s the real leverage.
