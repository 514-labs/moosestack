---
title: Improving the Performance of Your Dashboards
description: Build fast dashboards using MooseStack and ClickHouse
previewVariant: "dashboards"
languages: ["typescript"]
---

import {
  CTACards,
  CTACard,
  Callout,
  BulletPointsCard,
  ToggleBlock,
  CustomizePanel,
  CustomizeGrid,
  SelectField,
  ConditionalContent,
  CommunityCallout,
} from "@/components/mdx";

# Improving the Performance of Your Dashboards

<Callout type="info" title="TL;DR">
* Slow dashboards are usually a database architecture problem, not a frontend or charting library problem.
* Running analytical queries on your transactional database degrades your entire application's performance and reliability.
* Moving these analytical queries to ClickHouse yields 10–100x faster dashboards

This guide will:

1. Help you recognize when OLTP has become your analytics bottleneck
2. Show an incremental path to move analytical workloads to OLAP
3. Walk through shipping the change end-to-end with MooseStack + ClickHouse
</Callout>

<CTACards columns={2}>
  <CTACard
    title="Overview"
    description="Business context, warning signs, and what success looks like"
    ctaLink="#overview"
    ctaLabel="Read first"
    Icon="Eye"
  />
  <CTACard
    title="Tutorial"
    description="Step-by-step: port dashboard logic to ClickHouse via MooseStack"
    ctaLink="#tutorial-migrate-your-dashboard-to-clickhouse"
    ctaLabel="Start building"
    Icon="Rocket"
  />
</CTACards>

## Overview

### What this guide is for

The root cause of slow dashboards is usually analytical queries (aggregations, wide table scans, complex joins) running on your existing transactional database, competing with the core transactional workloads that power the rest of your application.

You'll know you're hitting this when you see one (or more) of these symptoms:

<BulletPointsCard
  title="Common symptoms"
  bulletStyle="x"
  bullets={[
    {
      title: "Charts/metrics take 5+ seconds to load",
      description:
        "Users stare at spinners past the 3-second attention threshold, causing them to lose focus, abandon tasks, or complain about a slow product.",
    },
    {
      title: "Small report changes are expensive",
      description:
        "Adding a new report or filter is too risky and complex to build, so engineering prioritizes other features instead.",
    },
    {
      title: "Reporting traffic overwhelms production OLTP",
      description:
        "Analytics queries cause collateral damage to core app workflows.",
    },
  ]}
/>

This guide lays out a step-by-step path to offload those analytical workloads to a purpose-built analytical database (ClickHouse), incrementally, without needing to rearchitect your existing application.

At a high level, you'll:

1. Stand up ClickHouse and replicate the data your dashboards need
2. Set up a local development workflow that supports ClickHouse-backed analytics
3. Migrate one dashboard/report at a time by translating OLTP queries to ClickHouse
4. Ship your changes to production on Boreal Cloud

### Why this matters

Customer-facing analytics becomes mission-critical once users depend on it to understand their behavior, progress, or outcomes. Slow or unreliable dashboards drive down engagement: lower retention, higher churn, and direct revenue impact as users stop trusting your product for insights. Fast dashboards do the opposite—they encourage exploration, increase repeat usage, and let customers interact with your data in ways you can't predict upfront.

<Callout type="info" title="Case study: F45's LionHeart experience">
LionHeart is where F45's most engaged members track workout performance and progress. Their original OLTP-backed implementation meant they had to ship reports as static images rather than interactive charts.

After migrating the analytics backend to an OLAP architecture (Fiveonefour stack), LionHeart shipped fast, interactive dashboards in weeks:

- **Outcome:** [70% increase in user satisfaction, and over 1 full star increase in the F45 App Store rating](https://www.fiveonefour.com/blog/case-study-f45)
</Callout>

### Why you haven’t solved it yet

Most teams don't start with a dedicated analytical database (OLAP), and it's often the right call early on.

<BulletPointsCard
  title="Why teams delay adopting OLAP"
  bulletStyle="default"
  bullets={[
    {
      title: "The OLTP path is the highest-ROI path early on",
      description:
        "Your transactional database already powers your core application workflows. Early on, it can handle analytics queries too, so adding a second database delivers little marginal value.",
    },
    {
      title: "Performance doesn't degrade right away",
      description:
        'OLTP-backed analytics can look "fine" until data volume and concurrency cross a threshold (typically 10-50M rows, depending on query complexity and concurrent users).',
    },
    {
      title: "Shipping the first version is fastest on existing infrastructure",
      description:
        "The quickest path to value is usually building reporting directly on the systems you already run and understand.",
    },
    {
      title: "A second database is a real operational commitment",
      description:
        "Adding OLAP introduces new reliability, cost, security, and ownership concerns—not just a new query engine.",
    },
    {
      title:
        "Most OLAP stacks weren’t built for modern software engineering workflows",
      description:
        "Tooling can feel data-engineering-native, slowing adoption when the builders are primarily software engineering teams building on a application stack (e.g., Next.js, React, Ruby on Rails, etc.).",
    },
  ]}
/>

While it's common to delay adopting OLAP, there's an inflection point at which it becomes a real business risk. OLAP migrations are non-trivial and can take months with traditional data engineering tooling. If you wait too long to start or choose tooling with a steep learning curve, you may not move fast enough to fix the problem before users start churning.

<Callout type="info" title="Feeling daunted? This is designed to be incremental">
You don't need to migrate your entire analytics stack at once. In this guide, you'll translate **one dashboard/report at a time**: replicate only the data you need, model it in ClickHouse, and cut over reads when you’re confident.

If you get stuck on a schema design decision or an especially complex query, join the [Moose community Slack](https://join.slack.com/t/moose-community/shared_invite/zt-2fjh5n3wz-cnOmM9Xe9DYAgQrNu8xKxg) and we’ll help you map out the next step.

</Callout>

### When to pull the trigger

Users want more metrics, filters, breakdowns, or longer time ranges in their dashboards, but engineering can't deliver these seemingly small changes without risking the transactional workloads your core product depends on. Load times creep up. Customers complain. Before long, those complaints surface in retention metrics, support tickets, and sales calls.

Throwing more hardware at the problem either isn't possible or is prohibitively expensive. Even when feasible, this approach only buys temporary relief. The underlying issue is certain to resurface.

### What success looks like

You'll have migrated to ClickHouse successfully if you see a measurable impact on both product velocity and system reliability.

Dashboards stay fast as data grows, instead of degrading over time. Analytical workloads run on dedicated infrastructure, so your transactional database can focus on what it does best.

Engineering can ship new dashboards, metrics, and breakdowns as routine product work—not risky, backlog-bound projects. Features that used to require careful capacity planning become straightforward additions.

## Tutorial: Migrate Your Dashboard to ClickHouse

This tutorial assumes you already have a dashboard or report running in production, and you want to make it faster using OLAP best practices.

Your source database (Postgres or SQL Server) remains your transactional system of record, and your application continues writing to it as usual. CDC (ClickPipes or Debezium) streams changes into ClickHouse in real time so dashboards can read from ClickHouse instead of competing with OLTP workloads.

MooseStack is where you define your ClickHouse analytics layer **in code**. You’ll build views/materializations on top of the CDC-replicated raw tables and keep your existing dashboard APIs and request/response contracts the same—only the backing queries change.

By the end of this guide you’ll have:

1. A real-time replication pipeline feeding production ClickHouse from your source database.
2. A MooseStack project that defines ClickHouse resources (views, materialized views, serving tables) on top of the CDC-replicated raw tables, plus the query/API handlers your dashboard will call.
3. A local dev workflow for evolving those resources safely, and a Boreal-backed deploy workflow with preview environments + migration plans so you can validate changes before merging to `main`.

<Callout
  type="info"
  title="Starting from scratch?"
  href="/guides/chat-in-your-app"
  ctaLabel="Get started"
>
  If you’re building a new dashboard or data-connected chat (not migrating an
  existing dashboard), start with [Chat in Your App](/guides/chat-in-your-app).
</Callout>

### Customize this tutorial

<CustomizePanel
  title="Where does your current data live?"
  description="Select the database you use today so we can show the right way to replicate this data into ClickHouse"
>
  <CustomizeGrid columns={1}>
    <SelectField
      id="source-database"
      label="Source database"
      options={[
        { value: "sqlserver", label: "SQL Server" },
        { value: "postgres", label: "Postgres" },
      ]}
      defaultValue="postgres"
      persist
    />
    <SelectField
      id="os"
      label="Operating System"
      options={[
        { value: "macos", label: "macOS or Linux" },
        { value: "windows", label: "Windows (WSL 2)" },
      ]}
      defaultValue="macos"
      persist
    />
  </CustomizeGrid>
</CustomizePanel>

You'll work through:

<ConditionalContent whenId="source-database" whenValue="postgres">
  1. [Set up ClickHouse Cloud
  ClickPipes](#set-up-clickhouse-cloud-clickpipes-postgres-cdc) to replicate
  your Postgres tables into ClickHouse. 2. [Add MooseStack to your
  repo](#add-moosestack-to-your-project) and run `moose db pull` to generate
  typed models for the ClickPipes-owned tables. 3. [Connect
  Boreal](#deploy-your-moosestack-project-to-boreal) so pull requests get
  preview environments and migration plans. 4. [Migrate one dashboard
  component](#migrate-a-dashboard-component-to-clickhouse) using
  ClickHouse-native resources (views, materialized views, serving tables). 5.
  [Go to production](#going-to-production) using the preview environment and
  reviewed migrations.
</ConditionalContent>

<ConditionalContent whenId="source-database" whenValue="sqlserver">
  1. [Add MooseStack to your repo](#add-moosestack-to-your-project) (you’ll
  define the Debezium ingest pipeline in this project). 2. [Connect
  Boreal](#deploy-your-moosestack-project-to-boreal) so pull requests get
  preview environments and migration plans. 3. [Prepare your local dev
  environment](#prepare-your-local-development-environment) to work with
  ClickHouse. 4. [Set up Debezium CDC](#set-up-change-data-capture-cdc) to
  stream SQL Server changes into ClickHouse via your MooseStack ingest API. 5.
  [Migrate one dashboard
  component](#migrate-a-dashboard-component-to-clickhouse) using
  ClickHouse-native resources (views, materialized views, serving tables). 6.
  [Go to production](#going-to-production) using the preview environment and
  reviewed migrations.
</ConditionalContent>

<Callout type="info" title="AI-assisted Development">
  We recommend using an AI copilot to accelerate the migration to handle complex
  query translations. However, you can complete every step manually if you
  prefer. Any AI-enabled editor (Claude Code, Cursor, Codex, Opencode, GitHub
  Copilot, Windsurf, etc.) will work. Editors that support MCPs can make this
  workflow even faster.
</Callout>

**Language:** This guide is written for TypeScript developers. Python developers can follow along—the concepts translate directly, and MooseStack supports both languages. The main differences are syntax and framework-specific patterns.

<CommunityCallout title="Want Python Examples?">
  If you'd like to see Python-specific examples, let us know in our Slack
  community and we'll prioritize creating them.
</CommunityCallout>

<ConditionalContent whenId="source-database" whenValue="postgres">
### Set up ClickHouse Cloud ClickPipes (Postgres CDC)

ClickPipes is a managed service from ClickHouse Cloud that mirrors changes from Postgres into ClickHouse in real time. If you're already using ClickPipes and your replicated tables are present in ClickHouse, you can skip this step.

Set up ClickPipes first so the replicated raw tables exist in ClickHouse:

- [Create a new ClickHouse Cloud Account](https://auth.clickhouse.cloud/u/signup)
- [Docs for Setting up ClickPipes CDC with Postgres](https://clickhouse.com/docs/cloud/reference/billing/clickpipes/postgres-cdc)

<Callout type="info" title="ClickPipes owns the raw tables">
ClickPipes creates and manages the replicated raw tables in ClickHouse. MooseStack reads from those tables and creates its own ClickHouse resources (views, materialized views, serving tables) alongside them. MooseStack does not rewrite or migrate ClickPipes-owned tables.

In MooseStack, you’ll treat ClickPipes tables as **externally managed**. See [External Tables](/moosestack/olap/external-tables).

</Callout>

<CommunityCallout type="info" title="Not using ClickPipes?">
  If your CDC provider lands tables in ClickHouse (for example, Debezium,
  Supabase Live, or a custom pipeline), you can follow the same workflow: pull
  the table schemas into code as external models, then build ClickHouse-native
  views/materializations on top. If you want help mapping your setup to
  MooseStack, join the [MooseStack
  community](https://join.slack.com/t/moose-community/shared_invite/zt-2fjh5n3wz-cnOmM9Xe9DYAgQrNu8xKxg).
</CommunityCallout>

Once your ClickPipes pipeline is running and you can see replicated tables in ClickHouse Cloud, continue to [**Add MooseStack to your Project**](#add-moosestack-to-your-project). In that step, you’ll initialize `moosestack/` and run `moose db pull` to generate typed models for the ClickPipes-owned tables.

</ConditionalContent>

:::include /shared/guides/performant-dashboards/add-moosestack-to-your-project.mdx

:::include /shared/guides/performant-dashboards/deploy-your-moosestack-project-to-boreal.mdx

:::include /shared/guides/performant-dashboards/prepare-your-local-development-environment.mdx

:::include /shared/guides/performant-dashboards/set-up-change-data-capture-cdc.mdx

### Migrate a dashboard component to ClickHouse

In this section, you'll take an existing dashboard component that's currently served by an OLTP-backed API endpoint and switch it to an OLAP-backed implementation (ClickHouse + MooseStack). Concretely, you'll update the existing backend handler so it reads from the ClickHouse tables you've just built, instead of querying your OLTP database.

The rest of your application stays the same: routing, auth, request/response contracts, and frontend behavior. For each component you migrate, you'll add a small function in your MooseStack project that builds and runs the ClickHouse query, importing your `OlapTable` objects so column access is type-safe. Then you'll repoint the existing API handler to call that new OLAP function in place of the original OLTP query logic.

This guide follows a three-phase migration pattern:

1. **Parity (raw translation)**: Do a direct, SQL-for-SQL translation of your OLTP logic into ClickHouse so the endpoint returns the same result as the original OLTP endpoint. The goal here is correctness, not perfect OLAP code.
2. **Precompute (make it OLAP-native)**: Refactor that raw query by shifting joins and upfront transformations to Materialized Views and prepared tables. This makes reads cheaper and the model easier to extend.
3. **Serve (semantic/query layer)**: Layer a query/semantic model over those prepared tables so defining dashboard metrics, group-bys, filters, and other controls becomes clean, reusable, and maintainable, so you don't have to rewrite raw dynamic SQL in every handler.

#### (Recommended) Build a copilot context pack

Copy the starter kit to create a dedicated workspace to compile all the context your copilot needs to migrate each component. It includes a template for a `context-map.md` file (a worksheet that tracks the location of relevant files in your codebase for each component) and a full example to help your copilot complete each phase of the migration.

```bash
pnpm dlx tiged 514-labs/moose/examples/dashboard-migration moosestack/context
```

As you complete each phase of this guide, you will attach the required input files in your initial prompt to your copilot. From there, the copilot will update the workspace files as it completes the steps in this guide. 

Pick a specific dashboard component or report to migrate. You'll work through one component at a time.

<GuideStepper id="performant-dashboards-migration" persist>
  <GuideStepper.Step
    id="phase-1"
    number={1}
    title="Parity translation (OLTP → ClickHouse parity function)"
    summary="Translate your existing endpoint logic directly to ClickHouse and prove output parity."
  >
    <GuideStepper.AtAGlance title="Attach context files to your prompt">
      - `moosestack/context/dashboard-migration/<component>/` — the context directory for this component
      - The API specification for the endpoint
      - Your existing backend endpoint handler (e.g. Express route, Fastify handler) that serves the dashboard data
      - The OLTP query file(s) that the handler calls (e.g. SQL builder, ORM query, raw query function)
    </GuideStepper.AtAGlance>

    :::include /shared/guides/performant-dashboards/phase-1-checkpoints.mdx

  </GuideStepper.Step>

  <GuideStepper.Step
    id="phase-2"
    number={2}
    title="Performance optimization (Materialized Views)"
    summary="Move expensive read-time logic into serving tables and materialized views."
  >
    <GuideStepper.AtAGlance title="Attach context files to your prompt">
      - `moosestack/context/dashboard-migration/<component>/` — the context directory (context map, test cases from Phase 1)
      - The unoptimized parity query from Phase 1 (e.g. `moosestack/app/queries/<component>-olap-translation.ts`)
      - The MooseStack source table model files referenced in the parity query (e.g. `moosestack/app/models/*.ts`)
    </GuideStepper.AtAGlance>

    :::include /shared/guides/performant-dashboards/phase-2-checkpoints.mdx

  </GuideStepper.Step>

  <GuideStepper.Step
    id="phase-3"
    number={3}
    title="Serve the materialized view to your frontend"
    summary="Expose the serving table via Query Layer and preserve the existing API contract."
  >
    <GuideStepper.AtAGlance title="Attach context files to your prompt">
      - `moosestack/context/dashboard-migration/<component>/` — the context directory (test cases, context map)
      - `moosestack/app/models/<ServingTable>-mv.ts` — the serving table and MV file from Phase 2
      - Your existing backend endpoint handler that currently calls the OLTP or parity query (e.g. Express route, Fastify handler)
      - `moosestack/query-layer/` — the Query Layer source copied in the step above
    </GuideStepper.AtAGlance>

    :::include /shared/guides/performant-dashboards/phase-3-checkpoints.mdx

  </GuideStepper.Step>
</GuideStepper>

:::include /shared/guides/performant-dashboards/going-to-production.mdx
