---
title: Improving the Performance of Your Dashboards
description: Build fast dashboards using MooseStack and ClickHouse
previewVariant: "dashboards"
languages: ["typescript"]
---

import {
  CTACards,
  CTACard,
  Callout,
  BulletPointsCard,
  ToggleBlock,
  CustomizePanel,
  CustomizeGrid,
  SelectField,
} from "@/components/mdx";

# Improving the Performance of Your Dashboards

<Callout type="info" title="TL;DR">
* Slow dashboards are usually a database architecture problem, not a frontend or charting library problem.
* Running analytical queries on your transactional database degrades your entire application's performance and reliability.
* Moving these analytical queries to ClickHouse yields 10‚Äì100x faster dashboards

This guide will:

1. Help you recognize when OLTP has become your analytics bottleneck
2. Show an incremental path to move analytical workloads to OLAP
3. Walk through shipping the change end-to-end with MooseStack + ClickHouse
</Callout>

<CTACards columns={2}>
  <CTACard
    title="Overview"
    description="Business context, warning signs, and what success looks like"
    ctaLink="#overview"
    ctaLabel="Read first"
    Icon="Eye"
  />
  <CTACard
    title="Tutorial"
    description="Step-by-step: port dashboard logic to ClickHouse via MooseStack"
    ctaLink="#tutorial-migrate-your-dashboard-to-clickhouse"
    ctaLabel="Start building"
    Icon="Rocket"
  />
</CTACards>

## Overview

### What this guide is for

This guide is for you if you already have customer-facing dashboards or reports in production, and you're experiencing:

<BulletPointsCard
  title="Common symptoms"
  bulletStyle="bullet"
  bullets={[
    {
      title: "Charts/metrics take 5+ seconds to load",
      description: "Users stare at spinners past the 3-second attention threshold, causing them to lose focus, abandon tasks, or complain about a slow product."
    },
    {
      title: "Small report changes are expensive",
      description: "Adding a new report or filter is too risky and complex to build, so engineering prioritizes other features instead."
    },
    {
      title: "Reporting traffic overwhelms production OLTP",
      description: "Analytics queries cause collateral damage to core app workflows."
    }
  ]}
/>

The root cause of slow dashboards is usually analytical queries (aggregations, wide table scans, complex joins) running on your existing transactional database, which compete with the core transactional workloads that power the rest of your application.

This guide lays out a step-by-step path to offload those analytical workloads to a purpose-built analytical database (ClickHouse), that can be done incrementally and without needing to rearchitect your existing application.

### Why this matters

Customer-facing analytics becomes mission-critical once users depend on it to understand their behavior, progress, or outcomes. Slow or unreliable dashboards drive down engagement: lower retention, higher churn, and direct revenue impact as users stop trusting your product for insights.

<Callout type="info" title="Case study: F45's LionHeart experience">
LionHeart is where F45's most engaged members track workout performance and progress. Their original OLTP-backed implementation meant they had to ship reports as static images rather than interactive charts.

After migrating the analytics backend to an OLAP architecture (Fiveonefour stack), LionHeart shipped fast, interactive dashboards in weeks:

- **Outcome:** [70% increase in user satisfaction, and over 1 full star increase in the F45 App Store rating](https://www.fiveonefour.com/blog/case-study-f45)
</Callout>

### Why you haven‚Äôt solved it yet

Most teams don't start with a dedicated analytical database (OLAP), and it's often the right call early on.

<BulletPointsCard
  title="Why teams delay adopting OLAP"
  bulletStyle="bullet"
  bullets={[
    {
      title: "The OLTP path is the highest-ROI path early on",
      description: "Your transactional database already powers your core application workflows. Early on, it can handle analytics queries too, so adding a second database delivers little marginal value."
    },
    {
      title: "Performance doesn't degrade right away",
      description: "OLTP-backed analytics can look \"fine\" until data volume and concurrency cross a threshold (typically 10-50M rows, depending on query complexity and concurrent users)."
    },
    {
      title: "Shipping the first version is fastest on existing infrastructure",
      description: "The quickest path to value is usually building reporting directly on the systems you already run and understand."
    },
    {
      title: "A second database is a real operational commitment",
      description: "Adding OLAP introduces new reliability, cost, security, and ownership concerns‚Äînot just a new query engine."
    },
    {
      title: "Most OLAP stacks weren‚Äôt built for modern software engineering workflows",
      description: "Tooling can feel data-engineering-native, slowing adoption when the builders are primarily software engineering teams building on a application stack (e.g., Next.js, React, Ruby on Rails, etc.)."
    }
  ]}
/>

While it's common to delay adopting OLAP, there's an inflection point at which it becomes a real business risk. OLAP migrations are non-trivial and can take months with traditional data engineering tooling. If you wait too long to start or choose tooling with a steep learning curve, you may not move fast enough to fix the problem before users start churning. 

### When to pull the trigger

Users want more metrics, filters, breakdowns, or longer time ranges in their dashboards. But engineering can't deliver these seemingly small changes without risking the transactional workloads your core product depends on. Load times creep up. Customers complain. Before long, those complaints surface in retention metrics, support tickets, and sales calls.

Throwing more hardware at the problem either isn't possible or is prohibitively expensive. Even when feasible, this approach only buys temporary relief. The underlying issue is certain to resurface.

### What success looks like

When this works, the impact shows up in both product velocity and system reliability.

Dashboards stay fast as data grows, instead of degrading over time. Analytical workloads run on dedicated infrastructure, so your transactional database can focus on what it does best.

Engineering can ship new dashboards, metrics, and breakdowns as routine product work‚Äînot risky, backlog-bound projects. Features that used to require careful capacity planning become straightforward additions.


## Tutorial: Migrate Your Dashboard to ClickHouse

This tutorial assumes you already have a dashboard or report running in production, and you want to make it faster using OLAP best practices. You'll work through:

1. Getting you up and running with a ClickHouse database
2. Setting up your local dev environment to work with ClickHouse
2. Migrating your dashboard business logic from OLTP to MooseStack/ClickHouse
3. Shipping your changes to production on Boreal Cloud

**AI copilot (optional but recommended):** We recommend using an AI copilot to accelerate the migration to handle complex query translations. However, you can complete every step manually if you prefer. Any AI-enabled editor (Claude Code, Cursor, Codex, Opencode, Github Copilot, Windsurf, etc.) will work.

**Language:** This guide is written for TypeScript developers. Python developers can follow along‚Äîthe concepts translate directly, and MooseStack supports both languages. The main differences are syntax and framework-specific patterns. If you'd like to see Python-specific examples, let us know and we'll prioritize creating them.

<CustomizePanel
  title="Where does your current data live?"
  description="Select the database you use today so we can show the right way to replicate this data into ClickHouse"
>
  <CustomizeGrid columns={1}>
    <SelectField
      id="source-database"
      label="Source database"
      options={[
        { value: "sqlserver", label: "SQL Server" },
        { value: "postgres", label: "Postgres" },
      ]}
      defaultValue="postgres"
      persist
    />
  </CustomizeGrid>
</CustomizePanel>


<ConditionalContent whenId="cdc-method" whenValue="postgres">
### ClickHouse Cloud ClickPipes
 To get started, you'll need to create a new ClickHouse Cloud account and create a new ClickPipes pipeline to mirror your Postgres data into ClickHouse.

- [Create a new ClickHouse Cloud Account](https://auth.clickhouse.cloud/u/signup)
- [Docs for Setting up ClickPipes CDC with Postgres](https://clickhouse.com/docs/cloud/reference/billing/clickpipes/postgres-cdc)
</ConditionalContent>

### Project setup

In this section, you'll:

* Install MooseStack and its dependencies
* Initialize MooseStack in your repo
* Create a Boreal project connected to your repo
* Capture your production ClickHouse connection string

#### Installing MooseStack

<BulletPointsCard
  title="Prerequisites"
  bulletStyle="check"
  compact={true}
  divider={false}
  bullets={[
    {
      title: "Node.js 20+",
      description: "Check your version with `node --version`.",
      link: { text: "Download", href: "https://nodejs.org/en/download", external: true }
    },
    {
      title: "Docker Desktop",
      description: "For local ClickHouse and Redpanda. Make sure it's running before proceeding.",
      link: { text: "Download", href: "https://docs.docker.com/desktop/", external: true }
    },
    {
      title: "macOS, Linux, or WSL 2+ on Windows",
      description: "Windows users: see the appendix for WSL 2 setup instructions."
    }
  ]}
/>

**Install MooseStack**

```bash
bash -i <(curl -fsSL https://fiveonefour.com/install.sh) moose
```

<Callout type="info" title="Make sure to restart your terminal after installing MooseStack.">
MooseStack is installed globally, so you need to restart your terminal to make the `moose` command available.
</Callout>

#### Add MooseStack to your existing application

If you have an existing monorepo with your application (e.g., a Next.js frontend with an API backend), you can add MooseStack as a subdirectory:

**Step 1: Initialize MooseStack in a subdirectory**

From your monorepo root:

```shell
mkdir moosestack
cd moosestack
moose-cli init . typescript
```

**Step 2: Update your PNPM workspace configuration**

Edit your root `pnpm-workspace.yaml` to include the `moosestack` directory:

```yaml
packages:
  - .                    # Your main app (e.g., Next.js)
  - moosestack                # MooseStack project
```

**Step 3: Configure the MooseStack package**

Edit `moosestack/package.json` to set it up as a workspace package:

```json
{
  "name": "moosestack",
  "private": true,
  "version": "0.0.1",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    }
  },
  "scripts": {
    "build": "moose-cli build",
    "dev": "moose-cli dev",
    "moose": "moose-cli"
  },
  "dependencies": {
    "@514labs/moose-lib": "latest",
    "typia": "^10.1.0"
  },
  "devDependencies": {
    "@514labs/moose-cli": "latest",
    "@types/node": "^24",
    "typescript": "^5"
  }
}
```

**Step 4: Add the `pnpm` configuration to your root package.json**

Edit your root `package.json` to add the `pnpm.onlyBuiltDependencies` section (this ensures required native/built dependencies are handled correctly by PNPM):

```json
{
  "pnpm": {
    "onlyBuiltDependencies": [
      "@confluentinc/kafka-javascript",
      "@514labs/kafka-javascript"
    ]
  }
}
```

#### Create your Boreal project (production access)

Step 0 - Sign up for Boreal Cloud.

Go to [boreal.cloud](https://boreal.cloud) and sign up for an account.

Once you have signed up, you‚Äôll land on the `Projects` page. Click `New Project` to start importing your repository.

![Authorize GitHub](/deployment/authorize-github.png)

Before you can create a new project, you‚Äôll be prompted to authorize the Boreal GitHub app for the GitHub account (or organization) that hosts your repository.

This authorization connects your repo to your deployment pipeline so that:

- every `git push` can trigger an automated deployment, and
- pull requests can get their own preview environment (one per branch), with production deploys tied to merges into your main branch.

In the GitHub authorization screen:

- choose the GitHub account/org that owns the repo
- select which repositories Boreal can access (prefer ‚Äúonly select repositories‚Äù if you want tighter scope)
- click `Authorize` to continue back to the project import flow

**Step 1 ‚Äì Define Project Details**

![Project Details](/deployment/import-existing-project.png)

On the ‚ÄúImport Existing Project‚Äù page, confirm the basic settings Boreal will use to build and deploy this repo:

- `Name`: this field should contain the desired project name. By default it shows the repository name; update it if necessary.

- `Project Root`: set the directory within the repository that contains your MooseStack project (for example, the folder that contains your Moose config like `project.toml`). If this is a monorepo, make sure you pick the specific subfolder for the app you‚Äôre deploying (e.g., `moosestack/`).

Once these fields look correct, click `Next` to proceed.

**Step 2 ‚Äì Select Data Services**

![Select Data Services](/deployment/configure-data-services.png)

The next screen configures the data services your deploys will use. In practice, this sets:

- where main-branch deployments apply schema changes, and
- where pull-request preview environments provision isolated databases.

You can accept the default managed services, or connect existing infrastructure.

<Callout type="info" title="Important">
If you already use ClickHouse Cloud, click `Customize` to connect to your existing instance.
</Callout>


**Step 3 ‚Äì Configure ClickHouse Connection**

![Configure ClickHouse Connection](/deployment/configure-clickhouse-connection.png)

If you chose `Customize`, open the ClickHouse configuration and select `Connect to existing ClickHouse instance`. Then fill out the form:

- `Host`: enter the hostname of your ClickHouse instance (e.g., your-instance.clickhouse.cloud).
- `Port`: specify the port number (for ClickHouse Cloud this is commonly 8443).
- `User & Password`: supply the credentials for your ClickHouse user. 

<Callout type="info" title="Admin Role Required">
This user must have the `admin` role. This is required in order to create preview database instances for each preview environment.
</Callout>
- `Connection Name`: give this connection an identifiable name (e.g., ‚ÄúProd ClickHouse Cloud‚Äù) for future reference.

Click `Test Connection` to verify connectivity. If the test fails, fix the connection details before continuing.

If you intend to use this connection for other projects, toggle `Use as Default`.

Finally, click `Add connection` to finish the setup.

**Step 4 ‚Äì Set the main-branch database target**

![Set Main Branch](/deployment/set-main-db.png)

Next, choose the database that `main` branch deployments should target. This is the ‚Äúproduction‚Äù analytics database in your ClickHouse instance: when you merge changes into your repository‚Äôs main branch, schema changes will be applied here.

In the database selector, choose the database you created in ClickHouse Cloud (the same one your CDC pipeline will write into).

**Step 5 - Deploy your project**

Click `Deploy` to run the initial build and create the first deployment for this repo.

This will redirect you to the `Deployments` page. You can see the status of your deployment in the `Status` column. Typically, it will take around 1-2 minutes to deploy.

Once the deployment completes, you‚Äôll be redirected to the project overview page. 

**Step 6 - Copy the connection string**

![Copy Connection String](/deployment/copy-connection-string.png)

From here, you can copy the ClickHouse HTTPS connection string. This will be used in the next section to seed your local ClickHouse database.

### Start Developing

At this point, your MooseStack project exists in your repo and your Boreal project is connected and configured. Next you'll start local development and tooling.

If you are not already in a terminal, open a new terminal and navigate to your repository.

#### Create a branch

Create a new branch in your repo to isolate your changes. When you open a pull request from this branch, you‚Äôll get a preview environment you can use to validate changes before merging.

```shell
git checkout -b <your-branch-name>
```

#### Run your Moose Development Server

Ensure Docker Desktop is running first.  

![Docker Desktop](/docker-desktop.png)


Navigate to your MooseStack project directory and run the development server:

```shell
cd moosestack  # or wherever your moose project is
pnpm install
moose dev
```

If the development server runs correctly, you'll see a list of the available API endpoints print:

```
üìç Available Routes:
  Base URL: http://localhost:4000

  Static Routes:
  METHOD  ENDPOINT                                           DESCRIPTION
  ------  --------                                           -----------
  GET     /admin/inframap                                    Admin: Get infrastructure map
  GET     /admin/reality-check                               Admin: Reality check - provides a diff when drift is detected between the running instance of moose and the db it is connected to
  GET     /health                                            Health check endpoint
  GET     /ready                                             Readiness check endpoint
...

```

And your local services will be running in Docker containers:  

![Docker Services](/docker-services.png)

#### Setting up developer / agent supporting services

We optionally recommend setting up your development environment with the following add-ons to let your AI copilot query your local database, inspect infrastructure, and validate SQL directly. These are not required, but can help you get the most out of your development environment.

**[Moose Dev MCP server](https://docs.fiveonefour.com/moosestack/moosedev-mcp?lang=typescript)**: 

Lets your AI copilot query your local ClickHouse database, inspect logs, and explore infrastructure using natural language. Your copilot can verify query results, debug issues, and validate migrations without you copying/pasting commands.

:::include /shared/prerequisites/install-mcp.mdx

**Language Server (LSP) for MooseStack TypeScript**: 
The [Moose LSP for Typescript](https://github.com/514-labs/moosestack-lsp) enables SQL validation and syntax highlighting for `sql` strings in Typescript objects in your Moose project. 
Install the extension via your IDE marketplace:
- [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=514-labs.moosestack-lsp)
- [Cursor Marketplace](https://open-vsx.org/extension/514-labs/moosestack-lsp)

<Callout type="info" title="Alpha release">
This feature is currently in alpha and has the following known limitation:
- Nested SQL and SQL fragments will be incorrectly highlighted as errors. 
If you would like access to experiment with this feature, let us know!
</Callout> 

<ConditionalContent whenId="source-database" whenValue="sqlserver">
### Change data capture (CDC)

For your dashboards to show accurate, real-time data, your ClickHouse analytics database needs to stay in sync with your production SQL Server database. Change Data Capture (CDC) streams every insert, update, and delete in real time. Your reports always reflect the latest state without custom sync jobs or batch ETL scripts.


This section walks you through setting up CDC using Debezium. Before you start, make sure you have your SQL Server connection credentials (host, port, username, password, database name).

You'll complete 3 main steps:

1. Configure SQL Server for CDC
2. Configure ingestion pipelines to land CDC events into ClickHouse tables (via MooseStack)
3. Deploy Debezium Server to your environment

#### How CDC works

CDC turns row-level changes in SQL Server into an ordered stream of change events that your MooseStack-defined ingestion pipeline receives and routes into ClickHouse landing tables. This diagram shows the high-level data flow:

```mermaid
flowchart LR
    SourceCustomers["Source DB<br/>UPDATE dbo.customers ..."] --> Debezium["Debezium"]
    SourceOrders["Source DB<br/>UPDATE dbo.orders ..."] --> Debezium
    Debezium --> Ingest["Ingest API (HTTP)"]
    Ingest --> Stream["Kafka/Redpanda Stream<br/>(shared CDC stream)"]
    Stream --> FanOut["Streaming Function<br/>(inspect metadata + fan out)"]
    FanOut --> CHCustomers["ClickHouse landing table<br/>customers_landing"]
    FanOut --> CHOrders["ClickHouse landing table<br/>orders_landing"]
```

<ToggleBlock openText="Expected CDC overhead (storage/CPU/I/O/memory)" closeText="Hide CDC overhead estimates">
#### Storage Impact

Primarily driven by change volume (rows changed / day) and retention period

| Component | Expected Impact |
| :---- | :---- |
| Tracking tables (`cdc.*_CT`) | ~10-30% of source table sizes (not 2x) |
| Retention | Default 3 days, then auto-purged |
| Transaction log | May grow if capture job lags |

#### CPU Impact

Primarily driven by change rate (changes / sec) and the number of CDC-enabled tables

| Component | Expected Overhead |
| :---- | :---- |
| CDC Capture Job | Low additional CPU |
| Multiple tables | Low-moderate (well within normal) |
| Polling interval (e.g. 2-min) | Negligible (just reads) |

#### I/O Impact

Primarily driven by write volume to tracking tables and transaction log read rate

| Operation | Expected Overhead |
| :---- | :---- |
| Transaction log reads | Low additional read I/O |
| Tracking table writes | Low additional write I/O |
| Debezium queries (periodic) | Minimal - batched reads |

#### Memory Impact

Primarily driven by the number of CDC-enabled tables and their row size

| Component | Expected Overhead |
| :---- | :---- |
| Capture job buffers | Low, measured in MB |
| Tracking table indexes | Proportional to change volume |
</ToggleBlock>

#### SQL Server CDC Configuration

Configure your SQL Server for CDC before proceeding. For detailed setup instructions, see the [Debezium SQL Server Connector documentation](https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#setting-up-sqlserver).

**Key steps:**

1. Enable CDC at the database level: `EXEC sys.sp_cdc_enable_db;`
2. Enable CDC on each table you want to track: `EXEC sys.sp_cdc_enable_table @source_schema = N'dbo', @source_name = N'your_table', @role_name = NULL;`
3. Verify SQL Server Agent is running (required for CDC capture jobs)

Once CDC is configured, proceed to set up the MooseStack ingestion pipeline below

#### Define your MooseStack ingestion pipeline

This section describes the CDC architecture used when Debezium cannot connect directly to Kafka (for example, due to firewall rules, network segmentation, or cloud provider restrictions). If your Debezium instance can reach Kafka directly, you can configure CDC to stream changes directly into Kafka topics instead of using the HTTP ingest pattern shown here.

In this architecture, all changes from your SQL Server database tables are captured using Debezium and sent to a single, Moose-managed Kafka (or Redpanda) Stream via a Moose-managed HTTP Ingest API endpoint *(there is no per-table endpoint or per-table ingest stream at this stage)*.

From there, CDC events are explicitly fanned out based on their metadata. Each SQL Server source table ultimately maps to:

* one table-specific Moose Stream, and  
* one downstream ClickHouse landing table.

This architecture implements a [Streaming Function](https://docs.fiveonefour.com/moosestack/streaming/consumer-functions?lang=typescript) that inspects each raw CDC event from the shared stream to identify the source table and routes it to the appropriate table-specific stream. Each of these streams feeds a corresponding ClickHouse table.

To summarize, the high level data flow is:

* Debezium ‚Üí single ingest API endpoint  
* Ingest API ‚Üí shared CDC stream  
* Streaming function ‚Üí table-specific streams  
* Table-specific streams ‚Üí ClickHouse tables

**Step 0: Prerequisites**

<BulletPointsCard
  title="Before continuing, confirm"
  bulletStyle="check"
  compact={true}
  bullets={[
    "SQL Server CDC is enabled and working",
    "MooseStack is running locally",
    "You can reach the Moose Ingest API from your Debezium environment",
    "Port 443 is available for egress",
  ]}
/>

If CDC is not enabled yet, complete the **SQL Server CDC Configuration** section first.

**Step 1: Add a Moose Ingest API Endpoint for CDC Events**

This endpoint will receive CDC events from Debezium. 

In your MooseStack project, create a new Ingest API sink that accepts Debezium `ChangeEvent` payloads:

**File location:** `moosestack/src/cdc/DebeziumChangeEvent.model.ts`

```ts
import { IngestApi, Stream, DeadLetterQueue, Int64 } from "@514labs/moose-lib";

/**
 * Debezium SQL Server Change Event Structure
 * Documentation: https://debezium.io/documentation/reference/stable/connectors/sqlserver.html
 *
 * This model represents the standardized event structure sent by Debezium
 * for all change data capture events from SQL Server.
 */

export interface DebeziumSource {
  version: string;
  connector: string;
  name: string;
  ts_ms: Int64; // Timestamp in milliseconds (epoch)
  snapshot?: string;
  db: string;
  sequence?: string;
  schema: string;
  table: string;
  change_lsn?: string;
  commit_lsn?: string;
  event_serial_no?: number;
}

export interface DebeziumTransaction {
  id?: string;
  total_order?: number;
  data_collection_order?: number;
}

/**
 * Main Debezium Change Event Payload
 * The 'before' and 'after' fields contain the actual row data
 * and their shape varies by table, so we use Record<string, any>
 */
export interface DebeziumChangeEvent {
  // Row data before the change (null for INSERT operations)
  before?: Record<string, any> | null;

  // Row data after the change (null for DELETE operations)
  after?: Record<string, any> | null;

  // Source metadata identifying where this change came from
  source: DebeziumSource;

  // Operation type: 'c' = create, 'u' = update, 'd' = delete, 'r' = read (snapshot)
  op: string;

  // Timestamp in milliseconds
  ts_ms: Int64;

  // Transaction metadata (optional)
  transaction?: DebeziumTransaction | null;
}

/**
 * Full Debezium Event Envelope (what actually gets POSTed by Debezium)
 * Debezium sends events with both schema and payload wrapped together
 */
export interface DebeziumEventEnvelope {
  schema?: Record<string, any>;
  payload: DebeziumChangeEvent;
}

/**
 * Stream for CDC events - fans out to table-specific streams via streaming function
 */
export const DebeziumChangeEventStream = new Stream<DebeziumEventEnvelope>("DebeziumChangeEvent");

/**
 * Dead Letter Queue for failed Debezium events
 */
export const DebeziumChangeEventDLQ = new DeadLetterQueue<DebeziumEventEnvelope>(
  "DebeziumChangeEvent_DLQ"
);

/**
 * Ingestion API endpoint for Debezium CDC events
 * Creates: POST /ingest/DebeziumChangeEvent
 *
 * Debezium sends events here, which flow through the streaming function
 * to fan out to table-specific Redpanda topics.
 */
export const DebeziumChangeEventIngestApi = new IngestApi<DebeziumEventEnvelope>(
  "DebeziumChangeEvent",
  {
    destination: DebeziumChangeEventStream,
    deadLetterQueue: DebeziumChangeEventDLQ,
  }
);
```

**Step 2: Map Source Tables to Moose Streams**

Before implementing the streaming function, define an explicit mapping between SQL Server source table names and their corresponding Moose Streams.

**File location:** `moosestack/src/cdc/tableStreamMap.ts`

```typescript
import { ProductStream } from "../models/Product.model";
import { CustomerStream } from "../models/Customer.model";
import { OrderStream } from "../models/Order.model";
import { OrderItemStream } from "../models/OrderItem.model";

export const TABLE_STREAM_MAP: Record<string, any> = {
  products: ProductStream,
  customers: CustomerStream,
  orders: OrderStream,
  order_items: OrderItemStream,
};
```

This mapping is what makes the fan-out deterministic and ensures each source table's changes flow through the correct stream and into the correct ClickHouse table.

**Step 3: Add Streaming Function to the API Endpoint**

This function acts as our fanout point.

When cdc events are posted from Debezium, we need to read the table name from the payload and route the cdc event to the correct stream.

**File location:** `moosestack/src/cdc/processDebeziumEvent.ts`

```ts
import {
  DebeziumEventEnvelope,
  DebeziumChangeEventStream,
} from "./DebeziumChangeEvent.model";
import { TABLE_STREAM_MAP } from "./tableStreamMap";

/**
 * Process and route CDC events to table-specific Redpanda topics
 *
 * ReplacingMergeTree CDC fields:
 * - ts_ms: Version column from payload.ts_ms (used to determine newest row)
 * - isDeleted: 1 for delete operations, 0 otherwise (ReplacingMergeTree collapses deleted rows)
 */
export default async function processDebeziumEvent(envelope: DebeziumEventEnvelope): Promise<void> {
  console.log(`[CDC] Processing event: ${JSON.stringify(envelope)}`);

  const event = envelope.payload;
  const { source, op, before, after, ts_ms } = event;

  const sourceTable = source.table;
  const targetStream = TABLE_STREAM_MAP[sourceTable];

  // Unknown table - log and skip
  if (!targetStream) {
    console.warn(`[CDC] Unknown table: ${sourceTable}`);
    return;
  }

  // Determine data and deleted flag based on operation type
  let rowData: Record<string, any> | null = null;
  let isDeleted: number = 0;

  switch (op) {
    case "c": // CREATE
    case "r": // READ (snapshot)
    case "u": // UPDATE
      rowData = after ?? null;
      isDeleted = 0;
      break;
    case "d": // DELETE - use 'before' data since 'after' is null for deletes
      rowData = before ?? null;
      isDeleted = 1;
      break;
    default:
      console.warn(`[CDC] Unknown op: ${op} for ${sourceTable}`);
      return;
  }

  if (!rowData) {
    console.warn(`[CDC] No data in ${op} event for ${sourceTable}`);
    return;
  }

  // Add CDC metadata columns for ReplacingMergeTree
  // Ensure isDeleted is explicitly UInt8 (0 or 1) for ClickHouse
  // Use bitwise OR with 0 to ensure it's an integer, not Float64
  const data = {
    ...rowData,
    ts_ms: ts_ms, // Version column - determines which row is newest
    isDeleted: (isDeleted | 0) as 0 | 1, // isDeleted flag - 1 for deletes, 0 otherwise (UInt8)
  };

  // Publish directly to table's Redpanda topic
  try {
    await targetStream.send(data);
    console.log(
      `[CDC] ${op.toUpperCase()} ${sourceTable} ‚Üí Redpanda topic (ts_ms=${ts_ms}, isDeleted=${isDeleted})`
    );
  } catch (error: any) {
    console.error(`[CDC] Failed to publish ${sourceTable}:`, error.message);
    throw error; // Trigger DLQ
  }
}

// Wire up the streaming function
DebeziumChangeEventStream.addConsumer(processDebeziumEvent);
```

#### Deploy Debezium Server

Debezium Server can be deployed using Kubernetes, Docker Compose, or as a standalone service. For deployment options and detailed configuration, see the [Debezium Server documentation](https://debezium.io/documentation/reference/stable/operations/debezium-server.html).

**Key configuration for MooseStack integration:**

Your Debezium `application.properties` must include:
- `debezium.sink.type=http` ‚Äî Use HTTP sink to send events to Moose
- `debezium.sink.http.url=<YOUR_MOOSE_INGEST_URL>/ingest/DebeziumChangeEvent` ‚Äî Your Moose Ingest API endpoint
- `debezium.source.connector.class=io.debezium.connector.sqlserver.SqlServerConnector` ‚Äî SQL Server connector

<Callout type="info" title="Example deployment manifests">
For complete Kubernetes deployment examples with MooseStack, see our [CDC examples repository](https://github.com/514-labs/moose-examples).
</Callout>
</ConditionalContent>

### Migrating a dashboard component to OLAP

In this section, you'll take an existing dashboard component that's currently served by an OLTP-backed API endpoint and switch it to an OLAP-backed implementation (ClickHouse + MooseStack). Concretely, you'll update the existing backend handler so it reads from the ClickHouse tables you've just built, instead of querying your OLTP database.

The rest of your application stays the same: routing, auth, request/response contracts, and frontend behavior. For each component you migrate, you'll add a small function in your MooseStack project that builds and runs the ClickHouse query, importing your `OlapTable` objects so column access is type-safe. Then you'll repoint the existing API handler to call that new OLAP function in place of the original OLTP query logic.

This guide follows a three-phase migration pattern:

1. **Parity (raw translation)**: Do a direct, SQL-for-SQL translation of your OLTP logic into ClickHouse so the endpoint returns the same result as the original OLTP endpoint. The goal here is correctness, not perfect OLAP code.
2. **Precompute (make it OLAP-native)**: Refactor that raw query by shifting joins and upfront transformations to Materialized Views and prepared tables. This makes reads cheaper and the model easier to extend.
3. **Serve (semantic/query layer)**: Layer a query/semantic model over those prepared tables so defining dashboard metrics, group-bys, filters, and other controls becomes clean, reusable, and maintainable, so you don't have to rewrite raw dynamic SQL in every handler.

#### Step 0: Prepare your environment

Before starting the migration, prepare your development environment by seeding your local ClickHouse database with a deterministic slice of production data. You‚Äôll use this data to validate parity during Phase 1.

Ensure your dev server is running and accessible:
```shell
moose dev
```

Run the [`moose seed` command](https://docs.fiveonefour.com/moosestack/moose-cli?lang=typescript#seed-clickhouse) to pull data from your production ClickHouse database:

```shell
moose seed clickhouse --connection-string <PRODUCTION_CONNECTION_STRING> --limit 1000
```

**Recommended (seed for parity):** seed the specific table(s) your component will query, ordered by recency, with a large-enough limit to cover the time window you‚Äôll test (e.g. ‚Äúlast 7‚Äì30 days‚Äù).

Run this once per table your endpoint depends on:

```shell
moose seed clickhouse \
  --connection-string <PRODUCTION_CONNECTION_STRING> \
  --table <TABLE_NAME> \
  --order-by '<TIMESTAMP_COLUMN> DESC' \
  --limit 100000
```

If you hit missing-data issues during parity validation, re-run seeding with a larger `--limit` (or seed full tables with `--all`).

**Strict parity (slowest):** seed full production tables with `--all` (can take hours for large datasets):

```shell
moose seed clickhouse --connection-string <PRODUCTION_CONNECTION_STRING> --all
```

`PRODUCTION_CONNECTION_STRING` is your connection string to your production ClickHouse database. 

**Important:** `moose seed clickhouse` uses ClickHouse's HTTPS protocol. Use a HTTPS connection string (e.g., `https://username:password@host:8443/database`).

Use the ClickHouse HTTPS connection string from the Boreal project you just created. 


Optionally set it as an environment variable:

```shell
export MOOSE_SEED_CLICKHOUSE_URL=PASTE_THE_CLICKHOUSE_HTTPS_URL_HERE
```

Once your local database is seeded, proceed to Step 1.

<ToggleBlock openText="Advanced: seed local ClickHouse from remote via HTTP" closeText="Hide advanced seeding">
Prompt your copilot to seed your local database from each remote table:

```
Goal: seed local ClickHouse from a remote Boreal ClickHouse via HTTP using `moose query`.

Assumptions:
- `BOREAL_CONNECTION_STRING` is set and looks like: `https://default:PASSWORD@HOST:8443/DB_NAME`
- Local ClickHouse has the same tables already created.
- Use `url()` + `JSONEachRow` to stream data from remote into local.

Steps:

1) Extract remote base URL + database name from `BOREAL_CONNECTION_STRING`.
2) Get list of tables (exclude system + materialized views).
3) For each table, generate a schema string ("col1 Type1, col2 Type2, ...") from `DESCRIBE TABLE ... FORMAT JSONEachRow`.
4) For each table, generate a seed SQL file with this pattern:

   INSERT INTO <table>
   SELECT * FROM url(
     '<base_url>/?database=<db>&query=SELECT+*+FROM+<table>+FORMAT+JSONEachRow',
     JSONEachRow,
     '<schema_string>'
   );

   (No LIMIT clauses: copy all rows.)

5) Run all generated seed SQL files with `moose query`.
6) Verify row counts for each table (use FINAL) using `moose query`.

Implement as a single bash script.
```
</ToggleBlock>

#### Step 1: Choose the component to migrate

Pick a specific dashboard component or report to migrate. You'll work through one component at a time.

For this guide, we'll use an e-commerce order fulfillment report:

- Dataset: `OrderFulfillment`
- Purpose: Track order processing metrics by merchant and time period

#### Phase 1: Parity translation (OLTP ‚Üí ClickHouse SQL)

The goal of this phase is to reproduce your existing API behavior exactly by translating your OLTP query logic into ClickHouse SQL.

<Callout type="tip" title="AI acceleration">
To complete this phase faster with AI assistance, give your copilot this guide section along with your component details: the API route, existing SQL/stored procedure, and 2-3 example request/response pairs.
</Callout>

Before you start, make sure you have your local development server running and seeded with sample data from your production ClickHouse database.

##### Step 1: Capture test cases

Because your local ClickHouse is usually seeded with only a slice of production data, your parity checks are only meaningful for requests that fall inside that seeded slice (e.g., ‚Äúlast 7‚Äì30 days‚Äù if you seeded the most recent rows).

If a test case fails because your local ClickHouse is missing underlying rows, **don‚Äôt** treat it as a translation bug‚Äîre-run seeding with a larger `--limit` (or `--all`) and try again.

Do this 2-5 times. For each test case, save two files:

- `test-case-N.request.json`: the exact JSON request body you send to the endpoint
- `test-case-N.response.json`: the exact JSON response body you receive from the endpoint

Save the endpoint path, HTTP method, and required headers once in your context pack (from Step 2). The goal here is to have a request body file you can pass directly to `curl`.

Example request body (`test-case-1.request.json`):

```json
{
  "merchantId": "123",
  "startDate": "2024-01-01",
  "endDate": "2024-01-31"
}
```

Example response body (`test-case-1.response.json`):

```json
{
  "REPLACE_ME": "paste the full JSON response body here"
}
```

These test cases are your source of truth for validating parity in your ClickHouse-backed implementation. You must return identical responses for every test case recorded.

##### Step 2: Locate the OLTP query logic

Find the SQL query or stored procedure that powers the existing endpoint. This is typically in your backend handler or a database layer. Gather:

- The raw SQL or stored procedure definition
- Any parameter substitution logic
- Join conditions and filter clauses

##### Step 3: Translate to ClickHouse SQL

Write a ClickHouse query that produces identical output to the OLTP query. Use the [ClickHouse SQL reference](https://clickhouse.com/docs/en/sql-reference) as you translate.

Important: Preserve the exact output structure and column names as in the OLTP query.

##### Step 4: Compare ClickHouse results to test cases

Run the ClickHouse query against your local database and compare the results to your test cases.

If there are differences:
- Check column names (case-sensitive in ClickHouse)
- Check data types (timestamps, decimals)
- Check sort order (add explicit `ORDER BY` if needed)

Iterate until the diff is empty.

Once all test cases pass, proceed to Phase 2.

#### Phase 2: Performance optimization (Materialized Views)

Phase 1 got you a working ‚Äúparity query‚Äù: it returns the right shape and results, but it‚Äôs often not the cleanest or fastest way to query in OLAP.

In this phase, you‚Äôll turn it into two OLAP-native building blocks:

- **A serving table**: a flattened, pre-joined, precomputed table designed for reads (your API queries this).
- **A Materialized View (MV)**: write-time logic that continuously populates the serving table from your source tables.

The end state is that your API reads from the serving table with lightweight, request-driven logic (filters, grouping, column selection), and does not have any joins, CTEs, or heavy reshaping logic.

<Callout type="tip" title="AI acceleration">
To complete this phase faster with AI assistance, give your copilot this guide section along with your working parity query from Phase 1.
</Callout>

Input: the working ClickHouse parity query from Phase 1.

##### Step 1: Define the serving table schema

Determine the grain of your serving table, meaning what one row represents (for example, one row per `merchant_id` per day).

To model a complex parity query (joins, aggregations, cascading CTEs), use this recipe:

1. **Start from the output contract**: copy the final `SELECT` list from your parity query. Those columns must exist in your serving table.
2. **Make the grain explicit**: decide what uniquely identifies one row (e.g. `merchant_id` + `day`). This becomes the ‚Äúprimary key‚Äù of your serving table.
3. **Flatten the query into stages**: rewrite CTEs as a sequence of named steps. If the query is large, create **multiple** serving/staging tables (one per major CTE) instead of one giant MV.
4. **Precompute everything not driven by request parameters**: joins, fixed-grain aggregates, and derived fields belong in MVs. Keep only truly dynamic filters (from the API request) for the final read query.
5. **Choose `orderByFields` from your grain + access pattern**: start with the grain columns, then add the most common filter/group-by fields.

Example: if your parity query looks like this:

```sql
SELECT
  merchant_id,
  merchant_name,
  day,
  total_orders,
  fulfilled_orders,
  fulfillment_rate
FROM OrderMetrics om
JOIN Merchants m ON m.id = om.merchant_id
WHERE om.merchant_id = {merchantId}
GROUP BY om.merchant_id, m.merchant_name, toDate(om.created_at)
```

Then your serving table should contain the final output columns, and `orderByFields` should match the grain (here, `["merchant_id", "day"]`).
```ts
// File: moosestack/src/models/OrderFulfillmentServing.model.ts

import { OlapTable, Int64 } from "@514labs/moose-lib";

interface OrderFulfillmentServing {
  merchant_id: string;
  merchant_name: string;
  day: Date;
  total_orders: Int64;
  fulfilled_orders: Int64;
  fulfillment_rate: number;
}
export const OrderFulfillmentServing = new OlapTable<OrderFulfillmentServing>("OrderFulfillmentServing", {
  orderByFields: ["merchant_id", "day"],
});
```

This table will be populated by the Materialized View and becomes the table your API reads from.


##### Step 2: Create the Materialized View

The Materialized View performs the joins/aggregations from your parity query and writes results to the serving table whenever source data changes.

```ts
// File: moosestack/src/views/OrderFulfillmentMV.view.ts

import { MaterializedView, sql } from "@514labs/moose-lib";
import { OrderMetrics } from "../models/OrderMetrics.model";
import { Merchants } from "../models/Merchants.model";
import { OrderFulfillmentServing } from "../models/OrderFulfillmentServing.model";

export const OrderFulfillmentMV = new MaterializedView<OrderFulfillmentServing>({
  selectStatement: sql`
    SELECT
      om.merchant_id,
      m.merchant_name,
      toDate(om.created_at) AS day,
      om.total_orders,
      om.fulfilled_orders,
      round(100 * om.fulfilled_orders / nullIf(om.total_orders, 0), 2) AS fulfillment_rate
    FROM ${OrderMetrics} om
    JOIN ${Merchants} m ON m.id = om.merchant_id
  `,
  targetTable: OrderFulfillmentServing,
  materializedViewName: "OrderFulfillmentMV",
  selectTables: [OrderMetrics, Merchants],
});
```

##### Step 3: Verify the MV is created

Save your files and watch the `moose dev` output:

```
[INFO] Created table: OrderFulfillmentServing
[INFO] Created materialized view: OrderFulfillmentMV
```

If you see errors, check:
- Column names match between SELECT and destination table
- Data types are compatible
- Source table exists

##### Step 4: Validate data in the serving table
The `moose dev` server will automatically populate the serving table when you create or update a Materialized View by running the `selectStatement` defined in the Materialized View. It is not necessary to backfill the serving table manually. 

Use `moose query` to verify the serving table is populated:

```bash
# Check row count in serving table
moose query "SELECT count() FROM OrderFulfillmentServing"

# Spot-check a specific merchant
moose query "SELECT * FROM OrderFulfillmentServing WHERE merchant_id = '123' ORDER BY day DESC LIMIT 5"
```

At this stage you are only checking that the data is present. Once the Materialized View is created and the serving table is populated, you can proceed to Phase 3. Phase 3 will wire the API to read from the serving table.

#### Phase 3: Serve the Materialized View to your frontend

In this phase, you will expose the serving table via an API endpoint that your frontend can call.

In this guide, you‚Äôll use the Query Layer. It‚Äôs a small helper library that sits on top of Moose‚Äôs `sql` utility and `OlapTable` objects so you can define metrics, group-bys, filters, sorting, and pagination once and reuse them across handlers.

##### Step 1: Add the Query Layer to your project

Run the following command to copy/paste the Query Layer source code into your project:

```bash
cd moosestack
pnpm dlx tiged 514-labs/query-layer/src ./query-layer
```

##### Step 2: Define a QueryModel for your serving table

Define a QueryModel that exposes the dimensions, metrics, filters, and sortable fields your endpoint needs:

```ts
// File: moosestack/src/query-models/orderFulfillment.model.ts

import { sql } from "@514labs/moose-lib";
import { defineQueryModel } from "../query-layer";
import { OrderFulfillmentServing } from "../models/OrderFulfillmentServing.model";

export const orderFulfillmentModel = defineQueryModel({
  table: OrderFulfillmentServing,

  // Dimensions: fields users can group by
  dimensions: {
    merchantName: { column: "merchant_name" },
    day: { column: "day" },
  },

  // Metrics: aggregations users can request
  metrics: {
    totalOrders: { agg: sql`sum(total_orders)` },
    fulfilledOrders: { agg: sql`sum(fulfilled_orders)` },
    fulfillmentRate: {
      agg: sql`round(100 * sum(fulfilled_orders) / nullIf(sum(total_orders), 0), 2)`
    },
  },

  // Filters: fields users can filter on
  filters: {
    merchantId: { column: "merchant_id", operators: ["eq", "in"] as const },
    day: { column: "day", operators: ["gte", "lte"] as const },
  },

  // Sortable: fields users can sort by
  sortable: ["merchantName", "day", "totalOrders", "fulfillmentRate"] as const,
});
```

##### Step 3: Use the QueryModel in your handler

Update your existing endpoint handler to build a Query Layer request from your endpoint contract and execute it against ClickHouse:

```ts
// File: moosestack/src/apis/orderFulfillment.api.ts

import { getMooseUtils } from "@514labs/moose-lib";
import { orderFulfillmentModel } from "../query-models/orderFulfillment.model";

interface OrderFulfillmentRequest {
  merchantId?: string;
  startDate?: string;
  endDate?: string;
  limit?: number;
  offset?: number;
  orderBy?: "merchantName" | "day" | "totalOrders" | "fulfillmentRate";
}

export async function getOrderFulfillment(
  request: OrderFulfillmentRequest,
) {
  const { client } = await getMooseUtils();

  return await orderFulfillmentModel.query(
    {
      dimensions: ["merchantName", "day"],
      metrics: ["totalOrders", "fulfilledOrders", "fulfillmentRate"],
      filters: {
        ...(request.merchantId && { merchantId: { eq: request.merchantId } }),
        ...(request.startDate && { day: { gte: request.startDate } }),
        ...(request.endDate && { day: { lte: request.endDate } }),
      },
      sortBy: request.orderBy ?? "merchantName",
      sortDir: "ASC",
      limit: request.limit ?? 100,
      offset: request.offset ?? 0,
    },
    client.query,
  );
}
```

##### Step 4: Test the endpoint locally with Phase 1 test cases

Rewire your existing API endpoint to call the new function from Step 3. Make sure to import the function from the `moosestack` package.

```ts
// File: moosestack/src/apis/orderFulfillment.api.ts

import { getOrderFulfillment } from "moosestack";
// ... existing code ...

  const clickhouseResult = await getOrderFulfillment(request);

  // Replace this with how your existing API endpoint returns the response
  return {
    ...existingResponse,
    data: clickhouseResult, // Now returns your ClickHouse results
  };
```

Now send a `curl` request to that endpoint using a saved Phase 1 test case request, and save the response:

```bash
# Test with a test case request
curl -X POST http://localhost:4000/api/order-fulfillment \
  -H "Content-Type: application/json" \
  -d @test-case-1.request.json \
  | jq -S '.' > localhost-result.json
```

Compare the response to your original test case response body:

```bash
diff <(jq -S '.' test-case-1.response.json) <(jq -S '.' localhost-result.json)
```

The output must match your original test cases exactly.

##### Step 5: Verify performance improvement

Measure response time for your heaviest test case (replace `test-case-heaviest.json` with the file you want to benchmark):

```bash
time curl -s -X POST http://localhost:4000/api/order-fulfillment \
  -H "Content-Type: application/json" \
  -d @test-case-heaviest.request.json
```

Verify the response time is significantly improved compared to the original OLTP query.

### Going to production

In this section, you'll apply your local changes to production:

1. Commit your branch and deploy a preview environment
2. Test the preview branch
3. Push to production
4. Backfill materialized views

<Callout type="info" title="Confirm Boreal project access">
To deploy to production (and to create branch preview environments), you need a Boreal project connected to the GitHub repo you‚Äôre working in. If you haven‚Äôt created a project yet, go back to [**Create your Boreal project**](#create-your-boreal-project-production-access) above and complete that setup first.
</Callout>

#### Generate a Migration Plan

Return to your IDE and confirm the following before moving on:

* New queries and materialized views run locally successfully
* `moose dev` starts without errors
* All relevant APIs return the expected results

If all three checks pass, you're ready for the final pre-production step: ensuring your changes can deploy without breaking production. The migration plan shows exactly which database tables and views will be created, modified, or deleted. Review it to catch destructive changes (like accidental table drops) before they cause data loss.

Open your terminal (ensure you `cd` to your MooseStack project root). Then run:

**Command:**
```bash
moose generate migration --save --url <BOREAL_HOST> --token <BOREAL_ADMIN_API_BEARER_TOKEN>
```

**Parameters:**
* `BOREAL_HOST` is the host for your production deployment in Boreal. Copy it from the URL in your project overview dashboard:
  {/* image */}
* `BOREAL_ADMIN_API_BEARER_TOKEN` is sent in the request header when calling the Boreal Admin API at `BOREAL_HOST`. This is the API key. It is a secret and must not be committed to source control. Store it securely in a password manager.

After successfully running `moose generate migration` with the correct `--url` and `--token`, a new `/migrations` directory should appear at the root of your MooseStack project. Open the `plan.yaml` file in that directory and review it carefully.

#### Migration plan review

[Review the migration plan to confirm which SQL resources will be created or modified](https://docs.fiveonefour.com/moosestack/migrate/plan-format?). Make sure it matches exactly what you intend to ship. As a rule of thumb:

* Expect mostly new tables and materialized views
* Carefully review schema changes to existing tables
* Avoid deleting existing tables at all costs

**Look for new tables and materialized views**

This is expected when optimizing queries. Any new materialized view should result in:

* A [`CreateTable` operation](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#createtable) that creates the backing table for the view
* A [`SqlResource` operation](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#rawsql) containing the `CREATE MATERIALIZED VIEW` statement, with the view explicitly writing `TO` that backing table

Seeing both confirms the Materialized View is being added cleanly and additively. For every new Materialized View in your branch, there should be exactly one `CreateTable` and one `SqlResource` defining it.

**Watch closely for column changes**

[Column-level changes](https://docs.fiveonefour.com/moosestack/migrate/plan-format#column-operations) are uncommon. If you encounter them:

* Pause and confirm the change is intentional
* Double-check your code for queries that reference affected columns

There are a [small number of cases where column changes are expected:](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn)

* If you added a column to an existing materialized view, you should see a single [`AddTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn) operation applied to the backing table for that view.
* If you renamed a column, the plan may show a [`DropTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#droptablecolumn) followed by an [`AddTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn). If this rename was intentional, replace those two operations with a single [`RenameTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#renametablecolumn) operation instead.

Outside of these cases, column-level changes should be treated with caution, especially [`DropTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#droptablecolumn) or [`ModifyTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#modifytablecolumn) operations. These changes are strongly discouraged. Instead, stick to strictly additive migrations. Undo the delete or modification in your `OlapTable` object, and introduce a new column instead.

**Sanity-check for `DropTable` operations**

If you see any [`DropTable` operations](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#droptable), proceed with extreme caution and review your changes carefully. They may indicate that an `OlapTable` or `MaterializedView` object defined in the codebase (and currently used in production) is being deleted, which can result in irreversible data loss if applied unintentionally.

If the plan shows changes you did not anticipate, stop and resolve that before proceeding.

Once the plan looks correct, you're ready to continue with preview and production rollout.

### Push to remote GitHub

Add the files you created above (or just the functional files if you don't want to commit your test scripts) and push to your version control. Create a Pull Request.

This branch will later be used in Boreal for a Branch Deployment, automatically triggered by creating the PR. 

#### Open a Pull Request and Inspect the Preview Environment

At this point you should have:

- a reviewed `plan.yaml` that matches your intended changes, and
- a feature branch with your code changes ready to ship.

Commit your changes, push the branch to GitHub, and open a pull request targeting `main`.

When the PR is opened, Boreal creates an isolated preview environment for the branch. Your PR‚Äôs code and the generated `plan.yaml` are applied against a preview database so you can validate schema and behavior before you merge anything into `main`.

In the PR, confirm that **boreal-cloud bot** posts a comment.

{/* image */}

This confirms that:

- the repo is connected and authorized, and
- the preview deployment has started for this branch.

If the bot does not appear, double-check that the Boreal GitHub app is authorized for this repository and that the Boreal project is connected to the correct repo/root directory.

In the **boreal-cloud bot** comment, you‚Äôll see a table. Click the link in the **Project** column (the link text will match your branch name). This opens the Boreal dashboard with the preview environment selected.

{/* image */}

From here, inspect the preview environment‚Äôs database state and verify the resources created by your migration plan match what you reviewed in `plan.yaml` (tables, views, and materialized views) before proceeding.

#### Connect to the Staging Database

In this step, you'll query the staging ClickHouse database directly using ClickHouse's HTTPS interface.

First, get the database HTTPS connection string from Boreal using the same steps you followed earlier. Make sure the Boreal dashboard is set to your feature branch, not `main`. You can confirm this by checking the branch selector in the left sidebar of the project dashboard.

**Set the staging connection string locally**

Create a temporary environment variable for your staging database URL:

**Command:**
```bash
export STAGING_DB=<your-staging-db-connection>
```

You can now safely use `$STAGING_DB` to run queries against the staging database via `curl`.

**Inspect Staging Database Tables**

In a terminal, run:

**Command:**
```bash
curl -sS \
  $STAGING_DB \
  --data-binary 'SHOW TABLES'
```

**Expected Response:**

You should see a plain-text list of all tables in the staging database if the command executed successfully:

```
customers
products
orders
order_items
merchants
order_metrics_daily
...
```

Use this output to confirm that:

* All new tables and materialized views defined in `plan.yaml` exist
* No unexpected tables were created
* Existing tables remain unchanged unless explicitly intended

If the list of tables does not match what you reviewed in the migration plan, stop here and fix the issue before proceeding.

Do not merge until the preview environment reflects exactly the database resources and behavior you expect to see in production.

#### Merge PR to Deploy to Prod

If everything lines up as you expect, you're ready to merge!

Merge your PR and now do the same thing: click the Boreal bot to view the deployment page. You should see the logs from the deployment and status there. The deployment should take a few minutes.

#### Backfill new Materialized Views

If your migration introduces new Materialized Views, they populate only for new incoming data. Your dashboards will show incomplete or empty results for historical time ranges until you backfill. To make historical data available immediately, explicitly backfill the MVs from existing ClickHouse tables.

This step uses the same HTTPS + `curl` workflow as before, but targets the production (`main`) database and performs a write operation to apply the backfill.

**Identify which materialized views need a backfill**

Open the migration `plan.yml` you just shipped and find the new materialized views you created (look for the `CREATE MATERIALIZED VIEW` statements in `SqlResource`).

For each one, note two things:

* the materialized view name
* the backing table name it writes `TO`

**Run the backfill (one MV at a time)**

Backfilling is done by inserting historical rows into the MV's backing table using the same `SELECT` logic used by the view.

In a terminal, run:

**Command:**
```bash
curl -sS \
  '$BOREAL_CONNECTION_STRING' \
  --data-binary "
  INSERT INTO <mv_backing_table>
  SELECT ...
  "
```

Use the exact `SELECT` statement from the `CREATE MATERIALIZED VIEW` definition (or the underlying `SELECT` you used when building it) and paste it in place of `SELECT ...`.

**Confirm the backfill worked**

After each backfill, sanity check that the backing table now has rows:

**Command:**
```bash
curl -sS \
  $BOREAL_CONNECTION_STRING \
  --data-binary 'SELECT count() FROM <mv_backing_table>'
```

**Expected Response:**

If the count is non-zero (and roughly matches what you expect), the backfill is complete.

**Repeat for each new MV you added**

Only backfill the MVs introduced in this change. Avoid reprocessing older MVs unless you intentionally want to rebuild them.
## Appendix

:::include /shared/prerequisites/wsl-setup.mdx