---
title: Improving the Performance of Your Dashboards
description: Learn how to build blazing-fast dashboards using MooseStack and ClickHouse
---

import { CTACards, CTACard, Callout, BulletPointsCard } from "@/components/mdx";

# Improving the Performance of Your Dashboards

<Callout type="info" title="TL;DR">
* Slow dashboards are usually an architecture problem, not a charting problem.  
* If you run OLAP-style queries on your OLTP database, performance (and reliability) eventually degrades.  
* You can get 10‚Äì100x faster dashboards by moving analytics to ClickHouse incrementally‚Äîwithout rewriting your frontend.

This guide will:

1. Help you recognize when OLTP has become your analytics bottleneck  
2. Show an incremental path to move analytical workloads to OLAP best practices  
3. Walk through shipping the change end-to-end with MooseStack + ClickHouse
</Callout>

<CTACards columns={2}>
  <CTACard
    title="Overview"
    description="Business context, warning signs, and what success looks like"
    ctaLink="#overview"
    ctaLabel="Read first"
    Icon="Eye"
  />
  <CTACard
    title="Tutorial"
    description="Step-by-step: port dashboard logic to ClickHouse via MooseStack"
    ctaLink="#tutorial"
    ctaLabel="Start building"
    Icon="Rocket"
  />
</CTACards>

## Overview

### What this guide is for

This guide is for product engineering teams that already have customer-facing dashboards/reports in production, where:

<BulletPointsCard
  title="Common symptoms"
  bulletStyle="bullet"
  bullets={[
    {
      title: "Charts/metrics take >5s to load",
      description: "Your users are staring at loading spinners."
    },
    {
      title: "Small report changes are expensive",
      description: "Updates are risky enough that they stay stuck in the backlog."
    },
    {
      title: "Reporting traffic overwhelms production OLTP",
      description: "Analytics queries cause collateral damage to core app workflows."
    }
  ]}
/>

In most cases, the root cause is that you‚Äôre running analytical query patterns (group by / aggregations / wide scans / joins) on the same transactional database that powers the rest of your application.

This guide lays out a step-by-step path to move those analytical workloads to OLAP using best practices, **without redesigning your product or rewriting your frontend**.

### Why your business needs to solve this

Customer-facing analytics stops being optional once users depend on it to understand their behavior, progress, or outcomes. When dashboards are slow or unreliable, engagement drops.

<Callout type="info" title="Case study: F45‚Äôs LionHeart experience">
LionHeart is where F45‚Äôs most engaged members track workout performance and progress. As usage and data volume increased, the original setup‚Äîrunning analytics directly on the core transactional database‚Äîbecame a bottleneck. Reports were shipped to users as static images of charts. There was no interactivity, and improving the experience carried too much risk to justify prioritization. The team wanted dynamic, interactive dashboards for years, but the existing architecture kept the work stuck in the backlog.

F45 migrated the LionHeart analytics backend to an OLAP-based architecture using the Fiveonefour stack in a matter of weeks. The mobile app moved from static chart images to fast, interactive visualizations. This led to tangible improvements in the bottom line metrics: [70% increase in user satisfaction, and over 1 full star increase in the F45 App Store rating](https://www.fiveonefour.com/blog/case-study-f45).

</Callout>

### Why you haven‚Äôt solved it yet

Normally teams don‚Äôt start with OLAP because it‚Äôs rational to delay it:

<BulletPointsCard
  title="Why teams delay adopting OLAP (and why it makes sense early on)"
  bulletStyle="bullet"
  bullets={[
    {
      title: "The OLTP path is the highest-ROI path‚Äîat first",
      description: "Early on, the transactional workload is mission-critical and easiest to justify optimizing."
    },
    {
      title: "Performance doesn‚Äôt degrade right away",
      description: "OLTP-backed analytics can look ‚Äúfine‚Äù until data volume and concurrency cross a threshold (often ~10M rows)."
    },
    {
      title: "Shipping the first version is fastest on existing infrastructure",
      description: "The quickest path to value is usually building reporting directly on the systems you already run and understand."
    },
    {
      title: "A second database is a real operational commitment",
      description: "Adding OLAP introduces new reliability, cost, security, and ownership concerns‚Äînot just a new query engine."
    },
    {
      title: "Most OLAP stacks weren‚Äôt built for modern software engineering workflows",
      description: "Tooling can feel data-engineering-native, slowing adoption when the builders are primarily software engineering teams building on a application stack (e.g., Next.js, React, Ruby on Rails, etc.)."
    }
  ]}
/>

This is why it's common to push OLTP database migrations until it becomes a real business risk. The issue is timing. OLAP migrations are non-trivial and routinely take months. If you wait too long and/or choose tooling with a steep learning curve, you may not move fast enough to fix the problem before users start churning.

### When to pull the trigger

You should seriously consider this shift once the warning signs appear in day-to-day product and engineering work. It usually starts small: dashboards feel a bit laggy, then noticeably slow as data grows. Users want more filters, breakdowns, or longer time ranges, but engineering can‚Äôt safely ship improvements because even small changes could jeopardize core transactional workloads the rest of your product depends on. Before long, customers complain about load times, and those complaints surface in retention metrics, support tickets, and sales calls. Throwing more hardware at the problem either isn‚Äôt possible or is prohibitively expensive; even when feasible, this approach only buys temporary relief. The underlying issue is certain to resurface.

### What this guide proposes

This guide outlines an incremental path for delivering **10‚Äì100x faster** dashboard load times without putting your core product at risk: move expensive analytical queries from your existing, mission-critical transactional database to a purpose-built analytical database **without changing your existing frontend UI or business logic**.

This approach slots right into how your engineering teams already operate. Changes are written in TypeScript, live in the same codebase as the rest of the application, and move through the same version-controlled review and release process. Everything can be validated in staging environments before shipping to production. Compared to a full-stack refactor or a data-team-led migration, this approach gets to production faster, with less risk, and provides a controlled way to scale analytics over time.

### What success looks like

When this works, the impact is immediate for both product and engineering. As data volume and usage grow, dashboards stay fast and predictable instead of degrading over time. Analytical workloads no longer compete with core transactional traffic, so the rest of your application runs more reliably. Engineering can ship new dashboards, metrics, and breakdowns as routine product work, not risky, backlog-bound projects. With performance and reliability handled, teams can ship rich, interactive data experiences across the product at low incremental cost and drive measurable improvements in engagement, retention, and revenue upside.


## Tutorial

This guide assumes that you have a dashboard or report up and running, and that you wish to improve the performance of your dashboard by leveraging OLAP best practices. It will guide you through:

1. getting your local dev environment up and running to work on this problem,   
2. the local development experience of taking the business logic you have powering your frontend dashboards / reports and porting them over to MooseStack / ClickHouse  
3. shipping your changes to production on Boreal Cloud

This guide proposes a AI copilot-centric approach for doing this, but each of these steps can be done manually as well. This guide also assumes that you have Visual Studio Code as your IDE, [configured with Github Copilot](https://code.visualstudio.com/docs/copilot/setup) as your coding assistant. 

This guide is written for a TypeScript developer. If you are interested in a Python guide, let us know!

# Setting up your development environment

This section will guide you through: 

* Installing MooseStack and its dependencies  
* Initiating or cloning your MooseStack project  
* Setting up developer / agent supporting services (MCP \+ Language Server)  
* Seeding your development environment with sample data 

## Installing MooseStack

### Prerequisites:

* Node.js 20+: [https://nodejs.org/en/download](https://nodejs.org/en/download)   
* Docker Desktop: [https://docs.docker.com/desktop/](https://docs.docker.com/desktop/)   
* MacOS, Linux, or WSL 2+ with Windows

### Installing on Windows:

[See appendix for instructions on setting up WSL2](#wsl:-setting-up-a-wsl2-linux-instance-to-run-moosestack-on-windows) if you do not already have a Linux box running on your windows machine.

### Install MooseStack:

```shell
bash -i <(curl -fsSL https://fiveonefour.com/install.sh) moose
```

**Make sure to restart your terminal after installing MooseStack.**

## Initialize or clone your MooseStack project locally

You have two options for setting up your MooseStack project:

### Option A: Clone an existing repository

If you already have a repository with MooseStack configured:

```shell
git clone <YOUR_REPOSITORY_URL>
cd <your-project>
pnpm install
```

### Option B: Add MooseStack to an existing PNPM monorepo

If you have an existing PNPM monorepo (e.g., a Next.js app) and want to add MooseStack:

#### Step 1: Initialize MooseStack in a subdirectory

From your monorepo root:

```shell
mkdir moosestack
cd moosestack
moose-cli init . typescript
```

#### Step 2: Update your PNPM workspace configuration

Edit your root `pnpm-workspace.yaml` to include the `moosestack` directory:

```yaml
packages:
  - .                    # Your main app (e.g., Next.js)
  - moosestack                # MooseStack project
```

#### Step 3: Configure the MooseStack package

Edit `moosestack/package.json` to set it up as a workspace package:

```json
{
  "name": "moosestack",
  "private": true,
  "version": "0.0.1",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    }
  },
  "scripts": {
    "build": "moose-cli build",
    "dev": "moose-cli dev",
    "moose": "moose-cli"
  },
  "dependencies": {
    "@514labs/moose-lib": "latest",
    "typia": "^10.1.0"
  },
  "devDependencies": {
    "@514labs/moose-cli": "latest",
    "@types/node": "^24",
    "typescript": "^5"
  }
}
```

#### Step 4: Add the `pnpm` configuration to your root package.json

Edit your root `package.json` to add the `pnpm.onlyBuiltDependencies` section (this ensures required native/built dependencies are handled correctly by PNPM):

```json
{
  "pnpm": {
    "onlyBuiltDependencies": [
      "@confluentinc/kafka-javascript",
      "@514labs/kafka-javascript"
    ]
  }
}
```

## Run your Moose Development Server

Ensure Docker Desktop is running first.  

![Docker Desktop](/docker-desktop.png)


Navigate to your MooseStack project directory and run the development server:

```shell
cd moosestack  # or wherever your moose project is
pnpm install
moose dev
```


If the development server runs correctly, you'll see a list of the available API endpoints print:

```
üìç Available Routes:
  Base URL: http://localhost:4000

  Static Routes:
  METHOD  ENDPOINT                                           DESCRIPTION
  ------  --------                                           -----------
  GET     /admin/inframap                                    Admin: Get infrastructure map
  GET     /admin/reality-check                               Admin: Reality check - provides a diff when drift is detected between the running instance of moose and the db it is connected to
  GET     /health                                            Health check endpoint
  GET     /ready                                             Readiness check endpoint
...

```

And your local services will be running in Docker containers:  

![Docker Services](/docker-services.png)

## Setting up developer / agent supporting services

This subsection will run a couple of services that can help you and your copilots / agents work with the MooseStack project:

1. The [Moose Dev MCP server](https://docs.fiveonefour.com/moosestack/moosedev-mcp?lang=typescript): The Moose development server includes a built-in Model Context Protocol (MCP) server that enables AI agents and IDEs to interact directly with your local development infrastructure. This allows you to use natural language to query data, inspect logs, explore infrastructure, and debug your Moose project. In particular, tools like `get_infra_map`, `query_olap`, and `get_logs` will be useful in accelerating your path through this guide.   
2. [The Language Server (LSP) for MooseStack Typescript](https://github.com/514-labs/moosestack-lsp), which allows the IDE to validate the SQL written within MooseStack objects for correctness. (Coming soon, see appendix).

### Moose Dev MCP setup

To install the Moose Dev MCP with VS Code 1.102+, add this to your project's `.vscode/mcp.json` or User Settings:

```json
{
  "servers": {
    "moose-dev": {
      "type": "http",
      "url": "http://localhost:4000/mcp"
    }
  }
}
```

Confirm the server is running and integrated with VSCode by running `MCP: List Servers` command from the Command Palette.

* For other options to install with VSCode, see [the VSCode docs](https://code.visualstudio.com/docs/copilot/customization/mcp-servers#_add-an-mcp-server).  
* For installation instructions for other IDEs / AI coding assistants, like Claude Code, Windsurf, and Cursor, see the [MooseStack docs](https://docs-v2-git-vscode-mcp-docs.preview.boreal.cloud/moosestack/moosedev-mcp?lang=typescript).

# CDC guide 

This section walks you through how to set up your MSSQL database to integrate with Debezium for change data capture. Before you start, make sure you have the following credentials for your database:

- Host  
- Port  
- Username  
- Password  
- DB Name

This setup comprises 3 main steps:

1. Configure MSSQL Server for CDC  
2. Configure ingestion pipelines to land CDC events into ClickHouse tables (via MooseStack)  
3. Deploy Debezium Server to your environment

### CDC Architecture:

The following example shows how CDC tracks changes to a `customers` table:

1\. **App executes:**        `UPDATE dbo.customers SET status='active' WHERE customer_id=123`  
2\. **Transaction Log:**     Log record written (LSN: 00000042:00000123:0001, BEFORE: 'inactive', AFTER: 'active')  
3\. **Capture Job:**         Reads log, finds change to CDC-enabled table `dbo.customers`  
4\. **Tracking Table:**      `INSERT INTO cdc.dbo_customers_CT (__$operation=4, customer_id=123, status='active', ...)`  
5\. **Debezium:**            Polls tracking table, creates JSON: `{"op":"u", "before":{...}, "after":{"status":"active"}}`  
6\. **HTTP Sink:**           Receives POST with change event payload

## MSSQL Server CDC Configuration:

You'll need to first make sure your MSSQL Server is properly configured to work with Debezium.

### Step 0: Powershell SQL Query Command

Useful for executing commands throughout this guide.

**Command:**
```powershell
Invoke-Sqlcmd -ServerInstance "$host,$port" -Database "dbName" -Username "username" -Password "<YOUR_PASSWORD>" -TrustServerCertificate -Query $sql
```

**Single line $sql statement:**
```powershell
$sql = "SELECT 1"
```

**Multiline $sql statement:**
```powershell
$sql = @"
SELECT 1;
"@
```

### Step 1: Test Database Connection

Verify connectivity to the SQL Server instance.

**Command:**
```sql
SELECT 1;
```

**Expected Response:**
```
Column1
-------
      1
```

### Step 2: Enable CDC at the Database Level

This enables CDC infrastructure on the database (creates the `cdc` schema, system tables, and jobs).

**Command:**
```sql
EXEC sys.sp_cdc_enable_db;
```

**Expected Response:**
```
Command(s) completed successfully.
```

### Step 3: Validate CDC System Tables Exist

Check that CDC system tables exist after enabling CDC.

**Command:**
```sql
-- Check if cdc schema and tables exist
SELECT 
    s.name AS SchemaName,
    t.name AS TableName,
    t.create_date AS CreatedDate
FROM sys.tables t
JOIN sys.schemas s ON t.schema_id = s.schema_id
WHERE s.name = 'cdc'
  AND t.name IN ('change_tables', 'captured_columns', 'ddl_history', 
                 'index_columns', 'lsn_time_mapping')
ORDER BY t.name;
```

**Expected Response:**
```
SchemaName   TableName           CreatedDate
-----------  ------------------  ----------------------
cdc          captured_columns    2025-12-16 16:48:50
cdc          change_tables       2025-12-16 16:48:50
cdc          ddl_history         2025-12-16 16:48:50
cdc          index_columns       2025-12-16 16:48:50
cdc          lsn_time_mapping    2025-12-16 16:48:50
```

CDC System Tables:

| Table | Purpose |
| :---- | :---- |
| `cdc.change_tables` | Registry of all CDC-enabled tables and their capture instances |
| `cdc.captured_columns` | Lists which columns are being tracked for each capture instance |
| `cdc.ddl_history` | Records DDL changes (ALTER TABLE, etc.) on CDC-enabled tables |
| `cdc.index_columns` | Stores index column information for tables with net changes support |
| `cdc.lsn_time_mapping` | Maps LSN values to commit timestamps (for time-based queries) |

### Step 4: Enable CDC at the Table Level

Enable CDC for each table you want to track. The command differs based on whether the table has a primary key.

#### Option A: Table with Primary Key

**Command:**
```sql
EXEC sys.sp_cdc_enable_table 
    @source_schema = N'dbo', 
    @source_name = N'customers',  -- Replace with your table name
    @role_name = NULL, 
    @supports_net_changes = 1;    -- Requires primary key
```

**Expected Response:**
```
Job 'cdc.dbo_customers_capture' started successfully.
Job 'cdc.dbo_customers_cleanup' started successfully.
```

#### Option B: Table without Primary Key

**Command:**
```sql
EXEC sys.sp_cdc_enable_table 
    @source_schema = N'dbo', 
    @source_name = N'TableName',  -- Replace with your table name
    @role_name = NULL, 
    @supports_net_changes = 0;    -- No primary key exists
```

**Expected Response:**
```
Job 'cdc.dbo_TableName_capture' started successfully.
Job 'cdc.dbo_TableName_cleanup' started successfully.
```

**Note:** Above commands generate the CDC tracking table with the format: `cdc.<schema>_<table>_CT`

### Step 5: Verifying Tables with CDC Enabled

Verify which tables now have CDC enabled.

**Command:**
```sql
SELECT 
    s.name AS SchemaName, 
    t.name AS TableName 
FROM sys.tables t 
JOIN sys.schemas s ON t.schema_id = s.schema_id 
WHERE t.is_tracked_by_cdc = 1 
ORDER BY s.name, t.name;
```

**Expected Response:**
```
SchemaName   TableName
-----------  ------------
dbo          customers
dbo          orders
dbo          order_items
dbo          products
```

### Step 6: Verify SQL Server Agent is Running

Confirm that SQL Server Agent is running (required for CDC to function).

**Command:**
```sql
EXEC xp_servicecontrol 'QueryState', 'SQLServerAGENT';
```

**Expected Response:**
```
Current Service State
---------------------
Running.
```

**‚ö†Ô∏è Critical:** SQL Server Agent MUST be running for CDC to capture changes!

If the service is not running, start it with:
```sql
EXEC xp_servicecontrol 'START', 'SQLServerAGENT';
```

### Step 7: Verify CDC Jobs

Verify that CDC capture and cleanup jobs were created.

**Command:**
```sql
EXEC sys.sp_cdc_help_jobs;
```

**Expected Response:**
```
job_id           : 9f9c5160-1bea-4b22-9818-50f16653be9e
job_type         : capture
job_name         : cdc.appdb_capture
maxtrans         : 500
maxscans         : 10
continuous       : True
pollinginterval  : 5
retention        : 0
threshold        : 0

job_id           : ee746d5b-9c6d-471c-97e7-757ad9418341
job_type         : cleanup
job_name         : cdc.appdb_cleanup
maxtrans         : 0
maxscans         : 0
continuous       : False
pollinginterval  : 0
retention        : 4320
threshold        : 5000
```

**What this shows:**
- **Capture job**: Continuously monitors transaction log for changes
- **Cleanup job**: Periodically removes old CDC data (retention = 4320 minutes = 3 days)

#### Example: CDC Tracking Table Schema

Source Table (E-commerce customers table):

```sql
CREATE TABLE dbo.customers (
    customer_id     INT PRIMARY KEY,
    customer_name   NVARCHAR(100),
    status          NVARCHAR(20),
    created_at      DATETIME,
    updated_at      DATETIME
);
```

CDC Tracking Table: `cdc.dbo_customers_CT`

```sql
-- Auto-generated by SQL Server
CREATE TABLE cdc.dbo_customers_CT (
    -- CDC System Columns (always present)
    __$start_lsn        BINARY(10)      NOT NULL,  -- LSN when change committed
    __$end_lsn          BINARY(10)      NULL,      -- Always NULL (reserved)
    __$seqval           BINARY(10)      NOT NULL,  -- Sequence within transaction
    __$operation        INT             NOT NULL,  -- 1=DEL, 2=INS, 3=UPD(before), 4=UPD(after)
    __$update_mask      VARBINARY(128)  NULL,      -- Bitmask of changed columns
    
    -- Your Table Columns (copied from source)
    customer_id         INT,
    customer_name       NVARCHAR(100),
    status              NVARCHAR(20),
    created_at          DATETIME,
    updated_at          DATETIME
);
```

## CDC via Pushing to Moose Ingest API

This section describes the CDC architecture used in our demo environment, where network constraints prevent Debezium from connecting directly to Kafka. In environments without these constraints, CDC can be configured to stream changes directly from Debezium into Kafka topics instead.

In this architecture, all changes from your MSSQL database tables are captured using Debezium and sent to a single, Moose-managed Kafka (or Redpanda) Stream via a Moose-managed HTTP Ingest API endpoint *(there is no per-table endpoint or per-table ingest stream at this stage)*.

From there, CDC events are explicitly fanned out based on their metadata. Each MSSQL source table ultimately maps to:

* one table-specific Moose Stream, and  
* one downstream ClickHouse landing table.

This architecture implements a [Streaming Function](https://docs.fiveonefour.com/moosestack/streaming/consumer-functions?lang=typescript) that inspects each raw CDC event from the shared stream to identify the source table and routes it to the appropriate table-specific stream. Each of these streams feeds a corresponding ClickHouse table.

To summarize, the high level data flow is:

* Debezium ‚Üí single ingest API endpoint  
* Ingest API ‚Üí shared CDC stream  
* Streaming function ‚Üí table-specific streams  
* Table-specific streams ‚Üí ClickHouse tables

### Step 0: Prerequisites

Before continuing, confirm:

* MSSQL Server CDC is enabled and working  
* MooseStack is running locally  
* You can reach the Moose Ingest API from your Debezium environment  
* Port 443 is available for egress

If CDC is not enabled yet, complete the **MSSQL Server CDC Configuration** section first.

### Step 1: Add a Moose Ingest API Endpoint for CDC Events

This endpoint will receive CDC events from Debezium. 

In your MooseStack project, create a new Ingest API sink that accepts Debezium `ChangeEvent` payloads:

**File location:** `moosestack/src/cdc/DebeziumChangeEvent.model.ts`

```ts
import { IngestApi, Stream, DeadLetterQueue, Int64 } from "@514labs/moose-lib";

/**
 * Debezium SQL Server Change Event Structure
 * Documentation: https://debezium.io/documentation/reference/stable/connectors/sqlserver.html
 *
 * This model represents the standardized event structure sent by Debezium
 * for all change data capture events from SQL Server.
 */

export interface DebeziumSource {
  version: string;
  connector: string;
  name: string;
  ts_ms: Int64; // Timestamp in milliseconds (epoch)
  snapshot?: string;
  db: string;
  sequence?: string;
  schema: string;
  table: string;
  change_lsn?: string;
  commit_lsn?: string;
  event_serial_no?: number;
}

export interface DebeziumTransaction {
  id?: string;
  total_order?: number;
  data_collection_order?: number;
}

/**
 * Main Debezium Change Event Payload
 * The 'before' and 'after' fields contain the actual row data
 * and their shape varies by table, so we use Record<string, any>
 */
export interface DebeziumChangeEvent {
  // Row data before the change (null for INSERT operations)
  before?: Record<string, any> | null;

  // Row data after the change (null for DELETE operations)
  after?: Record<string, any> | null;

  // Source metadata identifying where this change came from
  source: DebeziumSource;

  // Operation type: 'c' = create, 'u' = update, 'd' = delete, 'r' = read (snapshot)
  op: string;

  // Timestamp in milliseconds
  ts_ms: Int64;

  // Transaction metadata (optional)
  transaction?: DebeziumTransaction | null;
}

/**
 * Full Debezium Event Envelope (what actually gets POSTed by Debezium)
 * Debezium sends events with both schema and payload wrapped together
 */
export interface DebeziumEventEnvelope {
  schema?: Record<string, any>;
  payload: DebeziumChangeEvent;
}

/**
 * Stream for CDC events - fans out to table-specific streams via streaming function
 */
export const DebeziumChangeEventStream = new Stream<DebeziumEventEnvelope>("DebeziumChangeEvent");

/**
 * Dead Letter Queue for failed Debezium events
 */
export const DebeziumChangeEventDLQ = new DeadLetterQueue<DebeziumEventEnvelope>(
  "DebeziumChangeEvent_DLQ"
);

/**
 * Ingestion API endpoint for Debezium CDC events
 * Creates: POST /ingest/DebeziumChangeEvent
 *
 * Debezium sends events here, which flow through the streaming function
 * to fan out to table-specific Redpanda topics.
 */
export const DebeziumChangeEventIngestApi = new IngestApi<DebeziumEventEnvelope>(
  "DebeziumChangeEvent",
  {
    destination: DebeziumChangeEventStream,
    deadLetterQueue: DebeziumChangeEventDLQ,
  }
);
```

### Step 2: Map Source Tables to Moose Streams

Before implementing the streaming function, define an explicit mapping between MSSQL source table names and their corresponding Moose Streams.

**File location:** `moosestack/src/cdc/tableStreamMap.ts`

```typescript
import { ProductStream } from "../models/Product.model";
import { CustomerStream } from "../models/Customer.model";
import { OrderStream } from "../models/Order.model";
import { OrderItemStream } from "../models/OrderItem.model";

export const TABLE_STREAM_MAP: Record<string, any> = {
  products: ProductStream,
  customers: CustomerStream,
  orders: OrderStream,
  order_items: OrderItemStream,
};
```

This mapping is what makes the fan-out deterministic and ensures each source table's changes flow through the correct stream and into the correct ClickHouse table.

### Step 3: Add Streaming Function to the API Endpoint

This function acts as our fanout point.

When cdc events are posted from Debezium, we need to read the table name from the payload and route the cdc event to the correct stream.

**File location:** `moosestack/src/cdc/processDebeziumEvent.ts`

```ts
import {
  DebeziumEventEnvelope,
  DebeziumChangeEventStream,
} from "./DebeziumChangeEvent.model";
import { TABLE_STREAM_MAP } from "./tableStreamMap";

/**
 * Process and route CDC events to table-specific Redpanda topics
 *
 * ReplacingMergeTree CDC fields:
 * - ts_ms: Version column from payload.ts_ms (used to determine newest row)
 * - isDeleted: 1 for delete operations, 0 otherwise (ReplacingMergeTree collapses deleted rows)
 */
export default async function processDebeziumEvent(envelope: DebeziumEventEnvelope): Promise<void> {
  console.log(`[CDC] Processing event: ${JSON.stringify(envelope)}`);

  const event = envelope.payload;
  const { source, op, before, after, ts_ms } = event;

  const sourceTable = source.table;
  const targetStream = TABLE_STREAM_MAP[sourceTable];

  // Unknown table - log and skip
  if (!targetStream) {
    console.warn(`[CDC] Unknown table: ${sourceTable}`);
    return;
  }

  // Determine data and deleted flag based on operation type
  let rowData: Record<string, any> | null = null;
  let isDeleted: number = 0;

  switch (op) {
    case "c": // CREATE
    case "r": // READ (snapshot)
    case "u": // UPDATE
      rowData = after ?? null;
      isDeleted = 0;
      break;
    case "d": // DELETE - use 'before' data since 'after' is null for deletes
      rowData = before ?? null;
      isDeleted = 1;
      break;
    default:
      console.warn(`[CDC] Unknown op: ${op} for ${sourceTable}`);
      return;
  }

  if (!rowData) {
    console.warn(`[CDC] No data in ${op} event for ${sourceTable}`);
    return;
  }

  // Add CDC metadata columns for ReplacingMergeTree
  // Ensure isDeleted is explicitly UInt8 (0 or 1) for ClickHouse
  // Use bitwise OR with 0 to ensure it's an integer, not Float64
  const data = {
    ...rowData,
    ts_ms: ts_ms, // Version column - determines which row is newest
    isDeleted: (isDeleted | 0) as 0 | 1, // isDeleted flag - 1 for deletes, 0 otherwise (UInt8)
  };

  // Publish directly to table's Redpanda topic
  try {
    await targetStream.send(data);
    console.log(
      `[CDC] ${op.toUpperCase()} ${sourceTable} ‚Üí Redpanda topic (ts_ms=${ts_ms}, isDeleted=${isDeleted})`
    );
  } catch (error: any) {
    console.error(`[CDC] Failed to publish ${sourceTable}:`, error.message);
    throw error; // Trigger DLQ
  }
}

// Wire up the streaming function
DebeziumChangeEventStream.addConsumer(processDebeziumEvent);
```

## Deploy Debezium Server via K8s Manifests

Placeholders to replace:

* `<YOUR_DATABASE_NAME>` - your MS SQL database name (required)  
* `<YOUR_HTTP_SINK_URL>` - your HTTP sink endpoint URL (required)  
* `<YOUR_MSSQL_PASSWORD>` - your MS SQL SA password (required)

### Step 0: Namespace

`kubectl apply -f namespace.yaml`

```yaml
# namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: poc-debezium
  labels:
    name: poc-debezium
    app: debezium-kafka-connect
```

### Step 1: Config Map

`kubectl apply -f configmap.yaml`

```yaml
# configmap.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: debezium-server-config
  namespace: poc-debezium
data:
  application.properties: |
    # Debezium Server Source - MS SQL Server
    debezium.source.connector.class=io.debezium.connector.sqlserver.SqlServerConnector
    debezium.source.offset.storage.file.filename=/debezium/data/offsets.dat
    debezium.source.offset.flush.interval.ms=10000

    # MS SQL Server Connection
    debezium.source.database.hostname=mssql.poc-mssql.svc.cluster.local
    debezium.source.database.port=1433
    debezium.source.database.user=sa
    debezium.source.database.password=<YOUR_MSSQL_PASSWORD>
    debezium.source.database.names=<YOUR_DATABASE_NAME>
    debezium.source.database.encrypt=false
    debezium.source.database.trustServerCertificate=true

    # Server name - used as namespace for topic names and schema names
    debezium.source.topic.prefix=mssql
    debezium.source.database.server.name=mssql-server

    # Table whitelist
    debezium.source.table.include.list=*

    # Schema history - MS SQL Server CDC requires this
    # Use file-based schema history
    debezium.source.schema.history.internal=io.debezium.storage.file.history.FileSchemaHistory
    debezium.source.schema.history.internal.file.filename=/debezium/data/schema-history.dat

    # Snapshot mode
    debezium.source.snapshot.mode=schema_only

    # Snapshot optimization - conservative for low-impact CDC
    debezium.source.snapshot.max.threads=1
    debezium.source.snapshot.fetch.size=1000

    # Skip schema change events - only emit data changes
    debezium.source.include.schema.changes=false

    # HTTP Sink Configuration
    debezium.sink.type=http
    debezium.sink.http.url=<YOUR_HTTP_SINK_URL>
    debezium.sink.http.timeout.ms=60000

    # HTTP Sink Retry Settings
    debezium.sink.http.retries=5
    debezium.sink.http.retry.interval.ms=3000

    # Connector Error Handling and Retries
    debezium.source.errors.max.retries=5
    debezium.source.errors.retry.delay.initial.ms=300
    debezium.source.errors.retry.delay.max.ms=10000

    # Consumer configuration - conservative for low-impact
    debezium.source.max.batch.size=2048
    debezium.source.poll.interval.ms=120000

    # Queue capacity - reduced for lower memory footprint
    debezium.source.max.queue.size=8192
    debezium.source.max.queue.size.in.bytes=104857600

    # JSON format configuration
    debezium.format.key=json
    debezium.format.value=json
    debezium.format.value.json.schemas.enable=false

    # Logging
    quarkus.log.level=INFO
    quarkus.log.category."io.debezium.server.http".level=DEBUG
    quarkus.log.console.json=false
```

### Step 2: Secret

`kubectl apply -f secret.yaml`

```yaml
# secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: debezium-secrets
  namespace: poc-debezium
type: Opaque
stringData:
  # The password for the MS SQL Server 'sa' user
  MSSQL_SA_PASSWORD: "<YOUR_MSSQL_PASSWORD>"
```

### Step 3: Stateful Set

`kubectl apply -f statefulset.yaml`

```yaml
# statefulset.yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: debezium-server
  namespace: poc-debezium
  labels:
    app: debezium-server
spec:
  serviceName: debezium-server
  replicas: 1
  selector:
    matchLabels:
      app: debezium-server
  template:
    metadata:
      labels:
        app: debezium-server
    spec:
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        runAsNonRoot: true
      initContainers:
        - name: fix-permissions
          image: busybox:latest
          command:
            - sh
            - -c
            - chmod -R 777 /debezium/data && chown -R 1001:1001 /debezium/data
          securityContext:
            runAsUser: 0
            runAsNonRoot: false
          volumeMounts:
            - name: data
              mountPath: /debezium/data
      containers:
        - name: debezium-server
          image: quay.io/debezium/server:2.7
          env:
            - name: MSSQL_SA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: debezium-secret
                  key: MSSQL_SA_PASSWORD
            - name: JAVA_OPTS
              value: "-Xms4g -Xmx4g -XX:+UseG1GC -XX:MaxGCPauseMillis=200"
          volumeMounts:
            - name: config
              mountPath: /debezium/conf
            - name: data
              mountPath: /debezium/data
          resources:
            requests:
              memory: "4Gi"
              cpu: "2000m"
            limits:
              memory: "4Gi"
              cpu: "2000m"
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - test -e /proc/1/exe
            initialDelaySeconds: 120
            periodSeconds: 60
            timeoutSeconds: 10
            failureThreshold: 5
      volumes:
        - name: config
          configMap:
            name: debezium-server-config
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
```

### Step 4: Service

`kubectl apply -f service.yaml`

```yaml
# service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: debezium-server
  namespace: poc-debezium
  labels:
    app: debezium-server
spec:
  selector:
    app: debezium-server
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP
```

# Local development and testing

This section will guide you through the process of selecting a dashboard/report component to be improved, through:

* Creating a development branch locally (which will also be used by Boreal for branch preview environments)  
* Setting up CDC for the supporting tables (guide coming soon)  
* Setting up local context for use in developments  
  * Seeding your local development environment from the production database  
  * Bringing other context into the local environment  
* Creating, iterating on and testing the replication of dashboard elements in MooseStack

## Create a branch

A branch is used not just for managing your code versioning, but also for spinning up "preview branches" in Boreal.

```bash
git checkout -b <new-branch-name>
```

## Seeding your local database from prod with `moose seed`

[You can seed your local development environment with the `moose seed` command](https://docs.fiveonefour.com/moosestack/moose-cli?lang=typescript#seed-clickhouse).

**Command:**
```shell
moose seed clickhouse --connection-string <BOREAL_CONNECTION_STRING> --limit 100
```

You can set the limit to whatever amount of data you want to bring locally, or, alternatively, you can copy all the data from remote to local (be careful with this approach):

**Command:**
```shell
moose seed clickhouse --connection-string <BOREAL_CONNECTION_STRING> --all
```

### Finding your connection string in Boreal

`BOREAL_CONNECTION_STRING` is your connection string to a remote/cloud database. In Boreal, you can find that here:  
{/* image  */}

{/* image */}  
Note, there are a few forms of connection string, this query requires `HTTPS URL`.

You can set your Boreal connection string as a local environment variable for ease of use:

**Command:**
```shell
echo 'BOREAL_CONNECTION_STRING="PASTE_THE_HTTPS_URL_FROM_BOREAL_HERE"' >> .env.local && set -a && source .env.local && set +a
```

**Or export directly:**
```bash
export BOREAL_CONNECTION_STRING=PASTE_THE_HTTPS_URL_FROM_BOREAL_HERE
```

## Improving dashboard performance by replicating components in MooseStack

This section walks through migrating an existing, OLTP-backed dashboard/report to an OLAP-backed implementation (ClickHouse + MooseStack) **without changing how the frontend talks to the backend**.

The approach in this guide is to swap out the *query + storage layer* (ClickHouse) while keeping the rest of your application the same: routing, auth, request/response contracts, and frontend behavior.

You‚Äôll implement **new OLAP query handlers** in your MooseStack project‚Äîone per dashboard component/report you migrate. Then, in your existing application backend, you‚Äôll repoint the endpoint to call the new OLAP handler instead of the original OLTP implementation.

This is designed to be done with AI: you give your copilot the right context and example data for your "golden case" inputs and expected outputs, and it produces the translation and a plan, and you verify correctness at each step‚Äîso you get AI speed without scattering one-off ClickHouse SQL across your codebase.

This guide follows a three-phase migration pattern:

1. **Parity (correctness)**: reproduce existing API behavior on ClickHouse by translating the underlying OLTP query logic into ClickHouse SQL  
2. **Optimize (OLAP best practices)**: shift OLTP-style read patterns (joins/reshaping/stored-procedure-like logic) to write time using Materialized Views  
3. **Serve (stable interface)**: repoint your Query Layer `QueryModel` at MV-backed serving tables, so handlers stay thin and dashboards stay fast  


### Choose the component you want to migrate

Pick a specific dashboard component or report to migrate. You‚Äôll create one context pack per component.

For this guide, we‚Äôll use an e-commerce order fulfillment report:

- Dataset: `OrderFulfillment`  
- Purpose: Track order processing metrics by merchant and time period  

### Assemble the context for your AI copilot

This workflow uses the starter kit in `examples/dashboard-migration/` to keep your ‚Äúcontext pack‚Äù and prompt consistent across migrations.

1. Copy the starter kit into your project (creates `context/migrations/`):

```shell
pnpm dlx tiged 514-labs/moosestack/examples/dashboard-migration context/migrations
```

2. Duplicate the context pack template for your component:

```shell
cp context/migrations/component-context-pack-template.md context/migrations/COMPONENT_NAME.md
```

3. Fill out `context/migrations/COMPONENT_NAME.md` with:

- **The existing API contract**, including the API route name/path and request/response schema (OpenAPI, TypeScript types, JSON schema). 
- *Data for test cases*: 2-5 representative example API requests and their expected responses for the most important use cases that you want to validate against. If you can, copy paste the JSON request/response payloads directly into the file.
- **Current OLTP implementation (what you‚Äôre replacing)**: links/file paths to the frontend caller + backend handler, plus any supporting SQL artifacts it relies on (tables/views/stored procedures)

4. Prompt your copilot to translate the component using the context pack.

The starter prompts you added to your project includes a prompt template that you can copy and paste into your copilot. It is located at `context/migrations/prompt.md`. Replace placeholders with the file path of the component-specific context file you created in step 3, and paste it into your copilot. Make sure it references your filled file via `@context/migrations/COMPONENT_NAME.md`.

**Important:** don‚Äôt paste the prompt until the context pack is filled out. The copilot should treat `@context/migrations/COMPONENT_NAME.md` as the source of truth.

If you want a more detailed copilot workflow (refinement loop + testing patterns), see [Appendix: Copilot workflow for parity migrations](#appendix-copilot-workflow-for-parity-migrations).

## Run the playbook with your AI copilot

### Phase 1: Parity translation (OLTP query logic ‚Üí ClickHouse SQL)

In this phase, you translate the existing OLTP-backed query logic (joins, CTE chains, stored-procedure-like reshaping) into ClickHouse SQL so you can validate that the output matches the existing API exactly.

The priority is **correctness**, not OLAP optimization.

### You do

- Run the existing system for a set of golden requests and capture the expected responses (your ‚Äúgolden cases‚Äù).
- Give your copilot the right context (contract + OLTP handler/SQL + golden cases) so it can generate the parity SQL.
- Run the copilot‚Äôs generated ClickHouse SQL against your test data (typically a local ClickHouse seeded via `moose seed`) and compare the output to your golden cases.

### Copilot does

- Proposes parity SQL (possibly as a sequence of queries/CTEs).
- Suggests exactly how to test it (e.g., a `moose query` command) and what output to copy back into the chat. (see [Seeding your local database from prod with `moose seed`](#seeding-your-local-database-from-prod-with-moose-seed))
- Iterates based on your test results: you run the query, paste the output/diffs, and the copilot refines the SQL until the golden cases match.

### You verify

- All golden cases match (exact match unless a tolerance is explicitly documented).
- The parity query returns data for each case and respects auth/tenancy constraints.

### Phase 2: Performance optimization

At this point, you likely have a parity query that‚Äôs correct‚Äîbut still too expensive to run on every dashboard request.

That‚Äôs because OLTP query logic often relies on ‚Äúread-time orchestration‚Äù (just-in-time joins, layered CTE chains, stored-procedure-like reshaping). A 1:1 ClickHouse translation usually preserves that shape, like this:

```sql
WITH UserMerchants AS (
  SELECT merchant_id
  FROM getUserPermissions(:user_id)
),
OrdersWithMerchant AS (
  SELECT
    o.created_at,
    m.merchant_name,
    o.total_orders,
    o.fulfilled_orders
  FROM order_metrics o
  JOIN merchants m ON m.id = o.merchant_id
  WHERE o.merchant_id IN (SELECT merchant_id FROM UserMerchants)
),
AggregatedMetrics AS (
  SELECT
    merchant_name,
    count(*) AS total_orders,
    sum(fulfilled_orders) AS fulfilled_orders,
    sum(total_orders) AS total_orders
  FROM OrdersWithMerchant
  GROUP BY merchant_name
)
SELECT *
FROM AggregatedMetrics
ORDER BY merchant_name;
```

In OLAP, this ‚Äúread-time orchestration‚Äù is an antipattern: every dashboard refresh forces ClickHouse to redo the same joins, reshaping, and aggregations from scratch‚Äîeven when users are only changing filters, sorts, or time ranges.

ClickHouse best practices avoid stored-procedure-like pipelines at read time, and instead shift the reusable work to write time (using Materialized Views) whenever possible. See ClickHouse‚Äôs guidance here: [`Stored procedures and query parameters in ClickHouse`](https://clickhouse.com/docs/guides/developer/stored-procedures-and-prepared-statements).

So Phase 2 is about turning the parity query into a **serving-ready table**: promote the reusable joins/reshaping/derived fields into a [Materialized View](/moosestack/olap/model-materialized-view), then keep request-time logic focused on slicing/sorting/pagination over that serving table.

### You do

- Identify the OLTP ‚Äúread-time orchestration‚Äù patterns in the parity query (joins, layered CTE reshaping, stored-procedure-like pipelines).
- Prompt your copilot to propose what must move to write time ([Materialized Views](/moosestack/olap/model-materialized-view)) versus what stays flexible at query time.

### Copilot does

- Proposes MV(s) + backing table schema(s) that shift joins/reshaping/derived fields to write time.
- Updates the parity approach so the serving query becomes a thin slice over serving-ready tables.

### You verify

- The optimized path still passes golden cases.
- The serving query is materially simpler (fewer joins/CTEs at read time) and measurably faster.

### Implementing the Materialized Views (MVs) and serving table(s)

Now you'll implement the [Materialized Views](/moosestack/olap/model-materialized-view) and serving table(s) your copilot proposed. This is the step that turns a "correct but expensive" parity query into a stable, fast-serving surface for the Query Layer.

In MooseStack, Materialized Views allow you to precompute joins, aggregations, and derived fields and persist the results in a physical table. Your API handlers then query these precomputed tables instead of executing the full, complex query at request time. This keeps dataset handlers thin and predictable, and makes performance largely independent of query complexity.

#### Create the MV + serving table

- Define the **serving table schema** (the flattened / serving-ready shape your handler will read from).
- Define the **Materialized View** that writes into that serving table (the MV `SELECT` should perform the joins/reshaping/derived fields you want to move to write time).
- Save and watch `moose dev` logs until the MV and its backing table are created/updated successfully.

#### Validate the MV before moving on

- **Data is flowing**: new inserts populate the serving table (or you‚Äôve run a backfill if you need historical parity).
- **Row counts look sane**: use `moose query` to compare counts between source tables and the serving table for a known time window.
- **Spot-check results**: run a few representative queries against the serving table and confirm key fields/aggregates match your expectations.
- **Golden cases still pass**: update your handler to read from the serving table (even with temporary SQL) and confirm the golden responses match end-to-end.

Once those checks pass, you‚Äôre ready for the next step: point your Query Layer `QueryModel` at the serving table so the handler stays thin.

### Phase 3: Serve the Materialized View to your frontend

Once your MV produces a serving-ready table (with joins/reshaping moved upstream), you need an interface to serve that table to your dashboard.

You can do this in two ways:

1. **Dynamic raw SQL string composition in each API handler**: build the SQL in each API handler at request time (dynamic `WHERE` / `GROUP BY` / `ORDER BY` / pagination) over the serving table.
2. **Use the [Query Layer](https://github.com/514-labs/query-layer)**: define a typed model for how the serving table should be queried (what can be selected, grouped by, filtered by, sorted by, and paginated by) and reuse the same query-building logic across handlers. Because the MV pushed joins/reshaping to write time, the Query Layer can serve fast, flexible dashboard queries without reintroducing complex SQL string composition in every endpoint.

For example, if your Materialized View writes into a serving table like `OrderFulfillmentServing` (flattened merchant fields, precomputed counters, etc.), your `QueryModel` might look like this:

```ts
import { defineQueryModel } from "../query-layer";
import { OrderFulfillmentServing } from "../models";
import { sql } from "@514labs/moose-lib";

export const orderFulfillmentModel = defineQueryModel({
  table: OrderFulfillmentServing,

  dimensions: {
    merchantName: { column: "merchant_name" },
    day: { expression: sql`toDate(${OrderFulfillmentServing.columns.created_at})`, as: "day" },
  },

  metrics: {
    totalOrders: { agg: sql`sum(${OrderFulfillmentServing.columns.total_orders})` },
    fulfilledOrders: { agg: sql`sum(${OrderFulfillmentServing.columns.fulfilled_orders})` },
    fulfillmentRate: {
      agg: sql`round(100 * sum(${OrderFulfillmentServing.columns.fulfilled_orders}) / nullIf(sum(${OrderFulfillmentServing.columns.total_orders}), 0), 2)`,
    },
  },

  filters: {
    merchantId: { column: "merchant_id", operators: ["eq", "in"] as const },
    timestamp: { column: "created_at", operators: ["gte", "lte"] as const },
  },

  sortable: ["merchantName", "totalOrders", "fulfillmentRate", "timestamp"] as const,
});
```

### Add the Query Layer to your project

From your MooseStack project directory, copy the Query Layer starter kit into `src/query-layer`:

```bash
pnpm dlx tiged 514-labs/query-layer/src src/query-layer
```

### Define a `QueryModel` for your serving table

To customize the example above for your own migration:

- **Swap the table**: replace `OrderFulfillmentServing` with *your* MV-backed serving table (the table your MV writes into).
- **Update the surface area**:
  - `dimensions`: the group-by fields you want to expose (columns or computed expressions)
  - `metrics`: the aggregates your dashboard needs
  - `filters`: the allowed slice fields + operators
  - `sortable`: the allowed sort fields
- **Repoint the handler**: update your dataset handler to use your `QueryModel` (and re-run golden cases end-to-end).

If you want more detail on Query Layer capabilities and APIs, refer to the dedicated repo: [Moose Query Layer](https://github.com/514-labs/query-layer).


## Running your frontend locally

With your local dev server running, you can connect the rest of your application to your new analytical query in the local moose dev server using token authentication. Once connected, you can see data in your frontend and test the whole application without having to go through any cloud deployment.

## Push to remote github

Add the files you created above (or just the functional files if you don't want to commit your test scripts) and push to your version control. Create a Pull Request.

This branch will later be used in Boreal for a Branch Deployment, automatically triggered by creating the PR. 

# Going to production

This section will guide you through the process of applying your local changes to production:

1. Join Boreal
2. Committing a branch and deploying a preview branch
3. Testing the preview branch
4. Pushing to production
5. Back-filling materialized views

## Join Boreal

(Skip this step if you already have completed Boreal onboarding.)

To deploy changes to production, you need access to [Boreal](http://boreal.cloud) and to the organization that owns the project.

1. [Click here to sign up with your GitHub account.](https://www.boreal.cloud/sign-up)
2. After signing in, you'll be prompted to create or join an organization. Check for any pending invitations and join the organization that owns the project.
3. If no invitation appears in Boreal, check your email. You should have received an invitation to join the organization. Make sure the email address matches your GitHub account and follow the instructions in that email to accept.

Once you've joined the correct organization, you should be able to see the project in Boreal and proceed with your production rollout.

## Generate a Migration Plan

Return to your IDE and confirm the following before moving on:

* New queries and materialized views run locally successfully
* `moose dev` starts without errors
* All relevant APIs return the expected results

If all three checks pass, then you're ready for the final pre-production step: ensuring your changes can be deployed without breaking anything in production. To do this, you'll [generate and review a migration plan](https://docs.fiveonefour.com/moosestack/migrate/planned-migrations?lang=python).

Open your terminal (ensure you `cd` to your MooseStack project root). Then run:

**Command:**
```bash
moose generate migration --save --url <BOREAL_HOST> --token <BOREAL_ADMIN_API_BEARER_TOKEN>
```

**Parameters:**
* `BOREAL_HOST` is the host for your production deployment in Boreal. Copy it from the URL in your project overview dashboard:
  {/* image */}
* `BOREAL_ADMIN_API_BEARER_TOKEN` is sent in the request header when calling the Boreal Admin API at `BOREAL_HOST`. This is the API key. It is a secret and must not be committed to source control. Store it securely in a password manager.

After successfully running `moose generate migration` with the correct `--url` and `--token`, a new `/migrations` directory should appear at the root of your MooseStack project. Open the `plan.yaml` file in that directory and review it carefully.

## Migration plan review

[Review the migration plan to confirm which SQL resources will be created or modified](https://docs.fiveonefour.com/moosestack/migrate/plan-format?). Make sure it matches exactly what you intend to ship. As a rule of thumb:

* Expect mostly new tables and materialized views
* Carefully review schema changes to existing tables
* Avoid deleting existing tables at all costs

### Look for new tables and materialized views

This is expected when optimizing queries. Any new materialized view should result in:

* A [`CreateTable` operation](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#createtable) that creates the backing table for the view
* A [`SqlResource` operation](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#rawsql) containing the `CREATE MATERIALIZED VIEW` statement, with the view explicitly writing `TO` that backing table

Seeing both confirms the Materialized View is being added cleanly and additively. For every new Materialized View in your branch, there should be exactly one `CreateTable` and one `SqlResource` defining it.

### Watch closely for column changes

[Column-level changes](https://docs.fiveonefour.com/moosestack/migrate/plan-format#column-operations) are uncommon. If you encounter them:

* Pause and confirm the change is intentional
* Double-check your code for queries that reference affected columns

There are a [small number of cases where column changes are expected:](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn)

* If you added a column to an existing materialized view, you should see a single [`AddTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn) operation applied to the backing table for that view.
* If you renamed a column, the plan may show a [`DropTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#droptablecolumn) followed by an [`AddTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#addtablecolumn). If this rename was intentional, replace those two operations with a single [`RenameTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#renametablecolumn) operation instead.

Outside of these cases, column-level changes should be treated with caution, especially [`DropTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#droptablecolumn) or [`ModifyTableColumn`](https://docs.fiveonefour.com/moosestack/migrate/plan-format#modifytablecolumn) operations. These changes are strongly discouraged. Instead, stick to strictly additive migrations. Undo the delete or modification in your `OlapTable` object, and introduce a new column instead.

### Sanity-check for `DropTable` operations

If you see any [`DropTable` operations](https://docs.fiveonefour.com/moosestack/migrate/plan-format?lang=python#droptable), proceed with extreme caution and review your changes carefully. They may indicate that an `OlapTable` or `MaterializedView` object defined in the codebase (and currently used in production) is being deleted, which can result in irreversible data loss if applied unintentionally.

If the plan shows changes you did not anticipate, stop and resolve that before proceeding.

Once the plan looks correct, you're ready to continue with preview and production rollout.

## Open a Pull Request and Inspect the Preview Environment

Commit your changes, push them to a new branch, and open a pull request targeting `main`.

Once the PR is open, Boreal automatically deploys an isolated preview (staging) environment for the branch. Your code and the generated `plan.yaml` are applied to a staging database forked from production, so changes are created exactly as they would be in prod.

Confirm that **boreal-cloud bot** appears in the PR comments.

{/* image */}

This confirms that:

* Your GitHub account is correctly linked
* Boreal has started deploying the preview environment

If the bot does not appear, double check that you have correctly integrated your Github account with your Boreal account. If something doesn't look right, reach out to the 514 team for help.

In the **boreal-cloud bot** comment, you'll see a table. Click the link in the **Project** column (the link text will match your branch name). This opens the Boreal project dashboard with your preview environment selected.

{/* image */}

From here, you'll inspect the database state and validate that the resources created by your migration plan match what you reviewed and expected before proceeding.

## Connect to the Staging Database

In this step, you'll query the staging ClickHouse database directly using ClickHouse's HTTPS interface.

First, get the database HTTPS connection string from Boreal using the same steps you followed earlier. Make sure the Boreal dashboard is set to your feature branch, not `main`. You can confirm this by checking the branch selector in the left sidebar of the project dashboard.

### Set the staging connection string locally

Create a temporary environment variable for your staging database URL:

**Command:**
```bash
export STAGING_DB=<your-staging-db-connection>
```

You can now safely use `$STAGING_DB` to run queries against the staging database via `curl`.

### Inspect Staging Database Tables

In a terminal, run:

**Command:**
```bash
curl -sS \
  $STAGING_DB \
  --data-binary 'SHOW TABLES'
```

**Expected Response:**

You should see a plain-text list of all tables in the staging database if the command executed successfully:

```
customers
products
orders
order_items
merchants
order_metrics_daily
...
```

Use this output to confirm that:

* All new tables and materialized views defined in `plan.yaml` exist
* No unexpected tables were created
* Existing tables remain unchanged unless explicitly intended

If the list of tables does not match what you reviewed in the migration plan, stop here and fix the issue before proceeding.

Do not merge until the preview environment reflects exactly the database resources and behavior you expect to see in production.

## Merge PR to Deploy to Prod

If everything lines up as you expect, you're ready to merge!

Merge your PR and now do the same thing: click the Boreal bot to view the deployment page. You should see the logs from the deployment and status there. The deployment should take a few minutes.

## Backfill new Materialized Views

If your migration introduces any new Materialized Views, they will start populating only for new incoming data. To apply them to historical data, you must explicitly backfill them from existing ClickHouse tables.

This step uses the same HTTPS + `curl` workflow as before, but targets the production (`main`) database and performs a write operation to apply the backfill.

### Identify which materialized views need a backfill

Open the migration `plan.yml` you just shipped and find the new materialized views you created (look for the `CREATE MATERIALIZED VIEW` statements in `SqlResource`).

For each one, note two things:

* the materialized view name
* the backing table name it writes `TO`

### Run the backfill (one MV at a time)

Backfilling is done by inserting historical rows into the MV's backing table using the same `SELECT` logic used by the view.

In a terminal, run:

**Command:**
```bash
curl -sS \
  '$BOREAL_CONNECTION_STRING' \
  --data-binary "
  INSERT INTO <mv_backing_table>
  SELECT ...
  "
```

Use the exact `SELECT` statement from the `CREATE MATERIALIZED VIEW` definition (or the underlying `SELECT` you used when building it) and paste it in place of `SELECT ...`.

### Confirm the backfill worked

After each backfill, sanity check that the backing table now has rows:

**Command:**
```bash
curl -sS \
  $BOREAL_CONNECTION_STRING \
  --data-binary 'SELECT count() FROM <mv_backing_table>'
```

**Expected Response:**

If the count is non-zero (and roughly matches what you expect), the backfill is complete.

### Repeat for each new MV you added

Only backfill the MVs introduced in this change. Avoid reprocessing older MVs unless you intentionally want to rebuild them.
# Appendixes

## Appendix: Copilot workflow for parity migrations

### Set up your copilot

Select **Plan mode** (selected near the model selector in the chat pane), and **choose an advanced reasoning model** of your choice (like Claude Opus 4.5, GPT Codex 5.1 Max or GPT 5.2).

{/* image */}


### Generating documentation

Add documentation tasks you want the agent to produce alongside the implementation, e.g.

```
Generate OpenAPI spec for the created endpoints
```

```
Add a README summarizing endpoints and data lineage
Include example curl requests + sample responses, schema diagrams / entity summary (if relevant), and a "how to test locally" section
```

```
Add inline JSDoc for handlers, params, and response shapes
```

### Running the plan

Once you are satisfied with the plan, hit `Build`. This will experiment with SQL queries, build up materialized views and implement the new endpoint.

{/* image */}

You will likely be asked for various permissions along the way, including for MCP tool use to check work against local data, and curl commands to check work against the remote database.

Example of tool-use in the process of implementing the plan:

### Manual testing

You can ask the LLM to generate curl commands that you can use to test the generated API endpoints:

```
Generate curl commands to test this new endpoint
```

Which returns:

**Command:**
```shell
curl -X POST http://localhost:4000/dataset \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <YOUR_API_TOKEN>" \
  -d '{
    "datasetId": "<YOUR_DATASET_GUID>",
    "parameters": {
      "limit": "10",
      "offset": "0",
      "orderby": "MerchantName Asc"
    }
  }' | jq '.'
```

You can iterate on this curl command to test against known data values.

### Copilot-assisted testing

You can also instruct the agent to take the success criteria defined in the specification document and create a test suite, showing you the results. It is best practice to audit and get comfortable with the test suite and the output to ensure that it covers actual usage.

```
Generate a test script that iterates through the generated API, with the test cases defined in the specification document
```

Which generates a test script with multiple test cases, testing general API functionality like security, pagination, etc., as well as tests for input and output schemas, and exact input and output data for known values, e.g.:

```
Test cases:

‚úÖ Basic request with default parameters (limit 10, offset 0, ascending)
‚úÖ Request with limit 5
‚úÖ Request with offset (pagination - offset 10)
‚úÖ Request with descending order
‚úÖ Request with cacheId
‚úÖ Request with security filter (merchant)
‚úÖ Request with parameter.datasource parameter
‚úÖ Request with orderby leading space (edge case)
‚úÖ Exact structure validation (validates response structure matches document)
‚úÖ Exact value validation (validates first 10 records match document exactly)
```

## WSL: Setting up a WSL2 Linux instance to run MooseStack on Windows

### Prerequisites

* **Windows 10 or 11:** Ensure you are running Windows 10 version 2004 (build 19041) or later, or Windows 11 [\[1\]](https://learn.microsoft.com/en-us/windows/wsl/install#:~:text=Prerequisites).
* **Hardware virtualization:** WSL2 requires a 64-bit processor with virtualization support (e.g. VT-x/AMD-V) enabled in your BIOS, and at least 4 GB of RAM to run Docker [\[2\]](https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers#:~:text=%2A%2064,SLAT). Make sure virtualization is turned on in your system BIOS settings.
* **Internet access:** The setup will download Linux and various packages (Node.js, Docker, etc.), so an internet connection is required.
* **Disk space:** Have a few gigabytes of free disk space for the Linux filesystem and Docker images.
* **Docker Desktop:** We will use Docker Desktop for Windows (with WSL2 integration) to run MooseStack's containers. You can download and configure it during the steps below.
* *(Optional)* **Windows Terminal:** It's recommended to install Windows Terminal for a better command-line experience (multiple tabs, copy/paste, etc.) [\[3\]](https://learn.microsoft.com/en-us/windows/dev-environment/javascript/nodejs-on-wsl#:~:text=Windows%20Terminal), though you can use any terminal you prefer.

### Step 1: Enable WSL 2 and Install Ubuntu Linux

1. **Enable WSL 2:** Open PowerShell as Administrator (right-click Start menu, choose PowerShell (Admin)). Run the following command to enable the Windows Subsystem for Linux and install its components:

**Command:**
```shell
wsl --install
```

   1. This one command enables the necessary Windows features and downloads the latest Ubuntu Linux distribution by default [\[4\]](https://learn.microsoft.com/en-us/windows/wsl/install#:~:text=You%20can%20now%20install%20everything,command%2C%20then%20restart%20your%20machine). If prompted, restart your computer to complete the WSL installation. (On Windows 11, a restart may happen automatically.)

   2. *Note:* If WSL was already partially installed and the above command just shows help text, you can list available distros with `wsl --list --online` and then install a specific one with `wsl --install -d Ubuntu` [\[5\]](https://learn.microsoft.com/en-us/windows/wsl/install#:~:text=The%20above%20command%20only%20works,WSL%20%20or%20%207).

2. **Initial Ubuntu setup:** After reboot, Windows should automatically launch the newly installed *Ubuntu* for its first-time setup. If it doesn't open on its own, you can launch "Ubuntu" from the Start menu. A console window will appear as Ubuntu initializes (this may take a minute as files decompress) [\[6\]](https://learn.microsoft.com/en-us/windows/wsl/install#:~:text=The%20first%20time%20you%20launch,take%20less%20than%20a%20second).

3. **Create Linux user:** You will be prompted to create a new UNIX username and password for the Ubuntu instance (this is separate from your Windows credentials) [\[7\]](https://learn.microsoft.com/en-us/windows/wsl/install#:~:text=Set%20up%20your%20Linux%20user,info). Enter a username and a secure password ‚Äî you won't see characters as you type the password (that's normal). This will create a new user account in the Ubuntu environment and then drop you into a Linux shell.

4. **Verify WSL installation:** Once setup is complete, you should see a Linux terminal prompt (e.g. something like `username@DESKTOP-XYZ:~$`). At this point, you have a Ubuntu WSL instance running. You can check that it's using WSL version 2 by opening a PowerShell/Command Prompt (not the Ubuntu shell) and running `wsl -l -v`. It should list your Ubuntu distro with version "2". If it says version "1", upgrade it with `wsl --set-version Ubuntu 2`.

### Step 2: Update Linux Packages

Now that Ubuntu is running in WSL, update its package lists and upgrade installed packages:

Update apt repositories: In the Ubuntu terminal, run:

**Command:**
```shell
sudo apt update && sudo apt upgrade -y
```

* This fetches the latest package listings and installs any updates. It's good practice to do this right after installing a new Linux distro [\[8\]](https://learn.microsoft.com/en-us/windows/dev-environment/javascript/nodejs-on-wsl#:~:text=menu,dc). (The `-y` flag auto-confirms prompts during upgrade.)

* **(Optional) Install basic tools:** You may want to install some common utilities. For example, you can run `sudo apt install -y build-essential curl git` to ensure you have compilers and Git available. This is not strictly required for MooseStack, but can be useful for general development.

### Step 3: Install and Configure Docker (WSL Integration)

MooseStack uses Docker containers under the hood for components like ClickHouse (database), Redpanda (streaming), etc. On Windows, the recommended way to run Docker with WSL2 is via **Docker Desktop** [\[9\]](https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers#:~:text=containers%20by%20setting%20up%20Docker,Subsystem%20for%20Linux%2C%20version%202)

**Install Docker Desktop:**

1. **Download and Install:** If you don't have it already, download **Docker Desktop for Windows** from the Docker website and run the installer. During installation, it should prompt to enable WSL2 features if not already enabled (which we did in Step 1). Follow the prompts to install Docker Desktop.

2. **Start Docker Desktop:** After installation, launch Docker Desktop. In the taskbar, you'll see the Docker whale icon appear (it may take a few seconds to start up the first time).

**Configure Docker Desktop with WSL2**

1. **Enable WSL2 backend:** In Docker Desktop, open **Settings** (gear icon or via the taskbar menu). Under **General**, ensure **"Use the WSL 2 based engine"** is checked. This tells Docker to use WSL2 for running Linux containers.
  {/* image */}

2. **Integrate with Ubuntu:** Still in Settings, go to **Resources > WSL Integration**. You should see a list of your WSL distributions. Find **Ubuntu** (or the distro you installed) and **enable integration** for it (toggle it on). This allows Docker containers to be managed from within that WSL instance.
   {/* image */}

3. **Allocate sufficient resources:** Docker Desktop by default might allocate limited resources. MooseStack's stack requires at least **2.5 GB of memory** for Docker. Go to **Resources > Advanced** (or **Resources > Memory**) and increase the memory to 3-4 GB to be safe. You can also adjust CPUs if needed (2 CPUs is usually fine for dev).

4. **Apply settings:** Click "Apply & Restart" if you changed any Docker settings. Docker Desktop will restart its engine to apply the new configuration.

5. **Test Docker in WSL:** Open a new Ubuntu WSL terminal (or use the existing one). Run `docker --version` to ensure the Docker CLI is accessible, and then run a test container:

**Command:**
```shell
docker run hello-world
```

**Expected Response:**

The hello-world container should download and run, printing a "Hello from Docker!" message and exiting. This confirms that Docker is working inside WSL (via Docker Desktop). You can also try `docker ps` (which should list no running containers, until the Moose dev containers start later) to verify the Docker daemon is reachable from WSL.

**Troubleshooting:** If `docker run hello-world` fails, ensure Docker Desktop is running and that WSL integration is enabled for your distro. In some cases, you might need to install the Docker CLI tools inside WSL, but Docker Desktop usually handles that by exposing the `docker` command in WSL [\[10\]](https://stackoverflow.com/questions/70449927/how-can-i-access-wsl2-which-is-used-by-docker-desktop#:~:text=Docker%20Desktop%20,tab%20in%20Settings). Also verify that your Ubuntu WSL is set to version 2 (as Docker won't work with WSL1).

### Step 4: Proceed with MooseStack requirements and installation

You now have a working Ubuntu instance on your windows machine with Docker installed. You can proceed to install MooseStack and its requirements (Node.js 20+ and/or Python 3.12+) as if you were installing on a regular linux machine. Just make sure to install from the Linux terminal prompt in the Ubuntu instance.

## Expected CDC Impact

### Storage Impact

Primarily driven by change volume (rows changed / day) and retention period

| Component | Expected Impact |
| :---- | :---- |
| Tracking tables (`cdc.*_CT`) | ~10-30% of source table sizes (not 2x) |
| Retention | Default 3 days, then auto-purged |
| Transaction log | May grow if capture job lags |

### CPU Impact

Primarily driven by change rate (changes / sec) and the number of CDC enabled tables

| Component | Expected Overhead |
| :---- | :---- |
| CDC Capture Job | Low additional CPU |
| Multiple tables | Low-moderate (well within normal) |
| Polling interval (e.g. 2-min) | Negligible (just reads) |

### I/O Impact

Primarily driven by write volume to tracking tables and transaction log read rate

| Operation | Expected Overhead |
| :---- | :---- |
| Transaction log reads | Low additional read I/O |
| Tracking table writes | Low additional write I/O |
| Debezium queries (periodic) | Minimal - batched reads |

### Memory Impact

Primarily driven by the number of CDC-enabled tables and their row size

| Component | Expected Overhead |
| :---- | :---- |
| Capture job buffers | Low, measured in MB |
| Tracking table indexes | Proportional to change volume |

## Moose Language Server

The [Moose LSP for Typescript](https://github.com/514-labs/moosestack-lsp) is an experimental feature available for early access. It enables SQL validation and syntax highlighting for `sql` strings in Typescript objects in your Moose project. It currently has known limitations around nested SQL and SQL fragments that will be incorrectly highlighted as errors. If you would like access to experiment with this feature, let us know!

Beta distributions in IDE extension marketplaces (installation/access not guaranteed):

* vscode: [https://marketplace.visualstudio.com/items?itemName=514-labs.moosestack-lsp](https://marketplace.visualstudio.com/items?itemName=514-labs.moosestack-lsp)
* cursor: [https://open-vsx.org/extension/514-labs/moosestack-lsp](https://open-vsx.org/extension/514-labs/moosestack-lsp)

## Manually seeding local dev

Prompt your copilot to seed your local database from each remote table:

```
Goal: seed local ClickHouse from a remote Boreal ClickHouse via HTTP using `moose query`.

Assumptions:
- `BOREAL_CONNECTION_STRING` is set and looks like: `https://default:PASSWORD@HOST:8443/DB_NAME`
- Local ClickHouse has the same tables already created.
- Use `url()` + `JSONEachRow` to stream data from remote into local.

Steps:

1) Extract remote base URL + database name from `BOREAL_CONNECTION_STRING`.
2) Get list of tables (exclude system + materialized views).
3) For each table, generate a schema string ("col1 Type1, col2 Type2, ...") from `DESCRIBE TABLE ... FORMAT JSONEachRow`.
4) For each table, generate a seed SQL file with this pattern:

   INSERT INTO <table>
   SELECT * FROM url(
     '<base_url>/?database=<db>&query=SELECT+*+FROM+<table>+FORMAT+JSONEachRow',
     JSONEachRow,
     '<schema_string>'
   );

   (No LIMIT clauses: copy all rows.)

5) Run all generated seed SQL files with `moose query`.
6) Verify row counts for each table (use FINAL) using `moose query`.

Implement as a single bash script.
```

## Sample Plan

This is an example of a plan generated by an AI copilot for implementing a dataset API endpoint. Use this as a reference for the level of detail and structure your own AI-generated plans should have.

````markdown
# Implement Order Fulfillment Dataset API Endpoint

## Overview

Implement the OrderFulfillment dataset endpoint. The endpoint should return order fulfillment metrics by merchant and time period, matching the MSSQL query behavior from the original system.

## Analysis

### MSSQL Query Structure (from specification)

The original MSSQL query:

1. Gets user permissions via `getUserPermissions` stored procedure
2. Joins `merchants` ‚Üí `merchant_properties` ‚Üí `order_metrics`
3. Returns: `MerchantId`, `MerchantName`, `FulfillmentRate`, `TotalOrders`
4. Filters based on user permissions (if user doesn't have view-all access, filters by authorized merchants)

### Expected Response Format

Based on the API payload/response in the specification:

- **Payload fields**: `distinct([MerchantName]) AS [value], [FulfillmentRate] AS [metric], [TotalOrders]`

- **Response structure**:
  ```json
    {
      "value": "Acme Corp",          // MerchantName
      "metric": 95.5,                // FulfillmentRate
      "TotalOrders": 1250            // TotalOrders
    }
  ```

### ClickHouse Tables Available

- `merchants` - Merchant/company information (`merchant_name`)
- `merchant_properties` - Merchant properties and configurations
- `order_metrics` - Order and fulfillment data
- `orders` - Individual order records
- `order_items` - Line items for orders

## Implementation Plan

### Phase 1: Query Development & Testing

1. **Convert MSSQL to ClickHouse query**

- Map MSSQL syntax to ClickHouse equivalents
- Handle user permissions (simplify initially - may need to implement `getUserPermissions` logic later)
- Join: `merchants` ‚Üí `merchant_properties` ‚Üí `order_metrics`
- Calculate: FulfillmentRate = (fulfilled_orders / total_orders) * 100
- Return: `MerchantName`, `FulfillmentRate`, `TotalOrders`

2. **Test query against production ClickHouse**

- Base URL: `https://default:<PASSWORD>@<HOST>:8443/<DATABASE_NAME>`
- Verify query returns data
- Check column names and data types match expected format
- Iterate until query is correct

3. **Handle cascading queries if needed**

- If materialized views are needed, use CTEs or subqueries to emulate them
- Test each step of the query chain

### Phase 2: Handler Implementation

1. **Create handler file**: `moosestack/src/analytics/apis/order-fulfillment.api.ts`

- Follow pattern from existing API handlers
- Use `ApiUtil` with `client` and `sql` helpers
- Handle security/merchant filtering if needed

2. **Implement query with ClickHouse syntax**

- Use `sql` template tag for type-safe queries
- Import required table models
- Handle user permissions (may need to simplify for initial implementation)

3. **Format response to match expected structure**

- Return format matching the API response structure:
     ```typescript
          {
            cacheId: string;
            offset: number;
            limit: number;
            totalRecordCount: number;
            columns: Array<{name, dataField, displayName, type, precision, scale}>;
            recordsCount: number;
            records: Array<{value, metric, TotalOrders}>;
            failures: {};
            warnings: {};
          }
     ```

### Phase 3: Registration & Testing

1. **Register handler in dataset registry**

- Update `moosestack/src/dataset/registry.ts`
- Add handler for the OrderFulfillment dataset

2. **Test endpoint via curl**

- Test against route: `http://localhost:4000/dataset` (POST)
- Use proper authentication header
- Test with sample payload matching the specification format
- Verify response matches expected structure

3. **Iterate and fix**

- Fix any query issues
- Adjust response formatting if needed
- Handle edge cases (empty results, permissions, etc.)

## Files to Modify

1. **New file**: `moosestack/src/analytics/apis/order-fulfillment.api.ts`

- Handler function: `fetchOrderFulfillmentData`
- Query implementation
- Response formatting

2. **Update**: `moosestack/src/dataset/registry.ts`

- Import `fetchOrderFulfillmentData`
- Register handler for OrderFulfillment dataset

## Key Considerations

1. **User Permissions**: The MSSQL query uses `getUserPermissions` stored procedure. For initial implementation, we may need to:

- Simplify to return all merchants (if user context isn't available)
- Or implement a simplified version of the permission logic

2. **Field Mapping**: Ensure correct mapping between frontend field names and database columns.
3. **Security Filtering**: May need to handle merchant-level filtering for multi-tenant scenarios.
4. **Ordering**: Implement sorting by merchant name or fulfillment rate as specified.
5. **Pagination**: Implement `limit` and `offset` for large result sets.

## Testing Strategy

1. Test ClickHouse query directly first (before implementing handler)
2. Show query results at each iteration
3. Test handler function in isolation
4. Test full endpoint with curl
5. Validate response structure and data accuracy
````

## Security

*Security appendix coming soon. Security protocols have already been implemented in this project.*
