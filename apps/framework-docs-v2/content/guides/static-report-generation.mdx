---
title: Static Report Generation
description: Automate report generation with MooseStack workflows
---

# Static Report Generation

This guide shows you how to automate the generation of static reports using MooseStack workflows. You'll build a complete data pipeline from file ingestion through PDF export, learning how to leverage ClickHouse's time-series capabilities for fast analytics.

## What You'll Learn

- Defining type-safe data models with automatic infrastructure provisioning
- Ingesting data from CSV and JSON files into ClickHouse
- Creating materialized views for pre-aggregated time-series analytics
- Building parameterized consumption APIs for report generation
- Generating professional PDF reports from aggregated data

## How to Use This Guide

We've organized this guide for different audiences:

**Operations teams:** Focus on Part 1 to understand automation value, then skim Parts 2-6 to see what's possible. Share with your development team for implementation.

**Developers who learn by doing:** Jump directly to Part 2 and start building. Come back to Part 1 later if you want business context.

**Data engineers evaluating architecture:** Read Part 1 for the business case, review the architecture diagram, then follow the full tutorial. Part 7 explains the technical concepts in depth.

**Choose your path:**
- ðŸ“Š Business/ROI focus? â†’ Read Part 1, evaluate if this fits your needs
- ðŸ’» Hands-on builder? â†’ Jump to Part 2, start coding immediately
- ðŸ§  Concepts first? â†’ Read Part 1, build in Parts 2-6, learn theory in Part 7

## Prerequisites

- Node.js 18+ installed
- Docker Desktop installed and running
- Basic familiarity with TypeScript and SQL
- Terminal access
- ~40 minutes for hands-on work

### Verify Your Environment

Before starting, confirm your tools are ready:

**Check Node.js:**
```bash
node --version
```
You should see `v18.x.x` or higher.

**Check Docker:**
```bash
docker --version
docker info > /dev/null 2>&1 && echo "Docker is running" || echo "Docker is NOT running"
```
You should see a version number and "Docker is running". If Docker isn't running, start Docker Desktop before proceeding.

---

## Part 1: The Scenario

*Read this section to understand why automated report generation matters and what business problems it solves. This context helps you evaluate whether this approach fits your needs and provides justification for stakeholders.*

### Apex Retail Daily Sales Consolidation

Before diving into code, let's understand what we're building and why.

**The Business Context:**
You're a data engineer at **Apex Retail**, a growing retail chain with 12 stores across 4 regions (West, East, Central, South). Each night, store point-of-sale (POS) systems export the day's sales transactions as CSV or JSON files.

**The Current Pain:**
> An analyst manually downloads files from each store, copies data into Excel, and emails a summary to regional managers. This takes 2 hours every morning and is error-prone. When the analyst is sick, reports don't go out.

**What You'll Build:**
> An automated pipeline that ingests store file uploads, stores them in ClickHouse, pre-aggregates daily summaries with materialized views, and generates an executive PDF reportâ€”all with about 60 lines of TypeScript.

### Why Automate Report Generation?

**The hidden costs of manual reporting:**

| Cost Factor | Manual Process | Automated Pipeline |
|-------------|----------------|-------------------|
| Time per report | 2 hours | < 1 minute |
| Annual analyst hours | 500+ hours/year | ~10 hours/year (maintenance) |
| Error rate | 5-10% (copy/paste, formula errors) | 0% (deterministic code) |
| Availability | Depends on one person | Runs automatically, 24/7 |
| Audit trail | None (who changed what?) | Full version control |
| Scalability | Linear (more stores = more time) | Constant (1 store or 100 stores) |

**Real-world symptoms you might recognize:**

- Reports are late when the analyst is on vacation or sick
- "The numbers don't match" disputes between regional managers
- Hours spent debugging Excel formulas
- No historical record of past report calculations
- Can't easily answer "show me last Tuesday's numbers"

**Why not just use existing tools?**

| Approach | Limitation |
|----------|------------|
| **Excel macros** | No version control, formula errors, breaks when file formats change |
| **BI dashboards (Tableau, Looker)** | Great for interactive exploration, but PDF generation often requires custom scripting anyway |
| **Scheduled database queries** | No data transformation layer, raw SQL becomes unmaintainable |
| **Manual scripts** | No streaming, no type safety, hard to extend |

**This approach gives you:**
- **Type-safe data models** - Catch schema mismatches at compile time
- **Streaming ingestion** - Data flows through Redpanda for reliability
- **Pre-aggregated analytics** - Materialized views make queries instant
- **Version-controlled pipelines** - Git history shows exactly what changed
- **Reproducible reports** - Same inputs always produce same outputs

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Flow                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   CSV/JSON Files     POST /ingest/sales      Redpanda Stream        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º      â”‚
â”‚                                                                     â”‚
â”‚   ClickHouse Table   Materialized View      Consumption API         â”‚
â”‚   â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚   (Raw Transactions) (Daily Summaries)     (JSON Endpoint)          â”‚
â”‚                                                                     â”‚
â”‚                                              PDF Export             â”‚
â”‚                                             â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚                                             (Executive Summary)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The End Result

By the end of this tutorial, you'll have:
1. **Automated data ingestion** - Load CSV/JSON files with a simple script
2. **Pre-computed aggregations** - Daily summaries ready in milliseconds
3. **API endpoint** - Parameterized JSON endpoint for any date range
4. **Executive PDF report** - One-page summary for regional managers

---

## Part 2: Setup and Data Model

**Time: ~8 minutes**

### Step 1: Install the Moose CLI

First, install the Moose CLI:

```bash
bash -i <(curl -fsSL https://fiveonefour.com/install.sh) moose
```

After installation completes, verify with:

```bash
moose --version
```

### Step 2: Initialize Project

Create a new Moose project using the TypeScript template:

```bash
moose init sales-reports --language typescript
cd sales-reports
npm install
```

### Step 3: Start Development Server

Launch the full development stack including ClickHouse, Redpanda, and Temporal:

```bash
moose dev
```

You should see output indicating the API server started on http://localhost:4000. Keep this terminal open and use a new terminal for subsequent commands.

**Important:** The development server auto-reloads when you change files. Changes to your code are automatically applied.

#### Verify the Stack is Running

In a new terminal, confirm all services are healthy:

```bash
# Test the API server
curl http://localhost:4000/health
```

Expected output: `{"status":"ok"}` or similar success response.

```bash
# Test ClickHouse connection
curl http://localhost:18123/ping
```

Expected output: `Ok.`

If either check fails, wait 10-15 seconds for containers to fully initialize, then try again. See the Troubleshooting section if issues persist.

**What Just Happened:**

MooseStack automatically provisioned a complete development stack:
- **ClickHouse** (port 18123) - Columnar analytical database optimized for aggregations
- **Redpanda** (port 19092) - Kafka-compatible streaming platform for reliable data flow
- **Temporal** (port 8080) - Workflow orchestration for scheduled tasks
- **HTTP Server** (port 4000) - API endpoints for ingestion and consumption

All running locally in Docker containers. No cloud accounts or external services required.

### Step 4: Define the Data Model

Create the file `app/ingest/SalesTransaction.ts` with your data model. This represents what a store POS system exports each night:

```typescript
import { Key, IngestPipeline } from "@514labs/moose-lib";

// Define the data model - matches store POS export format
export interface SalesTransaction {
  // Primary key
  transactionId: Key<string>;  // e.g., "TXN-W001-20250115-0001"

  // Timestamp - the star of time-series analytics
  saleTimestamp: Date;

  // Store & location
  storeId: string;             // e.g., "STORE-W001"
  region: string;              // West, East, Central, South

  // Product info
  productId: string;
  productCategory: string;     // Electronics, Clothing, Food, Home, Sports

  // Customer (optional - from loyalty card)
  customerId: string;

  // Transaction amounts
  quantity: number;
  unitPrice: number;
  totalAmount: number;
}

// Create a complete ingestion pipeline with one declaration
export const salesPipeline = new IngestPipeline<SalesTransaction>("sales", {
  ingestApi: true,
  stream: true,
  table: {
    // Time-first ordering for efficient range queries
    orderByFields: ["saleTimestamp", "transactionId"],
  }
});
```

This single declaration creates three things automatically:
- A **ClickHouse table** matching your interface
- A **Redpanda stream** for real-time data flow
- A **REST API endpoint** at `/ingest/sales`

### Step 5: Export Models

Update `app/index.ts` to export your models:

```typescript
export * from "./ingest/SalesTransaction";
```

#### Verify Model Detection

Check your `moose dev` terminal. You should see output like:

```
âœ“ Detected model changes
âœ“ Created table: sales_SalesTransaction
âœ“ Created stream: sales
âœ“ Registered API endpoint: /ingest/sales
```

If you don't see this, ensure your file is saved and check for TypeScript errors in the terminal output.

**What Just Happened:**

From a single TypeScript interface, MooseStack automatically:
1. **Inferred the ClickHouse schema** - Mapped TypeScript types to ClickHouse column types
2. **Created the table** - With optimal ordering for time-series queries (`saleTimestamp` first)
3. **Set up streaming** - Redpanda topic for reliable, ordered data delivery
4. **Generated the API** - REST endpoint with automatic validation against your interface

This is the core value proposition: define your data model once, get infrastructure automatically.

### Step 6: Test Basic Ingestion

POST sample data to the auto-generated /ingest API. This simulates what a store POS would send:

```bash
curl -X POST http://localhost:4000/ingest/sales \
  -H "Content-Type: application/json" \
  -d '{
    "transactionId": "TXN-W001-20250115-0001",
    "saleTimestamp": "2025-01-15T10:30:00Z",
    "storeId": "STORE-W001",
    "region": "West",
    "productId": "PROD-ELE-001",
    "productCategory": "Electronics",
    "customerId": "CUST-1001",
    "quantity": 2,
    "unitPrice": 299.99,
    "totalAmount": 599.98
  }'
```

A successful response returns `{"success": true}`.

**What Just Happened:** You sent a single transaction that simulates a customer buying 2 items at the West region's Store W001. The data flowed through Redpanda and landed in ClickHouseâ€”all auto-provisioned from your TypeScript interface.

Verify data landed by querying ClickHouse directly:

> **Note:** The credentials `panda`/`pandapass` are local development defaults configured by `moose dev`. Never use these in productionâ€”configure secure credentials in your deployment environment.

```bash
clickhouse-client --host localhost --port 9000 --user panda --password pandapass \
  --query "SELECT * FROM local.sales_SalesTransaction"
```

---

## Part 3: File Ingestion

**Time: ~8 minutes**

In production, stores upload CSV or JSON files rather than calling APIs directly. Let's build a file loader.

### Step 1: Install Dependencies

```bash
npm install papaparse date-fns
npm install -D @types/papaparse
```

### Step 2: Create File Loader Script

Create `scripts/load-file.ts`:

```typescript
import Papa from "papaparse";
import fs from "fs";
import path from "path";

interface SalesTransaction {
  transactionId: string;
  saleTimestamp: string;
  storeId: string;
  region: string;
  productId: string;
  productCategory: string;
  customerId: string;
  quantity: number;
  unitPrice: number;
  totalAmount: number;
}

async function loadFile(filePath: string): Promise<void> {
  const ext = path.extname(filePath).toLowerCase();
  const content = fs.readFileSync(filePath, "utf-8");

  let records: SalesTransaction[];

  if (ext === ".csv") {
    const { data } = Papa.parse<SalesTransaction>(content, {
      header: true,
      dynamicTyping: true,
    });
    records = data.filter((r) => r.transactionId); // Filter empty rows
  } else if (ext === ".json") {
    const parsed = JSON.parse(content);
    records = Array.isArray(parsed) ? parsed : [parsed];
  } else {
    throw new Error(`Unsupported file format: ${ext}`);
  }

  console.log(`Parsed ${records.length} records from ${filePath}`);

  // Send records to ingest API (collecting errors for partial failure handling)
  const errors: Array<{ index: number; error: string }> = [];
  for (let i = 0; i < records.length; i++) {
    const response = await fetch("http://localhost:4000/ingest/sales", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(records[i]),
    });

    if (!response.ok) {
      errors.push({ index: i, error: response.statusText });
    }
  }

  if (errors.length > 0) {
    console.error(`Failed to ingest ${errors.length} records:`, errors);
  }
  console.log(`Successfully loaded ${records.length - errors.length} of ${records.length} records`);
}

// CLI execution
const filePath = process.argv[2];
if (!filePath) {
  console.error("Usage: npx ts-node scripts/load-file.ts <path-to-file>");
  process.exit(1);
}

loadFile(filePath).catch(console.error);
```

> **Note:** This script sends records one at a time for simplicity. For production workloads with large files, consider batch ingestion by posting arrays of records to improve throughput.

### Step 3: Create Sample Data

Create `sample-data/west-store-001.csv`:

```csv
transactionId,saleTimestamp,storeId,region,customerId,productId,productCategory,quantity,unitPrice,totalAmount
TXN-W001-20250115-0001,2025-01-15T09:15:00Z,STORE-W001,West,CUST-1001,PROD-ELE-001,Electronics,2,299.99,599.98
TXN-W001-20250115-0002,2025-01-15T09:32:00Z,STORE-W001,West,CUST-1002,PROD-CLO-015,Clothing,1,49.99,49.99
TXN-W001-20250115-0003,2025-01-15T10:05:00Z,STORE-W001,West,,PROD-FOO-042,Food,3,12.99,38.97
TXN-W001-20250115-0004,2025-01-15T10:45:00Z,STORE-W001,West,CUST-1003,PROD-HOM-008,Home,1,89.99,89.99
TXN-W001-20250115-0005,2025-01-15T11:20:00Z,STORE-W001,West,CUST-1004,PROD-SPO-022,Sports,2,34.99,69.98
TXN-W001-20250115-0006,2025-01-15T12:00:00Z,STORE-W001,West,,PROD-ELE-055,Electronics,1,149.99,149.99
TXN-W001-20250115-0007,2025-01-15T13:30:00Z,STORE-W001,West,CUST-1005,PROD-CLO-033,Clothing,3,29.99,89.97
TXN-W001-20250115-0008,2025-01-15T14:15:00Z,STORE-W001,West,CUST-1001,PROD-FOO-018,Food,5,8.99,44.95
TXN-W001-20250115-0009,2025-01-15T15:45:00Z,STORE-W001,West,CUST-1006,PROD-HOM-041,Home,1,199.99,199.99
TXN-W001-20250115-0010,2025-01-15T16:30:00Z,STORE-W001,West,,PROD-ELE-012,Electronics,1,599.99,599.99
```

**Note:** Empty `customerId` fields represent customers without loyalty cards (about 30% of transactions).

### Step 4: Load Sample Data

```bash
mkdir -p sample-data
# Create the CSV file above, then:
npx ts-node scripts/load-file.ts sample-data/west-store-001.csv
```

You should see:
```
Parsed 10 records from sample-data/west-store-001.csv
Successfully loaded 10 records
```

Verify in ClickHouse:

```bash
clickhouse-client --host localhost --port 9000 --user panda --password pandapass \
  --query "SELECT count() FROM local.sales_SalesTransaction"
```

### Step 5: Add Data Validation (Recommended)

Production data is messy. Before loading files, add validation to catch common issues early.

Update `scripts/load-file.ts` to include validation:

```typescript
// Add this validation function before the loadFile function

interface ValidationError {
  row: number;
  field: string;
  message: string;
}

function validateRecord(record: SalesTransaction, index: number): ValidationError[] {
  const errors: ValidationError[] = [];

  // Required fields
  if (!record.transactionId) {
    errors.push({ row: index, field: "transactionId", message: "Missing required field" });
  }
  if (!record.saleTimestamp) {
    errors.push({ row: index, field: "saleTimestamp", message: "Missing required field" });
  }
  if (!record.storeId) {
    errors.push({ row: index, field: "storeId", message: "Missing required field" });
  }

  // Date format validation
  if (record.saleTimestamp && isNaN(Date.parse(record.saleTimestamp))) {
    errors.push({ row: index, field: "saleTimestamp", message: "Invalid date format (expected ISO 8601)" });
  }

  // Future date check - transactions shouldn't be in the future
  if (record.saleTimestamp && !isNaN(Date.parse(record.saleTimestamp))) {
    const saleDate = new Date(record.saleTimestamp);
    const now = new Date();
    if (saleDate > now) {
      errors.push({ row: index, field: "saleTimestamp", message: "Future dates not allowed" });
    }
  }

  // Numeric validation - use Number.isFinite() to catch NaN/undefined
  if (!Number.isFinite(record.totalAmount)) {
    errors.push({ row: index, field: "totalAmount", message: "Amount must be a valid number" });
  } else if (record.totalAmount < 0) {
    errors.push({ row: index, field: "totalAmount", message: "Negative amount not allowed" });
  }

  if (!Number.isFinite(record.quantity)) {
    errors.push({ row: index, field: "quantity", message: "Quantity must be a valid number" });
  } else if (record.quantity <= 0) {
    errors.push({ row: index, field: "quantity", message: "Quantity must be positive" });
  }

  if (!Number.isFinite(record.unitPrice)) {
    errors.push({ row: index, field: "unitPrice", message: "Unit price must be a valid number" });
  }

  // Business rule validation - only check if all numeric fields are valid
  if (Number.isFinite(record.quantity) && Number.isFinite(record.unitPrice) && Number.isFinite(record.totalAmount)) {
    const expectedTotal = record.quantity * record.unitPrice;
    if (Math.abs(record.totalAmount - expectedTotal) > 0.01) {
      errors.push({
        row: index,
        field: "totalAmount",
        message: `Amount mismatch: expected ${expectedTotal.toFixed(2)}, got ${record.totalAmount}`
      });
    }
  }

  return errors;
}

// Then in loadFile(), add validation before sending:
// const validationErrors = records.flatMap((r, i) => validateRecord(r, i));
// if (validationErrors.length > 0) {
//   console.error("Validation errors:", validationErrors);
//   // Decide: skip invalid rows or abort entire file
// }
```

**Common data quality issues to catch:**

| Issue | Detection | Impact if Missed |
|-------|-----------|------------------|
| Missing transaction ID | Check for empty/null | Duplicate records, can't track lineage |
| Invalid date format | `Date.parse()` returns NaN | Query filters fail silently |
| Negative amounts | `totalAmount < 0` | Incorrect revenue totals |
| Math errors | `quantity Ã— unitPrice â‰  totalAmount` | Inconsistent reporting |
| Future dates | `saleTimestamp > now()` | Skews time-series analysis |

**Tip:** For production, consider rejecting entire files with validation errors rather than partial loads. This ensures data consistency and makes debugging easier.

---

## Part 4: Time-Series Aggregation

**Time: ~8 minutes**

Now let's create a materialized view that pre-aggregates daily summaries. This is where ClickHouse's time-series capabilities shine.

### Understanding ClickHouse Materialized Views

Before creating the view, understand how ClickHouse materialized views work:

- Materialized views execute on **INSERT**, not on a scheduleâ€”they process data as it arrives
- Data is aggregated per insert batch, not globally across all data
- Background merges eventually consolidate data, but **always use GROUP BY** when querying for accurate results
- SummingMergeTree works for `sum()` and `count()` only

### Step 1: Create Materialized View

Create `app/views/DailySalesSummary.ts`:

```typescript
import { MaterializedView, sql } from "@514labs/moose-lib";
import { salesPipeline } from "../ingest/SalesTransaction";

// Define the aggregated data model
export interface DailySalesSummary {
  saleDate: Date;
  region: string;
  productCategory: string;
  transactionCount: number;
  totalQuantity: number;
  totalRevenue: number;
}

// Get table reference from pipeline
const salesTable = salesPipeline.table;

// Create materialized view with auto-created target table
export const dailySalesMV = new MaterializedView<DailySalesSummary>({
  tableName: "DailySalesSummary",
  materializedViewName: "DailySalesSummary_MV",
  orderByFields: ["saleDate", "region", "productCategory"],
  selectTables: [salesTable],
  selectStatement: sql`
    SELECT
      toStartOfDay(saleTimestamp) AS saleDate,
      region,
      productCategory,
      count() AS transactionCount,
      sum(quantity) AS totalQuantity,
      sum(totalAmount) AS totalRevenue
    FROM ${salesTable}
    GROUP BY saleDate, region, productCategory
  `,
});
```

**Key Points:**
- `toStartOfDay()` truncates timestamps to daily granularity
- ORDER BY `saleDate` first enables efficient date-range queries
- SummingMergeTree (default) automatically merges rows with same ORDER BY keys

### Step 2: Update Exports

Add the view export to `app/index.ts`:

```typescript
export * from "./ingest/SalesTransaction";
export * from "./views/DailySalesSummary";
```

### Step 3: Verify Materialized View

Check that the materialized view was created:

```bash
clickhouse-client --host localhost --port 9000 --user panda --password pandapass \
  --query "SHOW TABLES FROM local"
```

You should see both `sales_SalesTransaction` and `DailySalesSummary`.

**What Just Happened:**

You created a materialized view that transforms your data pipeline:

1. **Triggers on INSERT** - Every time data lands in `sales_SalesTransaction`, ClickHouse automatically runs the aggregation query
2. **Pre-computes daily totals** - `toStartOfDay()` groups transactions by calendar day, collapsing potentially millions of rows into one row per day/region/category
3. **Creates a target table** - `DailySalesSummary` stores the pre-aggregated results permanently
4. **Uses SummingMergeTree** - Background merges automatically combine partial aggregates into final totals

**Why this matters for performance:**

| Query Target | Rows to Scan | Typical Query Time |
|--------------|--------------|-------------------|
| Raw transactions (1 year) | ~10 million | 2-10 seconds |
| Daily summaries (1 year) | ~1,500 | 10-50 milliseconds |

The materialized view gives you **100x faster queries** by doing the aggregation work once at insert time, not on every query.

### Step 4: Query the Aggregated Data

Query the pre-aggregated data (always use GROUP BY with SummingMergeTree):

```bash
clickhouse-client --host localhost --port 9000 --user panda --password pandapass \
  --query "
    SELECT
      saleDate,
      region,
      sum(transactionCount) AS transactions,
      sum(totalRevenue) AS revenue
    FROM local.DailySalesSummary
    GROUP BY saleDate, region
    ORDER BY saleDate, region
  "
```

### ClickHouse Time-Series Functions Reference

| Function | Output | Use Case |
|----------|--------|----------|
| `toStartOfHour(ts)` | `2025-01-15 10:00:00` | Intraday patterns |
| `toStartOfDay(ts)` | `2025-01-15` | Daily summaries |
| `toStartOfWeek(ts, 1)` | `2025-01-13` | Weekly trends (Monday start) |
| `toStartOfMonth(ts)` | `2025-01-01` | Monthly reporting |
| `toHour(ts)` | `10` (integer) | Hour-of-day analysis |
| `toDayOfWeek(ts)` | `3` (Wednesday) | Day-of-week patterns |

**Teaching Point:** Pre-aggregated materialized views typically improve query performance by 10-100x compared to scanning raw transaction data.

---

## Part 5: Report API

**Time: ~6 minutes**

Now let's build a consumption API that exposes the aggregated data with parameterized date ranges.

### Step 1: Create Consumption API

Create `app/apis/salesReport.ts`:

```typescript
import { Api } from "@514labs/moose-lib";
import { dailySalesMV } from "../views/DailySalesSummary";

// Define query parameters
export interface SalesReportParams {
  startDate: string;   // Format: YYYY-MM-DD
  endDate: string;     // Format: YYYY-MM-DD
  region?: string;     // Optional filter
}

// Define response interface
export interface SalesReportResponse {
  saleDate: string;
  region: string;
  productCategory: string;
  transactionCount: number;
  totalQuantity: number;
  totalRevenue: number;
}

// Create the API endpoint
export const salesReportApi = new Api<SalesReportParams, SalesReportResponse[]>(
  "sales-report",
  async ({ startDate, endDate, region }, { client, sql }) => {
    // Access the target table from the materialized view
    const targetTable = dailySalesMV.targetTable;

    // Build query with optional region filter
    // Note: We use separate queries for clarity. Production code could
    // consolidate these using conditional query building.
    let query;
    if (region) {
      query = sql`
        SELECT
          saleDate,
          region,
          productCategory,
          sum(transactionCount) AS transactionCount,
          sum(totalQuantity) AS totalQuantity,
          sum(totalRevenue) AS totalRevenue
        FROM ${targetTable}
        WHERE saleDate BETWEEN ${startDate} AND ${endDate}
          AND region = ${region}
        GROUP BY saleDate, region, productCategory
        ORDER BY saleDate DESC, totalRevenue DESC
      `;
    } else {
      query = sql`
        SELECT
          saleDate,
          region,
          productCategory,
          sum(transactionCount) AS transactionCount,
          sum(totalQuantity) AS totalQuantity,
          sum(totalRevenue) AS totalRevenue
        FROM ${targetTable}
        WHERE saleDate BETWEEN ${startDate} AND ${endDate}
        GROUP BY saleDate, region, productCategory
        ORDER BY saleDate DESC, totalRevenue DESC
      `;
    }

    const result = await client.query.execute<SalesReportResponse>(query);
    return result.json();
  }
);
```

### Step 2: Update Exports

Add the API export to `app/index.ts`:

```typescript
export * from "./ingest/SalesTransaction";
export * from "./views/DailySalesSummary";
export * from "./apis/salesReport";
```

**What Just Happened:**

You created a consumption API that:

1. **Exposes a REST endpoint** at `/consumption/sales-report` - accessible via HTTP GET
2. **Accepts query parameters** - `startDate`, `endDate`, and optional `region` filter
3. **Queries the materialized view** - Not the raw transaction table, so responses are fast
4. **Uses parameterized SQL** - The `sql` template literal prevents SQL injection automatically
5. **Returns typed JSON** - Response matches your `SalesReportResponse` interface

This API becomes the single source of truth for report data. The PDF generator, dashboards, and any other consumers all use this same endpoint, ensuring consistency.

### Step 3: Test the API

Query your report endpoint:

```bash
curl "http://localhost:4000/consumption/sales-report?startDate=2025-01-01&endDate=2025-01-31"
```

Filter by region:

```bash
curl "http://localhost:4000/consumption/sales-report?startDate=2025-01-15&endDate=2025-01-15&region=West"
```

The response contains aggregated daily sales data in JSON format, ready for transformation into PDF or any other output format.

---

## Part 6: PDF Export

**Time: ~8 minutes**

Now let's transform the JSON report data into a professional PDF document that stakeholders can download, email, and archive.

### Step 1: Install pdfmake

```bash
npm install pdfmake
npm install -D @types/pdfmake
```

### Step 2: Create PDF Export Script

Create `scripts/generate-pdf-report.ts`:

```typescript
import PdfPrinter from "pdfmake";
import { TDocumentDefinitions, TFontDictionary } from "pdfmake/interfaces";
import * as fs from "fs";

// Use standard PDF fonts (no external files needed)
const fonts: TFontDictionary = {
  Roboto: {
    normal: "Helvetica",
    bold: "Helvetica-Bold",
    italics: "Helvetica-Oblique",
    bolditalics: "Helvetica-BoldOblique",
  },
};

// Report data interface (matches API response)
interface SalesReportRow {
  saleDate: string;
  region: string;
  productCategory: string;
  transactionCount: number;
  totalQuantity: number;
  totalRevenue: number;
}

async function fetchReportData(
  startDate: string,
  endDate: string
): Promise<SalesReportRow[]> {
  const url = `http://localhost:4000/consumption/sales-report?startDate=${startDate}&endDate=${endDate}`;
  const response = await fetch(url);

  if (!response.ok) {
    throw new Error(`API request failed: ${response.statusText}`);
  }

  return response.json();
}

function aggregateByRegion(
  data: SalesReportRow[]
): Map<string, { revenue: number; transactions: number }> {
  const regionMap = new Map<string, { revenue: number; transactions: number }>();

  for (const row of data) {
    const existing = regionMap.get(row.region) || { revenue: 0, transactions: 0 };
    regionMap.set(row.region, {
      revenue: existing.revenue + row.totalRevenue,
      transactions: existing.transactions + row.transactionCount,
    });
  }

  return regionMap;
}

function aggregateByCategory(
  data: SalesReportRow[]
): Map<string, { revenue: number; transactions: number }> {
  const categoryMap = new Map<string, { revenue: number; transactions: number }>();

  for (const row of data) {
    const existing = categoryMap.get(row.productCategory) || {
      revenue: 0,
      transactions: 0,
    };
    categoryMap.set(row.productCategory, {
      revenue: existing.revenue + row.totalRevenue,
      transactions: existing.transactions + row.transactionCount,
    });
  }

  return categoryMap;
}

function formatCurrency(amount: number): string {
  return `$${amount.toLocaleString("en-US", { minimumFractionDigits: 2, maximumFractionDigits: 2 })}`;
}

function generatePdf(
  data: SalesReportRow[],
  reportDate: string,
  outputPath: string
): void {
  const printer = new PdfPrinter(fonts);

  // Calculate totals
  const totalRevenue = data.reduce((sum, row) => sum + row.totalRevenue, 0);
  const totalTransactions = data.reduce(
    (sum, row) => sum + row.transactionCount,
    0
  );

  // Aggregate by region and category
  const regionData = aggregateByRegion(data);
  const categoryData = aggregateByCategory(data);

  // Sort regions by revenue descending
  const sortedRegions = Array.from(regionData.entries()).sort(
    (a, b) => b[1].revenue - a[1].revenue
  );

  // Sort categories by revenue descending
  const sortedCategories = Array.from(categoryData.entries()).sort(
    (a, b) => b[1].revenue - a[1].revenue
  );

  const docDefinition: TDocumentDefinitions = {
    content: [
      // Header
      { text: "APEX RETAIL", style: "companyName" },
      { text: "Daily Sales Summary", style: "reportTitle" },
      { text: reportDate, style: "reportDate" },
      {
        canvas: [
          { type: "line", x1: 0, y1: 5, x2: 515, y2: 5, lineWidth: 1 },
        ],
      },

      // Executive Summary
      {
        text: "EXECUTIVE SUMMARY",
        style: "sectionHeader",
        margin: [0, 15, 0, 10],
      },
      {
        columns: [
          {
            text: [
              { text: "Total Revenue: ", bold: true },
              formatCurrency(totalRevenue),
            ],
          },
          {
            text: [
              { text: "Transactions: ", bold: true },
              totalTransactions.toLocaleString(),
            ],
          },
        ],
        margin: [0, 0, 0, 15],
      },

      // Revenue by Region
      {
        text: "REVENUE BY REGION",
        style: "sectionHeader",
        margin: [0, 10, 0, 10],
      },
      {
        table: {
          headerRows: 1,
          widths: ["*", "auto", "auto", "auto"],
          body: [
            [
              { text: "Region", style: "tableHeader" },
              { text: "Revenue", style: "tableHeader" },
              { text: "Transactions", style: "tableHeader" },
              { text: "Share", style: "tableHeader" },
            ],
            ...sortedRegions.map(([region, stats]) => [
              region,
              { text: formatCurrency(stats.revenue), alignment: "right" },
              { text: stats.transactions.toLocaleString(), alignment: "right" },
              {
                text: `${totalRevenue > 0 ? ((stats.revenue / totalRevenue) * 100).toFixed(1) : "0.0"}%`,
                alignment: "right",
              },
            ]),
          ],
        },
        layout: "lightHorizontalLines",
      },

      // Revenue by Category
      {
        text: "REVENUE BY CATEGORY",
        style: "sectionHeader",
        margin: [0, 20, 0, 10],
      },
      {
        table: {
          headerRows: 1,
          widths: ["*", "auto", "auto", "auto"],
          body: [
            [
              { text: "Category", style: "tableHeader" },
              { text: "Revenue", style: "tableHeader" },
              { text: "Transactions", style: "tableHeader" },
              { text: "Share", style: "tableHeader" },
            ],
            ...sortedCategories.map(([category, stats]) => [
              category,
              { text: formatCurrency(stats.revenue), alignment: "right" },
              { text: stats.transactions.toLocaleString(), alignment: "right" },
              {
                text: `${totalRevenue > 0 ? ((stats.revenue / totalRevenue) * 100).toFixed(1) : "0.0"}%`,
                alignment: "right",
              },
            ]),
          ],
        },
        layout: "lightHorizontalLines",
      },
    ],

    footer: {
      text: `Generated: ${new Date().toISOString()} | Apex Retail Confidential`,
      style: "footer",
      margin: [40, 0],
    },

    styles: {
      companyName: { fontSize: 24, bold: true, alignment: "center" },
      reportTitle: {
        fontSize: 16,
        alignment: "center",
        margin: [0, 5, 0, 5],
      },
      reportDate: { fontSize: 12, alignment: "center", color: "gray" },
      sectionHeader: { fontSize: 12, bold: true },
      tableHeader: { bold: true, fillColor: "#eeeeee" },
      footer: { fontSize: 8, color: "gray", alignment: "center" },
    },
    defaultStyle: { font: "Roboto", fontSize: 10 },
    pageMargins: [40, 40, 40, 60],
  };

  const pdfDoc = printer.createPdfKitDocument(docDefinition);
  pdfDoc.pipe(fs.createWriteStream(outputPath));
  pdfDoc.end();
  console.log(`PDF report saved to ${outputPath}`);
}

// Main execution
async function main() {
  const startDate = process.argv[2] || "2025-01-01";
  const endDate = process.argv[3] || "2025-12-31";
  const outputPath = process.argv[4] || "apex-retail-report.pdf";

  console.log(`Fetching report data from ${startDate} to ${endDate}...`);

  try {
    const data = await fetchReportData(startDate, endDate);

    if (data.length === 0) {
      console.log("No data found for the specified date range.");
      return;
    }

    console.log(`Generating PDF with ${data.length} data rows...`);
    generatePdf(data, `${startDate} to ${endDate}`, outputPath);
  } catch (error) {
    console.error("Error generating report:", error);
    process.exit(1);
  }
}

main();
```

### Step 3: Generate the Report

With your Moose dev server running, execute:

```bash
npx ts-node scripts/generate-pdf-report.ts 2025-01-15 2025-01-15 january-15-report.pdf
```

### Step 4: Verify the Output

Open `january-15-report.pdf` to see your static report with:
- Professional header with company name and date range
- Executive summary totals (revenue, transactions)
- Revenue breakdown by region with percentage share
- Revenue breakdown by category with percentage share
- Footer with generation timestamp

**Key Insight:** The PDF is generated from pre-aggregated ClickHouse data that powers your API. This means PDF generation is nearly instantaneous regardless of how many raw transactions exist in your system.

---

## Part 7: How It Works (Conceptual Deep Dive)

*This section explains the technical concepts behind what you built. Read this to understand why materialized views are so fast and when to use different aggregation strategies.*

### Why ClickHouse is Fast for Analytics

ClickHouse is a **columnar** database, which stores data fundamentally differently from traditional row-oriented databases like PostgreSQL or MySQL.

**Row-oriented storage (PostgreSQL, MySQL):**

Data is stored as complete rows, one after another:

```
Record 1: [txn_id=1, timestamp=..., store=W001, region=West, amount=599.98]
Record 2: [txn_id=2, timestamp=..., store=W001, region=West, amount=49.99]
Record 3: [txn_id=3, timestamp=..., store=W002, region=East, amount=89.99]
```

When you query `SELECT SUM(amount) FROM sales WHERE region='West'`, the database must:
1. Read every complete row from disk
2. Check if region matches 'West'
3. If yes, add amount to the sum
4. Repeat for millions of rows

**Problem:** You read 100% of the data to use 2% of it (just `region` and `amount` columns).

**Columnar storage (ClickHouse):**

Data is stored column-by-column:

```
txn_ids:    [1, 2, 3, ...]
timestamps: [2025-01-15T09:00, 2025-01-15T09:30, ...]
stores:     [W001, W001, W002, ...]
regions:    [West, West, East, ...]
amounts:    [599.98, 49.99, 89.99, ...]
```

When you query `SELECT SUM(amount) FROM sales WHERE region='West'`, ClickHouse:
1. Reads only the `region` column (small!)
2. Reads only the `amount` column (small!)
3. Scans both in parallel, summing where region matches
4. Never touches `txn_id`, `timestamp`, `store`, etc.

**Result:** Read 2% of the data, use 100% of what you read. **10-100x faster.**

### Why Materialized Views Beat Batch Jobs

Traditional approach: Run a scheduled job (cron, Airflow) every hour/day to aggregate data.

```
Raw Data â†’ [Wait for schedule] â†’ Batch Job â†’ Aggregated Table
```

**Problems:**
- Data is stale between runs
- Batch job must scan all new data since last run
- If batch fails, data is missing until next run
- Scaling requires bigger batch windows or more complex incremental logic

**ClickHouse materialized views:**

```
Raw Data â†’ [INSERT triggers MV] â†’ Aggregated Table (immediate)
```

**How it works:**

1. You INSERT a transaction into `sales_SalesTransaction`
2. ClickHouse immediately runs your MV's SELECT statement on the new rows
3. Results are INSERTed into `DailySalesSummary`
4. This happens synchronouslyâ€”by the time INSERT returns, the MV is updated

**Benefits:**
- Data is always current (milliseconds latency, not hours)
- No separate batch infrastructure to maintain
- Scales automatically with write throughput
- If an INSERT fails, the MV isn't corrupted

### Understanding SummingMergeTree

Your materialized view uses `SummingMergeTree`, a ClickHouse table engine designed for aggregation.

**How it works:**

1. When data arrives, ClickHouse writes it to disk immediately (fast writes)
2. In the background, ClickHouse periodically **merges** data files
3. During merge, rows with the same ORDER BY keys are combined:
   - Numeric columns are summed
   - Other columns take the last value

**Example:**

Two separate INSERTs for the same day/region/category:

```
INSERT 1: [saleDate=2025-01-15, region=West, category=Electronics, count=5, revenue=1000]
INSERT 2: [saleDate=2025-01-15, region=West, category=Electronics, count=3, revenue=600]
```

After background merge:

```
Merged:   [saleDate=2025-01-15, region=West, category=Electronics, count=8, revenue=1600]
```

**Important:** Merges happen asynchronously. Until merge completes, you might see both rows. That's why you must always use `GROUP BY` when querying:

```sql
-- Always correct, regardless of merge state
SELECT saleDate, region, sum(transactionCount), sum(totalRevenue)
FROM DailySalesSummary
GROUP BY saleDate, region
```

### When to Use Different Aggregation Strategies

| Strategy | Use When | Example |
|----------|----------|---------|
| **Raw queries** | Ad-hoc exploration, data is small (<1M rows) | Debugging, one-time analysis |
| **Materialized view** | Repeated queries, predictable aggregation patterns | Daily summaries, dashboards |
| **Pre-aggregated API** | External consumers, need caching/rate limiting | PDF reports, third-party integrations |

**Rule of thumb:** If you run the same aggregation query more than once, create a materialized view.

### Time-Series Best Practices

**1. Always order by time first:**

```typescript
orderByFields: ["saleTimestamp", "transactionId"]  // Good
orderByFields: ["transactionId", "saleTimestamp"]  // Bad for time queries
```

ClickHouse uses the ORDER BY for data layout on disk. Time-first ordering means date-range queries skip irrelevant data blocks entirely.

**2. Use appropriate time granularity:**

| Granularity | Function | Rows/Year (1 store) | Use Case |
|-------------|----------|---------------------|----------|
| Hourly | `toStartOfHour()` | ~8,760 | Intraday patterns |
| Daily | `toStartOfDay()` | ~365 | Standard reporting |
| Weekly | `toStartOfWeek()` | ~52 | Trend analysis |
| Monthly | `toStartOfMonth()` | ~12 | Executive summaries |

Finer granularity = more rows = slower queries but more detail. Choose based on your reporting needs.

**3. Consider multiple aggregation levels:**

For large-scale systems, create MVs at multiple granularities:

```
Raw transactions â†’ Hourly MV â†’ Daily MV â†’ Monthly MV
```

Each level is 10-30x smaller than the previous, enabling fast queries at any time scale.

### Summary

You now understand:

- **Columnar storage** - Why ClickHouse reads less data than row-oriented databases
- **Materialized views** - How they provide real-time aggregation without batch jobs
- **SummingMergeTree** - Why you need GROUP BY and how background merges work
- **Time-series patterns** - How to structure data for efficient time-range queries

These concepts apply beyond this tutorialâ€”they're fundamental to building any high-performance analytics system.

---

## Wrap-Up

### What You Built

Congratulations! You've built a complete data pipeline:

```
CSV/JSON Files â†’ Ingest API â†’ ClickHouse Table â†’ Materialized View â†’ API â†’ PDF
```

| Component | Purpose |
|-----------|---------|
| `SalesTransaction` model | Type-safe schema auto-provisioned to ClickHouse |
| File loader script | Parses CSV/JSON and sends to ingest API |
| `DailySalesSummary` MV | Pre-aggregates daily metrics at insert time |
| `sales-report` API | Parameterized endpoint for date-range queries |
| PDF generator | Transforms JSON into executive summary |

### Before vs After

**Before (Manual Process):**
- Analyst downloads 12 CSV files from store servers each morning
- Copies data into Excel, writes formulas for aggregations
- Formats a Word document with tables and charts
- Emails PDF to 4 regional managers
- **Time: 2+ hours daily**
- **Reliability: Depends on one person being available**
- **Audit trail: None**

**After (Automated Pipeline):**
- Store POS systems upload files to ingest API automatically
- ClickHouse stores and pre-aggregates data in real-time
- API serves any date range on demand
- PDF generation takes < 1 second
- **Time: < 1 minute (automated)**
- **Reliability: Runs 24/7, no human intervention**
- **Audit trail: Full version control, reproducible outputs**

**Business Impact:**
- **500+ analyst hours/year** freed for higher-value work
- **Zero delayed reports** due to sick days or vacations
- **Consistent numbers** - no more formula errors or copy/paste mistakes
- **Historical queries** - instantly pull any past date range
- **Scalable** - works the same for 12 stores or 1,200 stores

### Performance Comparison

| Operation | Traditional OLTP | Moose + ClickHouse |
|-----------|------------------|-------------------|
| Raw transaction scan | Seconds to minutes | Milliseconds |
| Daily aggregation query | 10-30+ seconds | `<100ms` |
| Report generation | Minutes (compute + format) | `<1 second` |

The pre-aggregated materialized view delivers 10-100x faster query performance, making PDF generation nearly instantaneous.

### What You Can Do Now

With this pipeline in place, you can:

1. **Generate reports for any date range** - Just change the API parameters
2. **Add new aggregations** - Create additional materialized views (hourly, weekly, by product)
3. **Build dashboards** - Connect BI tools directly to the API
4. **Schedule automated delivery** - Use cron or Temporal to email reports daily
5. **Scale to more stores** - Same code handles 10x more data with no changes
6. **Add data quality alerts** - Extend validation to catch anomalies

### Next Steps

To extend this implementation:

1. **Add more dimensions** - Customer, Product, Store dimension tables for a star schema
2. **Multiple aggregation levels** - Hourly, weekly, monthly materialized views
3. **Excel export** - Add ExcelJS for spreadsheet output alongside PDF
4. **Scheduled delivery** - Use Temporal workflows for automated report delivery
5. **Email integration** - Send PDF reports to stakeholders automatically
6. **Deploy to production** - Use Boreal Cloud for managed hosting

---

## Reference

### Sample Queries

**Daily revenue by region:**

```sql
SELECT
  saleDate,
  region,
  sum(totalRevenue) AS revenue,
  sum(transactionCount) AS transactions
FROM local.DailySalesSummary
GROUP BY saleDate, region
ORDER BY saleDate DESC, revenue DESC;
```

**Hourly sales pattern (raw data):**

```sql
SELECT
  toHour(saleTimestamp) AS hour_of_day,
  count() AS transactions,
  sum(totalAmount) AS revenue
FROM local.sales_SalesTransaction
GROUP BY hour_of_day
ORDER BY hour_of_day;
```

**Top categories in date range:**

```sql
SELECT
  productCategory,
  sum(totalRevenue) AS revenue,
  sum(transactionCount) AS transactions
FROM local.DailySalesSummary
WHERE saleDate BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY productCategory
ORDER BY revenue DESC
LIMIT 5;
```

### Project Structure

```
sales-reports/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ index.ts                    # Export all modules
â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â””â”€â”€ SalesTransaction.ts     # IngestPipeline definition
â”‚   â”œâ”€â”€ views/
â”‚   â”‚   â””â”€â”€ DailySalesSummary.ts    # MaterializedView
â”‚   â””â”€â”€ apis/
â”‚       â””â”€â”€ salesReport.ts          # Consumption API
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load-file.ts                # CSV/JSON file loader
â”‚   â””â”€â”€ generate-pdf-report.ts      # PDF generator
â”œâ”€â”€ sample-data/
â”‚   â””â”€â”€ west-store-001.csv          # Sample transactions
â”œâ”€â”€ moose.config.toml
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

### Configuration Reference

Your `moose.config.toml` file:

```toml
[project]
name = "sales-reports"
language = "typescript"

[features]
olap = true
streaming = true

[clickhouse_config]
db_name = "local"
host = "localhost"
host_port = 18123
native_port = 9000
user = "panda"
password = "pandapass"
use_ssl = false
```

### Troubleshooting

#### Setup Issues

**Port 4000 in use:**

```bash
# Find what's using port 4000
lsof -i :4000

# Kill the process if needed
kill -9 <PID>
```

**ClickHouse connection refused:**

After running `moose dev`, wait 10-15 seconds for containers to fully initialize. Verify with:

```bash
curl http://localhost:18123/ping
```

If it still fails, check Docker is running:

```bash
docker ps | grep clickhouse
```

**Docker not starting:**

```bash
# Check Docker daemon status
docker info

# On Mac/Windows: Open Docker Desktop and ensure it's running
# On Linux: sudo systemctl start docker
```

**Model not detected after saving:**

- Ensure file is saved (check for unsaved indicator in editor)
- Verify export in `app/index.ts`
- Check `moose dev` terminal for TypeScript compilation errors
- Restart `moose dev` if hot-reload isn't working

#### Data Issues

**CSV date parsing fails:**

Dates must use ISO 8601 format. Common mistakes:

| Wrong Format | Correct Format |
|--------------|----------------|
| `01/15/2025` (US format) | `2025-01-15T09:00:00Z` |
| `15-01-2025` (EU format) | `2025-01-15T09:00:00Z` |
| `2025-1-15` (no leading zeros) | `2025-01-15T09:00:00Z` |

**Ingestion returns error:**

Verify your JSON matches the TypeScript interface exactly:

```bash
# Test with minimal valid payload
curl -X POST http://localhost:4000/ingest/sales \
  -H "Content-Type: application/json" \
  -d '{"transactionId":"TEST-001","saleTimestamp":"2025-01-15T10:00:00Z","storeId":"STORE-001","region":"West","productId":"PROD-001","productCategory":"Electronics","customerId":"","quantity":1,"unitPrice":10.00,"totalAmount":10.00}'
```

Common type mismatches:
- `quantity` must be a number, not `"1"` (string)
- `totalAmount` must be a number, not `"10.00"` (string)
- `saleTimestamp` must be ISO format string

**Duplicate transaction IDs:**

ClickHouse doesn't enforce uniqueness. Check for duplicates:

```sql
SELECT transactionId, count() as cnt
FROM local.sales_SalesTransaction
GROUP BY transactionId
HAVING cnt > 1
```

To deduplicate queries, use the `FINAL` modifier:

```sql
SELECT * FROM local.sales_SalesTransaction FINAL
```

#### Query Issues

**Materialized view returns unexpected results:**

Remember: MVs aggregate per-batch, not globally. Always use GROUP BY:

```sql
-- Wrong (may show partial aggregates)
SELECT * FROM local.DailySalesSummary

-- Correct (consolidates partial aggregates)
SELECT
  saleDate,
  region,
  sum(transactionCount) AS transactionCount,
  sum(totalRevenue) AS totalRevenue
FROM local.DailySalesSummary
GROUP BY saleDate, region
```

**API returns empty results:**

1. Check date format uses `YYYY-MM-DD`:
   ```bash
   # Wrong
   curl "http://localhost:4000/consumption/sales-report?startDate=01-15-2025&endDate=01-31-2025"

   # Correct
   curl "http://localhost:4000/consumption/sales-report?startDate=2025-01-15&endDate=2025-01-31"
   ```

2. Verify data exists for the date range:
   ```sql
   SELECT min(saleDate), max(saleDate) FROM local.DailySalesSummary
   ```

**Query is slow:**

If queries take more than 100ms, check:
- Are you querying the materialized view or raw table?
- Is the date filter using the indexed column?
- Run `EXPLAIN` to see query plan:
  ```sql
  EXPLAIN SELECT ... FROM local.DailySalesSummary WHERE saleDate BETWEEN ...
  ```

#### PDF Issues

**PDF not generating:**

1. Ensure API is accessible:
   ```bash
   curl "http://localhost:4000/consumption/sales-report?startDate=2025-01-15&endDate=2025-01-15"
   ```

2. Check for fetch errors in the script output

3. Verify pdfmake is installed:
   ```bash
   npm list pdfmake
   ```

**PDF is empty or malformed:**

- Check API response has data (not empty array)
- Verify date range contains transactions
- Check script error output for JSON parsing issues

### Resources

- **Moose Documentation:** https://docs.fiveonefour.com/moose
- **Moose OLAP Guide:** https://docs.fiveonefour.com/moose/olap
- **Materialized Views:** https://docs.fiveonefour.com/moose/building/materialized-views
- **GitHub Repository:** https://github.com/514-labs/moosestack
- **Moose Quickstart:** https://docs.fiveonefour.com/moose/getting-started/quickstart
