---
title: QVD to ClickHouse
description: Transfer data from QlikView QVD files to ClickHouse using MooseStack workflows
---

import { Callout, BulletPointsCard, CTACards, CTACard } from "@/components/mdx";

# Transferring QVD Files to ClickHouse

This guide shows how to transfer data from QlikView Data (QVD) files to ClickHouse using a pre-built MooseStack pipeline. The pipeline automatically generates data models from QVD schema, handles incremental updates, and provides API monitoring capabilities.

<Callout type="info" title="Prerequisites">
This guide assumes familiarity with MooseStack workflows and data modeling. If you are new to these concepts, review the [Workflows documentation](/moosestack/workflows) and [Data Warehouses guide](/guides/data-warehouses) first.
</Callout>

## Overview

QVD (QlikView Data) files are a proprietary binary format used by QlikView and Qlik Sense for storing extracted data. Migrating from Qlik to ClickHouse offers significant advantages:

- **Query performance**: ClickHouse's columnar storage provides 10-100x faster analytical queries
- **Cost efficiency**: Open-source OLAP database without per-user licensing
- **Flexibility**: Standard SQL access for BI tools, notebooks, and custom applications
- **Scalability**: Handle billions of rows with consistent sub-second query times

## Pipeline Features

The QVD-to-ClickHouse pipeline provides:

- **Universal file system support** via `fsspec` (local, S3, HTTP, FTP, Azure Blob, Google Cloud Storage)
- **Auto-generated Pydantic models** from QVD schema introspection
- **Incremental updates** with ClickHouse-based file tracking
- **Batch processing** with configurable batch size and retry logic
- **REST API monitoring** for sync status, failures, and processing history
- **Scheduled execution** via MooseStack workflows

## Installation

### Add to Your Project

Add the pipeline to your MooseStack project's `pyproject.toml`:

```toml
[project]
dependencies = [
    # ... your other dependencies
    "qvd-to-clickhouse",  # Add the QVD pipeline
]
```

Then install dependencies:

```bash
uv sync
# or
pip install -e .
```

<Callout type="tip" title="Pipeline Dependencies">
The pipeline automatically includes all required dependencies (`pyqvd`, `fsspec`, `pandas`, `tenacity`, etc.) - you don't need to install them separately.
</Callout>

## Configuration

Configure the pipeline using environment variables. Create a `.env` file in your project:

```bash filename=".env"
# Required - Path or URL to QVD files
QVD_SOURCE=/path/to/qvd/files
# or
QVD_SOURCE=s3://my-bucket/qvd-exports

# Optional - File filtering
QVD_FILE_PATTERN=*.qvd
QVD_INCLUDE_FILES=Item,PurchaseOrder,Reception,Supplier
QVD_EXCLUDE_FILES=Archive,Temp,Backup

# Optional - Processing configuration
QVD_BATCH_SIZE=10000
QVD_SCHEDULE=@daily

# Optional - Tracking (defaults to ClickHouse tracking)
QVD_USE_CLICKHOUSE_TRACKING=true

# Optional - AWS credentials for S3 sources
# (Can also use AWS_PROFILE or IAM role)
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_DEFAULT_REGION=us-east-1
```

### Configuration Options

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `QVD_SOURCE` | Yes | - | Source path or URL for QVD files |
| `QVD_FILE_PATTERN` | No | `*.qvd` | Glob pattern for file matching |
| `QVD_INCLUDE_FILES` | No | - | Comma-separated whitelist (without .qvd extension) |
| `QVD_EXCLUDE_FILES` | No | - | Comma-separated blacklist |
| `QVD_BATCH_SIZE` | No | `10000` | Rows per insert batch |
| `QVD_SCHEDULE` | No | `@daily` | Workflow schedule (`@daily`, `@hourly`, cron expression) |
| `QVD_USE_CLICKHOUSE_TRACKING` | No | `true` | Use ClickHouse for tracking vs JSON file |

### Supported File Sources

The pipeline uses `fsspec` and supports:

- **Local filesystem**: `/path/to/files`
- **AWS S3**: `s3://bucket/path`
- **HTTP/HTTPS**: `https://example.com/files`
- **FTP/SFTP**: `ftp://server/path`
- **Azure Blob Storage**: `abfs://container/path`
- **Google Cloud Storage**: `gs://bucket/path`

## Generating Data Models

The pipeline includes a CLI tool to auto-generate Pydantic models from your QVD files:

### List Available Files

First, check what QVD files are available:

```bash
# Local files
uv run init_qvd.py --list-files --source /path/to/qvd

# S3 files
uv run init_qvd.py --list-files --source s3://my-bucket/qvd-exports
```

### Generate Models

Generate Pydantic models for your QVD files:

```bash
# Generate models for all QVD files
uv run init_qvd.py --generate-models --source $QVD_SOURCE

# Generate for specific files only
uv run init_qvd.py --generate-models --source $QVD_SOURCE --files Item,PurchaseOrder

# Exclude certain files
uv run init_qvd.py --generate-models --source $QVD_SOURCE --exclude Archive,Temp
```

The tool will:
1. Read the QVD file schema
2. Generate a Pydantic model with appropriate field types
3. Create a MooseStack `OlapTable` definition
4. Save the models to `app/ingest/qvd.py`

Example generated model:

```python filename="app/ingest/qvd.py"
from moose_lib import OlapTable, Key
from pydantic import BaseModel
from typing import Optional
from datetime import datetime

class QvdItemModel(BaseModel):
    item_number: Optional[str] = None
    description: Optional[str] = None
    quantity: Optional[int] = None
    price: Optional[float] = None
    created_date: Optional[datetime] = None

QvdItemModel = OlapTable(
    name="QvdItem",
    columns=[
        Key("item_number", "String"),
        # ... other columns
    ]
)
```

<Callout type="warning" title="Regenerating Models">
If your QVD schema changes, re-run `init_qvd.py --generate-models` to update the models. Existing data will remain in ClickHouse.
</Callout>

## Running the Pipeline

### Start the Workflow

Start your MooseStack development server:

```bash
moose dev
```

The QVD sync workflow will:
1. Run on the configured schedule (default: daily)
2. List QVD files from your source
3. Apply include/exclude filters
4. Check which files are new or modified
5. Generate table names (e.g., `Item.qvd` â†’ `QvdItem`)
6. Read QVD data in batches
7. Insert into ClickHouse tables
8. Track processing status, timing, and errors

### Manual Execution

For testing or one-off imports, run the workflow manually:

```bash
# Run the sync workflow directly
uv run python -c "
from app.pipelines.qvd_to_clickhouse.app.workflows.qvd_sync_with_tracking import sync_with_tracking_task
from moose_lib import TaskContext
sync_with_tracking_task(TaskContext(None))
"
```

## Monitoring

### API Endpoint

The pipeline includes a REST API endpoint for monitoring:

```bash
# Get overall status
curl http://localhost:4001/api/qvd_status

# Filter by file name
curl "http://localhost:4001/api/qvd_status?file_name=Item"

# Show only failures
curl "http://localhost:4001/api/qvd_status?status=failed"

# Include processing history
curl "http://localhost:4001/api/qvd_status?include_history=true&limit=10"
```

Example response:

```json
{
  "summary": {
    "total_files": 45,
    "total_syncs_all_time": 1250,
    "syncs_today": 23,
    "total_rows_processed": 15000000,
    "current_failures": 0,
    "currently_processing": 0,
    "last_sync_at": "2026-02-03T10:30:00Z"
  },
  "files": [
    {
      "file_name": "Item",
      "table_name": "QvdItem",
      "status": "completed",
      "row_count": 125000,
      "file_size_bytes": 45123456,
      "processed_at": "2026-02-03T10:30:00Z",
      "processing_duration_seconds": 12.5,
      "rows_per_second": 10000.0,
      "source_type": "s3",
      "error_message": null
    }
  ],
  "failures": []
}
```

### ClickHouse Tracking

Query the tracking table directly for detailed history:

```sql
-- View all file processing history
SELECT
    file_name,
    status,
    row_count,
    processing_duration_seconds,
    processed_at
FROM local.QvdFileTracking FINAL
ORDER BY processed_at DESC
LIMIT 20;

-- Check for failures
SELECT
    file_name,
    error_message,
    processed_at
FROM local.QvdFileTracking FINAL
WHERE status = 'failed'
ORDER BY processed_at DESC;

-- View processing performance
SELECT
    file_name,
    AVG(processing_duration_seconds) as avg_duration,
    AVG(row_count / processing_duration_seconds) as avg_rows_per_sec
FROM local.QvdFileTracking
WHERE status = 'completed'
GROUP BY file_name;
```

## Error Handling

### Missing Models

If a QVD file doesn't have a corresponding model:

1. The file is marked as `failed` in the tracking table
2. The error message indicates which model is missing
3. Other files continue processing normally
4. The failed file will be retried on the next run

To fix:

```bash
# Generate the missing model
uv run init_qvd.py --generate-models --source $QVD_SOURCE --files MissingFile

# Check failures via API
curl "http://localhost:4001/api/qvd_status?status=failed"
```

### File Access Issues

For S3 or cloud storage access errors:

```bash
# Verify credentials
aws s3 ls s3://your-bucket/path/

# Check environment variables
echo $AWS_ACCESS_KEY_ID
echo $AWS_DEFAULT_REGION

# Test with verbose logging
uv run init_qvd.py --list-files --source s3://your-bucket --verbose
```

## Example Workflows

### Local Development

```bash
# 1. Configure local source
export QVD_SOURCE=/Users/me/data/qvd
export QVD_INCLUDE_FILES=Item,PurchaseOrder,Reception

# 2. Generate models
uv run init_qvd.py --generate-models --source $QVD_SOURCE

# 3. Run once to test
uv run python -c "from app.workflows.qvd_sync_with_tracking import sync_with_tracking_task; sync_with_tracking_task(None)"

# 4. Start scheduled runs
moose dev
```

### Production S3 Setup

```bash
# 1. Configure S3 source
export QVD_SOURCE=s3://production-data/qvd-exports
export QVD_SCHEDULE=@hourly
export QVD_BATCH_SIZE=50000

# 2. Use IAM role or set credentials
export AWS_DEFAULT_REGION=us-east-1

# 3. Generate models
uv run init_qvd.py --generate-models --source $QVD_SOURCE

# 4. Deploy and run
moose dev  # or deploy to production
```

## Next Steps

<CTACards>
  <CTACard
    title="Workflows Documentation"
    description="Learn more about MooseStack workflows and scheduling"
    href="/moosestack/workflows"
  />
  <CTACard
    title="Data Warehouses Guide"
    description="Explore data warehouse patterns and best practices"
    href="/guides/data-warehouses"
  />
  <CTACard
    title="ClickHouse Integration"
    description="Deep dive into ClickHouse OLAP tables"
    href="/moosestack/olap-tables"
  />
</CTACards>
