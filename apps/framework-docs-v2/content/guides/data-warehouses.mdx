# Building Your First Data Warehouse

## Build a working data warehouse in one afternoon

This guide gets you building immediately. You'll create a real analytics system that answers business questions in seconds instead of minutes or hours.

Your current operational database works great for running your application, but analytics queries are slowing things down. Reports that used to take seconds now take minutes. Your database is struggling under the weight of complex joins and aggregations across millions of rows. You need a better solution.

That's where data warehouses come in. They're purpose-built for analytics, storing data in ways that make complex queries fly. And you're about to build one yourself.

### What You'll Build

By the end of this tutorial, you'll have:

- **E-commerce analytics warehouse** with real data tracking customers, products, and orders
- **Multiple data sources consolidated** in one place (operational database, CSV files, sample data)
- **Interactive dashboard** showing key business metrics like revenue trends and top products
- **Queries that run 100x faster** than your operational database by leveraging columnar storage

This isn't a toy example. It's a real warehouse architecture you can adapt for your own data.

### Time and Prerequisites

**Time:** 3-4 hours hands-on (plus 30 minutes reading if you include the optional sections)

**You'll need:**
- Basic SQL knowledge (you can write SELECT statements and understand JOINs)
- Computer with Docker installed
- Terminal/command line access
- Text editor for code
- (Optional) Access to a PostgreSQL operational database if you want to extract from real production data

### How This Guide Works

We've organized this guide for different audiences:

**Decision-makers:** Start with Part 1 to understand business value, use cases, and ROI before committing resources.

**Developers who want context first:** Read Part 1 for the "why", then jump to Part 2 to build.

**Developers who want to build immediately:** Skip to Part 2, build your warehouse, then come back to understand the concepts in Parts 1 and 3.

### Structure

- **Part 1: Understanding Data Warehouses** (25 min read) - Why you need one, business value, ROI, and when to invest
- **Part 2: Build Your Warehouse** (3-4 hours hands-on) - Step-by-step tutorial with real code
- **Part 3: How Warehouses Work** (15 min read) - Technical deep dive into columnar storage, star schemas, and OLAP vs OLTP

**Choose your path:**
- ðŸ“Š Business/ROI focus? â†’ Read Part 1, evaluate if this is right for you
- ðŸ’» Technical/hands-on? â†’ Jump to Part 2, start building
- ðŸ§  Concepts first? â†’ Read Part 1, build in Part 2, learn details in Part 3


## Part 1: Understanding Data Warehouses

*Read this section to understand why data warehouses exist, what business problems they solve, and when to invest in one. This is essential for decision-makers and provides valuable context for developers before diving into the hands-on tutorial in Part 2.*

### The Core Problem

Your operational database is excellent at running your application, but terrible at analytics. Here's why.

**Operational databases are optimized for transactions:**

Every time a customer places an order, updates their profile, or checks inventory, your database:
1. Finds the specific record (using indexes on primary keys)
2. Reads/writes a few rows
3. Returns immediately (milliseconds)
4. Handles thousands of these per second

This is OLTP (Online Transaction Processing) - finding needles in haystacks, very fast.

**Analytics requires the opposite:**

When you want to know "What were total sales last quarter by region?", you need to:
1. Scan millions of order records
2. Aggregate across many rows
3. Join to multiple tables
4. Group and calculate totals
5. Return a summary (seconds to minutes)

This is OLAP (Online Analytical Processing) - processing haystacks to find patterns.

**The fundamental conflict:**

Your operational database cannot optimize for both. Indexes that speed up single-row lookups slow down full-table scans. Row-oriented storage that's perfect for transactions wastes I/O for analytics.

**Real-world symptoms:**

You'll recognize these problems:

- **Slow reports impact production**: Running a sales report makes the website slow for customers
- **Queries time out**: "What were our top products last year?" crashes the database
- **Can't do historical analysis**: "Compare Q1 2024 to Q1 2023" requires keeping years of data in the operational DB
- **BI tool abuse**: Business analysts run Tableau/Looker queries that lock tables
- **Replication lag**: Read replicas can't keep up with analytical query load
- **Manual exports**: Data team exports CSV files nightly because queries are too slow during business hours

### Why You Can't Just "Scale" Your Operational Database

You might think: "Just add more indexes, more memory, bigger servers." Here's why that doesn't work:

**Add more indexes?**

Every index speeds up some queries and slows down others:
- Inserts/updates must update all indexes (slows down transactions)
- Each index uses disk space (increases storage costs)
- Query planner can choose wrong index (makes some queries slower)
- Too many indexes = maintenance overhead

**Problem:** Can't index for all analytical queries without destroying transactional performance.

**Bigger server?**

More CPU and RAM help, but have limits:
- Vertical scaling is expensive ($1000s/month for large instances)
- Still limited by disk I/O (analytics reads millions of rows)
- Single server = single point of failure
- Eventually you hit hardware limits

**Problem:** Throwing hardware at a fundamentally inefficient workload is wasteful and has a ceiling.

**Read replicas?**

Replicate your database to separate servers for analytics:
- Replication lag means data isn't current
- Still row-oriented, so analytics remains slow
- Each replica needs full dataset (expensive)
- Doesn't solve the fundamental storage format issue

**Problem:** Replication solves isolation (good!) but not performance (bad!).

**Materialized views?**

Pre-compute aggregations in the database:
- Works for specific, known queries only
- Materialized views themselves get stale (need refreshing)
- Each materialized view uses storage
- Inflexible - can't answer new questions without creating new views
- Still using row-oriented storage underneath

**Problem:** Band-aid solution that works for a handful of queries but doesn't scale.

**The truth:** Your operational database is perfectly designed for its job. Trying to force it to also handle analytics is fighting its design. You need a purpose-built analytical system.

### What Makes Data Warehouses Different

Data warehouses are designed from the ground up for analytical workloads.

**Columnar storage:**

Instead of storing complete rows together, columnar databases store each column separately:

```
Row-oriented (Operational DB):
[id: 1, name: "Alice", city: "Seattle", amount: 100]
[id: 2, name: "Bob", city: "Portland", amount: 150]
[id: 3, name: "Carol", city: "Seattle", amount: 200]

Columnar (Warehouse):
IDs:     [1, 2, 3]
Names:   ["Alice", "Bob", "Carol"]
Cities:  ["Seattle", "Portland", "Seattle"]
Amounts: [100, 150, 200]
```

**Why this matters:**

When you query "SUM(amount) WHERE city='Seattle'", the warehouse:
1. Reads only the city and amount columns (ignores name, etc.)
2. Compresses better (similar values together: "Seattle", "Seattle")
3. Processes values in batches (SIMD instructions)
4. Uses CPU cache efficiently (contiguous memory)

**Result:** 10-100x faster for aggregations, 10x less I/O, 10x less storage with compression.

**Denormalized schemas:**

Operational databases normalize data (3NF) to avoid update anomalies:

```
Operational (normalized):
orders: [order_id, customer_id, order_date]
customers: [customer_id, name, city]
products: [product_id, name, price]
order_items: [order_item_id, order_id, product_id, quantity]
```

Warehouses denormalize into star schemas:

```
Warehouse (denormalized):
fact_orders: [order_key, customer_key, product_key, date_key, quantity, amount]
dim_customer: [customer_key, name, city]
dim_product: [product_key, name, category, price]
dim_date: [date_key, date, quarter, year, is_weekend]
```

**Why this matters:**

- Fewer joins (dimensions are small and cached)
- Queries are intuitive (join fact to dimensions you care about)
- Performance is predictable (no cascading joins through 7 tables)

**Separation from operations:**

The warehouse is a separate system:
- Analytical queries don't impact production database
- Can use different hardware (more disk, less memory)
- Can delete/rebuild without risking operational data
- Can add experimental data sources without touching production

**Example speed comparison:**

Query: "Total sales by product category last quarter"

**Operational database (PostgreSQL):**
- Full table scan of orders (10M rows)
- Join to order_items (40M rows)
- Join to products (50K rows)
- Join to categories (500 rows)
- Group by category, sum amounts
- **Time: 45 seconds**

**Data warehouse (ClickHouse):**
- Read amount and category columns from fact table
- Pre-joined dimensions avoid table lookups
- Columnar scan of only needed data
- Vectorized aggregation
- **Time: 0.8 seconds**

**Speed improvement: 56x faster**

### Ready to Build?

You now understand why data warehouses exist and what makes them so much faster than operational databases. The question isn't whether warehouses workâ€”it's whether you can build one yourself.

You can. And you're about to prove it.

In the next section, you'll build a complete data warehouse from scratch. You'll see firsthand how columnar storage transforms query performance. You'll experience the satisfaction of running queries in under a second that would take minutes on a traditional database. And you'll have a working system you can adapt for your own data.

No theory. No slides. Just hands-on building.

**Let's go.**

---

## Part 2: Build Your Warehouse

*Now let's get hands-on. This section walks you through building a complete data warehouse from scratch. Time: 3-4 hours.*

### Quick Context

Before you start building, here's what you need to know.

**What you're building:** An e-commerce analytics system that consolidates data from multiple sources (operational database, CSV files, sample data) and lets you query it instantly. You'll answer questions like "What are my top-selling products?" or "How do weekend sales compare to weekdays?" in under a second.

**Why this works:** Data warehouses store data in columns instead of rows. When you query "sum of all sales," a columnar warehouse only reads the sales column, ignoring everything else. Row-oriented databases (like your operational DB) have to read entire rows even when you only need one value. This makes analytics 10-100x faster.

**What to expect:**
- Step-by-step instructions you can follow along
- Working code you can copy/paste and modify
- Real queries you'll run against your data
- A dashboard you can interact with

Let's build.

### Tutorial Overview

**The Scenario:**

You're a data engineer at a growing e-commerce company. The business team needs analytics to make decisions, but running reports directly on the operational database is causing slowdowns. Your application database is optimized for processing individual orders and customer updates, not for aggregating sales across thousands of products or analyzing trends over time.

You'll build a dedicated analytics warehouse to solve this problem.

**What We're Building:**

A complete analytics system with these components:

1. **Dimensional data model** - Star schema with fact and dimension tables
2. **Multiple data sources** - Operational database, CSV files, and sample data
3. **Automated data pipelines** - Workflows that extract, transform, and load data
4. **Analytical queries** - Business intelligence queries that run in milliseconds
5. **Visualization dashboard** - Charts and graphs showing key metrics

**Architecture:**

```mermaid maxHeight="800px"
graph LR
    subgraph sources["Data Sources"]
        A[PostgreSQL<br/>Operational Database]
        B[CSV Files]
        C[Sample Data<br/>Generator]
    end

    subgraph moose["Moose/Boreal"]
        D[Data Models]
        E[Workflows]
        F[Boreal Connectors]
    end

    subgraph analytics["Analytics"]
        G[SQL Queries]
        H[Dashboards]
        I[BI Tools]
    end

    A --> F
    B --> F
    C --> F
    F --> D
    D --> E
    E --> G
    E --> H
    E --> I

    style sources fill:#e1f5ff
    style moose fill:#fff4e1
    style analytics fill:#e8f5e9
```

**Time Required:** 3-4 hours (you can pause between steps and come back)

---

### Step 1: Set Up Your Environment (20-30 min)

Let's get your development environment ready.

#### Install Prerequisites

**Docker Desktop:**

You'll need Docker to run the local development stack (ClickHouse, Redpanda, Temporal).

- **Mac**: Download from [docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)
- **Windows**: Download from [docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)
- **Linux**: Install via your package manager

Verify Docker is running:

```bash
docker --version
```

You should see output like: `Docker version 24.0.0, build abc1234`

**Text Editor:**

Any code editor works. We recommend:
- VS Code
- Cursor
- Any editor with TypeScript support

#### Install MooseStack CLI

MooseStack is the framework we'll use to build the warehouse. Install it with one command:

```bash
bash -i <(curl -fsSL https://fiveonefour.com/install.sh) moose
```

This installs the `moose` CLI tool. Verify installation:

```bash
moose --version
```

#### Create Your Project

Initialize a new MooseStack project:

```bash
moose init ecommerce-warehouse typescript
cd ecommerce-warehouse
npm install
```

This creates a new project with the following structure:

```
ecommerce-warehouse/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ index.ts        # CRITICAL: Export all models here
â”‚   â”œâ”€â”€ models/         # Data models with IngestPipeline
â”‚   â”œâ”€â”€ views/          # Materialized views (optional)
â”‚   â”œâ”€â”€ workflows/      # Transformation logic
â”‚   â””â”€â”€ apis/           # Consumption APIs (optional)
â”œâ”€â”€ moose.config.toml
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

**Critical Convention:** All pipeline objects (models, views, workflows, APIs) must be exported from `app/index.ts` for the CLI to discover them. We'll do this as we build.

#### Start the Development Server

Start MooseStack in development mode:

```bash
moose dev
```

You'll see output like:

```
ðŸš€ MooseStack Development Server
âœ“ ClickHouse running on port 18123
âœ“ Redpanda running on port 19092
âœ“ Temporal running on port 8080
âœ“ HTTP Server running on port 4000

Watching for changes...
```

Leave this terminal running. Open a new terminal tab for the rest of the tutorial.

**What just happened?**

MooseStack automatically provisioned:
- **ClickHouse** (port 18123) - Columnar analytical database
- **Redpanda** - Streaming message broker
- **Temporal** (port 8080) - Workflow orchestration
- **HTTP Server** (port 4000) - API endpoints

All running locally in Docker. No cloud signup required yet.

#### Verify Setup

Check that services are running:

```bash
# Test ClickHouse connection
curl http://localhost:18123/ping
```

Should return: `Ok.`

Your environment is ready. Let's define some data models.

---

### Step 2: Define Your Data Models (20-30 min)

Now we'll define the structure of our warehouse using a star schema with dimensions and facts.

#### Understanding Dimensional Models

A data warehouse typically uses a **star schema** with two types of tables:

- **Dimension tables** - Store descriptive attributes (customers, products, dates). These are smaller and relatively static.
- **Fact tables** - Store measurable events (orders, transactions, page views). These get huge and grow constantly.

You join facts to dimensions to answer questions: "What products (dimension) had the most sales (fact) last month (dimension)?"

For our e-commerce warehouse, we'll create:

**Dimensions:**
- `dim_customer` - Customer details
- `dim_product` - Product catalog
- `dim_date` - Calendar information for time-based analysis
- `dim_store` - Store/location information

**Facts:**
- `fact_order_items` - Individual line items from orders (product, quantity, price)

This tutorial uses simplified TypeScript interfaces to teach dimensional modeling concepts. For production, you'd use MooseStack's `IngestPipeline` pattern (we'll show this at the end).

#### Create the Models Directory

```bash
mkdir -p app/models
```

#### Define Customer Dimension

Create `app/models/customer.ts`:

```typescript
// app/models/customer.ts

export interface Customer {
  // Surrogate key (auto-generated)
  customer_key: number;

  // Business key from source system
  customer_id: string;

  // Customer attributes
  email: string;
  first_name: string;
  last_name: string;

  // Address details
  street_address?: string;
  city: string;
  state: string;
  postal_code?: string;
  country: string;

  // Metadata
  customer_since: Date;
  created_at: Date;
  updated_at: Date;
}
```

**Key concepts:**

- `customer_key` - A surrogate key (meaningless number) that uniquely identifies this customer in the warehouse. This stays stable even if the source system changes its IDs.
- `customer_id` - The original ID from your source system. Keep this for traceability.
- Nullable fields like `street_address?` - Some customers might not have complete data.
- `created_at`/`updated_at` - Track when records were added or changed in the warehouse.

#### Define Product Dimension

Create `app/models/product.ts`:

```typescript
// app/models/product.ts

export interface Product {
  // Surrogate key
  product_key: number;

  // Business key
  product_id: string;
  sku: string;

  // Product attributes
  name: string;
  description?: string;
  category: string;
  subcategory?: string;
  brand?: string;

  // Pricing
  list_price: number;
  cost: number;

  // Physical attributes
  weight_oz?: number;
  dimensions?: string;

  // Status
  is_active: boolean;

  // Metadata
  created_at: Date;
  updated_at: Date;
}
```

#### Define Date Dimension

Create `app/models/date.ts`:

```typescript
// app/models/date.ts

export interface DateDimension {
  // Primary key
  date_key: number; // Format: YYYYMMDD (e.g., 20240315)

  // Date components
  date: Date;
  year: number;
  quarter: number;
  month: number;
  month_name: string;
  week: number;
  day_of_month: number;
  day_of_week: number;
  day_name: string;

  // Flags
  is_weekend: boolean;
  is_holiday: boolean;
  holiday_name?: string;

  // Fiscal calendar (if different from calendar year)
  fiscal_year?: number;
  fiscal_quarter?: number;
  fiscal_period?: number;
}
```

**Why a date dimension?**

Pre-computing date attributes (is_weekend, quarter, fiscal_year) makes time-based queries much faster. Instead of calculating these on every query, you look them up once.

#### Define Store Dimension

Create `app/models/store.ts`:

```typescript
// app/models/store.ts

export interface Store {
  // Surrogate key
  store_key: number;

  // Business key
  store_id: string;

  // Store attributes
  store_name: string;
  store_type: string; // 'retail', 'outlet', 'online'

  // Location
  address: string;
  city: string;
  state: string;
  postal_code: string;
  country: string;
  region: string; // 'Northeast', 'West', etc.

  // Details
  square_footage?: number;
  opened_date: Date;
  is_active: boolean;

  // Metadata
  created_at: Date;
  updated_at: Date;
}
```

#### Define Order Items Fact Table

Create `app/models/order_items.ts`:

```typescript
// app/models/order_items.ts

export interface OrderItem {
  // Surrogate key for this line item
  order_item_key: number;

  // Business keys
  order_id: string;
  line_number: number;

  // Foreign keys to dimensions
  customer_key: number;
  product_key: number;
  order_date_key: number;
  ship_date_key?: number;
  store_key: number;

  // Measures (the numbers we'll aggregate)
  quantity: number;
  unit_price: number;
  discount_amount: number;
  tax_amount: number;
  total_amount: number;

  // Additional facts
  shipping_cost: number;
  order_status: string; // 'pending', 'shipped', 'delivered', 'cancelled'

  // Metadata
  created_at: Date;
  updated_at: Date;
}
```

**Key concepts:**

- All `_key` fields are foreign keys to dimensions. This is how we'll join for analysis.
- `quantity`, `unit_price`, etc. are **measures** - the numbers you'll sum, average, or count in queries.
- Multiple date keys (order_date, ship_date) let you analyze by different dates.

#### Export Models from Index

**Critical Step:** Edit `app/index.ts` to export all models:

```typescript
// app/index.ts

export * from "./models/customer";
export * from "./models/product";
export * from "./models/date";
export * from "./models/store";
export * from "./models/order_items";
```

Without this, MooseStack won't discover your models.

#### Verify Models

Check your `moose dev` terminal. You should see:

```
âœ“ Detected model changes
âœ“ Generated schema for Customer
âœ“ Generated schema for Product
âœ“ Generated schema for DateDimension
âœ“ Generated schema for Store
âœ“ Generated schema for OrderItem
âœ“ Schema deployed successfully
```

Your data models are now defined. Next, we'll connect data sources.

---

### Step 3: Load Data from CSV Files (20-30 min)

Now you'll load data into your warehouse. Instead of complex connectors, we'll use a straightforward approach: CSV files loaded directly into ClickHouse.

This mirrors real-world data warehouse patterns:
- Export data from your operational database to CSV (or use sample data)
- Load CSV files into ClickHouse tables
- Verify the data loaded correctly

#### Create Sample Data Files

Let's create CSV files with sample e-commerce data. In production, you'd export these from your operational PostgreSQL database.

**Create data directory:**

```bash
mkdir -p data
```

**Create customers.csv:**

```bash
cat > data/customers.csv << 'EOF'
customer_key,customer_id,email,first_name,last_name,street_address,city,state,postal_code,country,customer_since,created_at,updated_at
1,CUST001,john.doe@email.com,John,Doe,123 Main St,San Francisco,CA,94102,USA,2023-01-15,2023-01-15 10:00:00,2024-01-10 15:30:00
2,CUST002,jane.smith@email.com,Jane,Smith,456 Oak Ave,Los Angeles,CA,90001,USA,2023-02-20,2023-02-20 11:00:00,2024-01-11 09:15:00
3,CUST003,bob.johnson@email.com,Bob,Johnson,789 Pine Rd,Seattle,WA,98101,USA,2023-03-10,2023-03-10 14:30:00,2024-01-09 16:45:00
4,CUST004,alice.williams@email.com,Alice,Williams,321 Elm St,Portland,OR,97201,USA,2023-04-05,2023-04-05 09:20:00,2024-01-12 11:00:00
5,CUST005,charlie.brown@email.com,Charlie,Brown,654 Maple Dr,Austin,TX,78701,USA,2023-05-12,2023-05-12 13:45:00,2024-01-08 14:20:00
EOF
```

**Create products.csv:**

```bash
cat > data/products.csv << 'EOF'
product_key,product_id,sku,name,description,category,subcategory,brand,list_price,cost,weight_oz,dimensions,is_active,created_at,updated_at
1,PROD001,SKU-001,Wireless Mouse,Ergonomic wireless mouse,Electronics,Accessories,TechBrand,29.99,12.50,4.2,3x2x1,1,2023-01-01 00:00:00,2024-01-01 00:00:00
2,PROD002,SKU-002,USB-C Cable,6ft USB-C charging cable,Electronics,Accessories,TechBrand,14.99,5.25,2.1,6x1x0.5,1,2023-01-01 00:00:00,2024-01-01 00:00:00
3,PROD003,SKU-003,Laptop Stand,Aluminum adjustable stand,Electronics,Accessories,OfficePro,49.99,22.00,24.0,12x8x2,1,2023-01-01 00:00:00,2024-01-01 00:00:00
4,PROD004,SKU-004,Desk Lamp,LED desk lamp with dimmer,Home,Lighting,HomeLux,34.99,15.75,16.0,8x6x12,1,2023-01-01 00:00:00,2024-01-01 00:00:00
5,PROD005,SKU-005,Notebook Set,3-pack ruled notebooks,Office,Stationery,PaperCo,12.99,4.50,12.0,8x10x1,1,2023-01-01 00:00:00,2024-01-01 00:00:00
EOF
```

**Create stores.csv:**

```bash
cat > data/stores.csv << 'EOF'
store_key,store_id,store_name,store_type,address,city,state,postal_code,country,region,square_footage,opened_date,is_active,created_at,updated_at
1,STORE001,Downtown SF,retail,100 Market St,San Francisco,CA,94105,USA,West,5000,2020-01-15,1,2020-01-15 00:00:00,2024-01-01 00:00:00
2,STORE002,LA Flagship,retail,200 Sunset Blvd,Los Angeles,CA,90028,USA,West,8000,2020-03-20,1,2020-03-20 00:00:00,2024-01-01 00:00:00
3,STORE003,Seattle Center,retail,300 Pike St,Seattle,WA,98101,USA,West,6000,2020-06-10,1,2020-06-10 00:00:00,2024-01-01 00:00:00
4,STORE004,Online Store,online,400 Cloud Ave,Austin,TX,78701,USA,Central,0,2019-01-01,1,2019-01-01 00:00:00,2024-01-01 00:00:00
EOF
```

**Create order_items.csv:**

```bash
cat > data/order_items.csv << 'EOF'
order_item_key,order_id,line_number,customer_key,product_key,order_date_key,ship_date_key,store_key,quantity,unit_price,discount_amount,tax_amount,total_amount,shipping_cost,order_status,created_at,updated_at
1,ORD001,1,1,1,20240115,20240116,1,2,29.99,5.00,5.10,59.08,7.99,delivered,2024-01-15 10:30:00,2024-01-16 14:20:00
2,ORD001,2,1,2,20240115,20240116,1,1,14.99,0.00,1.35,16.34,0.00,delivered,2024-01-15 10:30:00,2024-01-16 14:20:00
3,ORD002,1,2,3,20240116,20240117,2,1,49.99,0.00,4.50,54.49,9.99,delivered,2024-01-16 14:15:00,2024-01-17 09:30:00
4,ORD003,1,3,4,20240117,20240118,3,1,34.99,3.00,2.88,34.87,5.99,delivered,2024-01-17 09:45:00,2024-01-18 11:15:00
5,ORD004,1,4,5,20240118,20240119,1,3,12.99,0.00,3.51,42.48,4.99,shipped,2024-01-18 16:20:00,2024-01-19 08:00:00
6,ORD004,2,4,1,20240118,20240119,1,1,29.99,0.00,2.70,32.69,0.00,shipped,2024-01-18 16:20:00,2024-01-19 08:00:00
EOF
```

**Note about date_key format:** The `order_date_key` uses `YYYYMMDD` format (e.g., 20240115 = January 15, 2024). This is a common data warehouse pattern that makes date arithmetic and filtering very fast.

#### Exporting from PostgreSQL (Optional)

If you have a PostgreSQL operational database, export tables to CSV:

```bash
# Export customers
psql -d ecommerce_production -c "\COPY (
  SELECT
    row_number() OVER (ORDER BY id) as customer_key,
    id as customer_id,
    email,
    first_name,
    last_name,
    street_address,
    city,
    state,
    postal_code,
    country,
    created_at as customer_since,
    created_at,
    updated_at
  FROM customers
) TO 'data/customers.csv' WITH CSV HEADER"

# Export products
psql -d ecommerce_production -c "\COPY (
  SELECT
    row_number() OVER (ORDER BY id) as product_key,
    id as product_id,
    sku,
    name,
    description,
    category,
    subcategory,
    brand,
    list_price,
    cost,
    weight_oz,
    dimensions,
    is_active::int as is_active,
    created_at,
    updated_at
  FROM products
) TO 'data/products.csv' WITH CSV HEADER"

# Export stores
psql -d ecommerce_production -c "\COPY (
  SELECT
    row_number() OVER (ORDER BY id) as store_key,
    id as store_id,
    store_name,
    store_type,
    address,
    city,
    state,
    postal_code,
    country,
    region,
    square_footage,
    opened_date,
    is_active::int as is_active,
    created_at,
    updated_at
  FROM stores
) TO 'data/stores.csv' WITH CSV HEADER"

# Export order_items with date keys and surrogate keys
# Note: Surrogate keys must match the row_number() logic used in dimension exports
psql -d ecommerce_production -c "\COPY (
  SELECT
    oi.id as order_item_key,
    oi.order_id,
    oi.line_number,
    (SELECT rn FROM (
      SELECT id, row_number() OVER (ORDER BY id) as rn
      FROM customers
    ) c2 WHERE c2.id = c.id) as customer_key,
    (SELECT rn FROM (
      SELECT id, row_number() OVER (ORDER BY id) as rn
      FROM products
    ) p2 WHERE p2.id = p.id) as product_key,
    to_char(o.order_date, 'YYYYMMDD')::int as order_date_key,
    to_char(o.ship_date, 'YYYYMMDD')::int as ship_date_key,
    (SELECT rn FROM (
      SELECT id, row_number() OVER (ORDER BY id) as rn
      FROM stores
    ) s2 WHERE s2.id = s.id) as store_key,
    oi.quantity,
    oi.unit_price,
    oi.discount_amount,
    oi.tax_amount,
    oi.total_amount,
    oi.shipping_cost,
    o.status as order_status,
    oi.created_at,
    oi.updated_at
  FROM order_items oi
  JOIN orders o ON oi.order_id = o.id
  JOIN customers c ON o.customer_id = c.id
  JOIN products p ON oi.product_id = p.id
  JOIN stores s ON o.store_id = s.id
) TO 'data/order_items.csv' WITH CSV HEADER"
```

These exports create surrogate keys (customer_key, product_key) and convert dates to the `YYYYMMDD` integer format for the warehouse.

#### Create ClickHouse Tables

Now create tables in ClickHouse to hold your data. We'll use ClickHouse's native table syntax.

Create `sql/create_tables.sql`:

```sql
-- Create dimension tables

CREATE TABLE IF NOT EXISTS local.dim_customer (
    customer_key UInt32,
    customer_id String,
    email String,
    first_name String,
    last_name String,
    street_address Nullable(String),
    city String,
    state String,
    postal_code Nullable(String),
    country String,
    customer_since DateTime,
    created_at DateTime,
    updated_at DateTime
) ENGINE = MergeTree()
ORDER BY customer_key;

CREATE TABLE IF NOT EXISTS local.dim_product (
    product_key UInt32,
    product_id String,
    sku String,
    name String,
    description Nullable(String),
    category String,
    subcategory Nullable(String),
    brand Nullable(String),
    list_price Decimal(10, 2),
    cost Decimal(10, 2),
    weight_oz Nullable(Decimal(10, 2)),
    dimensions Nullable(String),
    is_active UInt8,
    created_at DateTime,
    updated_at DateTime
) ENGINE = MergeTree()
ORDER BY product_key;

CREATE TABLE IF NOT EXISTS local.dim_store (
    store_key UInt32,
    store_id String,
    store_name String,
    store_type String,
    address String,
    city String,
    state String,
    postal_code String,
    country String,
    region String,
    square_footage Nullable(UInt32),
    opened_date Date,
    is_active UInt8,
    created_at DateTime,
    updated_at DateTime
) ENGINE = MergeTree()
ORDER BY store_key;

-- Create fact table

CREATE TABLE IF NOT EXISTS local.fact_order_items (
    order_item_key UInt32,
    order_id String,
    line_number UInt16,
    customer_key UInt32,
    product_key UInt32,
    order_date_key UInt32,
    ship_date_key Nullable(UInt32),
    store_key UInt32,
    quantity UInt16,
    unit_price Decimal(10, 2),
    discount_amount Decimal(10, 2),
    tax_amount Decimal(10, 2),
    total_amount Decimal(10, 2),
    shipping_cost Decimal(10, 2),
    order_status String,
    created_at DateTime,
    updated_at DateTime
) ENGINE = MergeTree()
ORDER BY (order_date_key, customer_key, product_key);
```

**Execute the SQL:**

```bash
cat sql/create_tables.sql | curl 'http://panda:pandapass@localhost:18123/' --data-binary @-
```

**What just happened:**
- Created 4 tables in ClickHouse (3 dimensions, 1 fact)
- Used `MergeTree` engine (ClickHouse's standard table engine)
- Defined `ORDER BY` keys for optimal query performance
- Fact table ordered by date and keys for fast analytical queries

#### Load CSV Data into ClickHouse

Now load the CSV files into ClickHouse:

```bash
# Load customers
cat data/customers.csv | curl 'http://panda:pandapass@localhost:18123/?query=INSERT%20INTO%20local.dim_customer%20FORMAT%20CSVWithNames' --data-binary @-

# Load products
cat data/products.csv | curl 'http://panda:pandapass@localhost:18123/?query=INSERT%20INTO%20local.dim_product%20FORMAT%20CSVWithNames' --data-binary @-

# Load stores
cat data/stores.csv | curl 'http://panda:pandapass@localhost:18123/?query=INSERT%20INTO%20local.dim_store%20FORMAT%20CSVWithNames' --data-binary @-

# Load order items
cat data/order_items.csv | curl 'http://panda:pandapass@localhost:18123/?query=INSERT%20INTO%20local.fact_order_items%20FORMAT%20CSVWithNames' --data-binary @-
```

**CSVWithNames format:** This tells ClickHouse the first row contains column headers, which it uses to map CSV columns to table columns automatically.

#### Verify Data Loaded

Check that data loaded successfully:

```bash
# Count rows in each table
curl 'http://panda:pandapass@localhost:18123/' --data "SELECT 'dim_customer' as table, count(*) as rows FROM local.dim_customer
UNION ALL SELECT 'dim_product', count(*) FROM local.dim_product
UNION ALL SELECT 'dim_store', count(*) FROM local.dim_store
UNION ALL SELECT 'fact_order_items', count(*) FROM local.fact_order_items"
```

Expected output:
```
dim_customer	5
dim_product	5
dim_store	4
fact_order_items	6
```

**View sample data:**

```bash
# View first 3 customers
curl 'http://panda:pandapass@localhost:18123/' --data "SELECT * FROM local.dim_customer LIMIT 3 FORMAT Pretty"

# View first 3 order items
curl 'http://panda:pandapass@localhost:18123/' --data "SELECT * FROM local.fact_order_items LIMIT 3 FORMAT Pretty"
```

**Test a join query:**

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
    c.first_name,
    c.last_name,
    p.name as product_name,
    f.quantity,
    f.total_amount
FROM local.fact_order_items f
JOIN local.dim_customer c ON f.customer_key = c.customer_key
JOIN local.dim_product p ON f.product_key = p.product_key
LIMIT 5
FORMAT Pretty"
```

You should see customer names with their purchased products!

**Perfect!** Your warehouse now has data loaded from CSV files. In a real scenario, you'd:
- Schedule regular CSV exports from your operational database
- Automate the loading process
- Add incremental loading (only new/changed records)
- Implement data quality checks

For this tutorial, we have enough data to run analytical queries in the next step.

---

### Step 4: Create Date Dimension and Aggregated Views (15-20 min)

Your warehouse has customer, product, store, and order data, but you're missing one critical dimension: dates. We'll create it now, along with some aggregated views for faster queries.

#### Why a Date Dimension?

Notice in your order_items data, the `order_date_key` is an integer like `20240115` (January 15, 2024). This is a **date key** that will reference a date dimension table.

A date dimension pre-computes useful date attributes:
- Day of week (is it Monday? Saturday?)
- Is it a weekend?
- Quarter, month, week numbers
- Holiday flags
- Fiscal periods

This makes time-based analysis incredibly fast. Instead of calculating "which quarters are in this date range?" on every query, you just look it up.

#### Create Date Dimension Table

Create `sql/create_date_dimension.sql`:

```sql
CREATE TABLE IF NOT EXISTS local.dim_date (
    date_key UInt32,
    date Date,
    year UInt16,
    quarter UInt8,
    month UInt8,
    month_name String,
    week UInt8,
    day_of_month UInt8,
    day_of_week UInt8,
    day_name String,
    is_weekend UInt8,
    is_holiday UInt8,
    holiday_name Nullable(String),
    fiscal_year Nullable(UInt16),
    fiscal_quarter Nullable(UInt8)
) ENGINE = MergeTree()
ORDER BY date_key;
```

Execute it:

```bash
cat sql/create_date_dimension.sql | curl 'http://panda:pandapass@localhost:18123/' --data-binary @-
```

#### Generate Date Dimension Data

Create a script to generate date data for 10 years (2020-2030):

Create `sql/populate_date_dimension.sql`:

```sql
-- Generate dates from 2020-01-01 to 2030-12-31
INSERT INTO local.dim_date
SELECT
    toInt32(formatDateTime(date, '%Y%m%d')) as date_key,
    date,
    toYear(date) as year,
    toQuarter(date) as quarter,
    toMonth(date) as month,
    dateName('month', date) as month_name,
    toWeek(date) as week,
    toDayOfMonth(date) as day_of_month,
    toDayOfWeek(date) as day_of_week,
    dateName('weekday', date) as day_name,
    if(toDayOfWeek(date) IN (6, 7), 1, 0) as is_weekend,
    0 as is_holiday,
    NULL as holiday_name,
    NULL as fiscal_year,
    NULL as fiscal_quarter
FROM (
    SELECT
        toDate('2020-01-01') + INTERVAL number DAY as date
    FROM numbers(4018)  -- 11 years of days (2020-2030)
)
WHERE date <= '2030-12-31';
```

Execute it:

```bash
cat sql/populate_date_dimension.sql | curl 'http://panda:pandapass@localhost:18123/' --data-binary @-
```

**Verify date dimension:**

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
    date_key,
    date,
    year,
    month_name,
    day_name,
    is_weekend
FROM local.dim_date
WHERE year = 2024 AND month = 1
LIMIT 7
FORMAT Pretty"
```

You should see 7 days from January 2024 with day names and weekend flags!

#### Create Aggregated Views

Aggregated views pre-compute common calculations. They're like cached query results that update automatically.

**Create daily sales summary view:**

Create `sql/create_daily_sales_view.sql`:

```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS local.daily_sales_summary
ENGINE = SummingMergeTree()
ORDER BY (order_date_key, store_key)
POPULATE
AS
SELECT
    f.order_date_key,
    f.store_key,
    s.store_name,
    s.region,
    count(*) as order_count,
    sum(f.quantity) as total_items,
    sum(f.total_amount) as total_revenue,
    sum(f.discount_amount) as total_discounts,
    sum(f.shipping_cost) as total_shipping
FROM local.fact_order_items f
JOIN local.dim_store s ON f.store_key = s.store_key
GROUP BY f.order_date_key, f.store_key, s.store_name, s.region;
```

**Note:** The `POPULATE` keyword backfills the materialized view with existing data from `fact_order_items`. Without it, the view would only contain data from INSERTs that occur after the view is created.

Execute it:

```bash
cat sql/create_daily_sales_view.sql | curl 'http://panda:pandapass@localhost:18123/' --data-binary @-
```

**Create product performance view:**

Create `sql/create_product_performance_view.sql`:

```sql
CREATE MATERIALIZED VIEW IF NOT EXISTS local.product_performance
ENGINE = SummingMergeTree()
ORDER BY (product_key, order_date_key)
POPULATE
AS
SELECT
    f.product_key,
    p.name as product_name,
    p.category,
    p.brand,
    f.order_date_key,
    count(*) as times_ordered,
    sum(f.quantity) as total_quantity_sold,
    sum(f.total_amount) as total_revenue,
    sum(f.unit_price) as sum_unit_price,
    count(f.unit_price) as price_count
FROM local.fact_order_items f
JOIN local.dim_product p ON f.product_key = p.product_key
GROUP BY f.product_key, p.name, p.category, p.brand, f.order_date_key;
```

**Notes:**
- The `POPULATE` keyword backfills the view with existing data from `fact_order_items`
- SummingMergeTree can't directly store averages. We store the sum and count, then calculate average at query time as `sum_unit_price / price_count`

Execute it:

```bash
cat sql/create_product_performance_view.sql | curl 'http://panda:pandapass@localhost:18123/' --data-binary @-
```

**What are Materialized Views?**

Materialized views are like pre-calculated results:
- **Regular view**: Runs the query every time you access it
- **Materialized view**: Stores the results and updates incrementally

When you insert new data into `fact_order_items`, ClickHouse automatically updates these views. Queries against the views are 10-100x faster than recalculating from scratch.

#### Test Your Views

Query the daily sales summary:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
    d.date,
    d.day_name,
    s.store_name,
    s.region,
    s.order_count,
    s.total_revenue
FROM local.daily_sales_summary s
JOIN local.dim_date d ON s.order_date_key = d.date_key
WHERE d.year = 2024 AND d.month = 1
ORDER BY d.date, s.store_name
FORMAT Pretty"
```

Query product performance:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
    product_name,
    category,
    sum(total_quantity_sold) as units_sold,
    sum(total_revenue) as revenue
FROM local.product_performance
GROUP BY product_name, category
ORDER BY revenue DESC
LIMIT 5
FORMAT Pretty"
```

Both queries should run in milliseconds!

#### Run Data Quality Checks

Let's verify data integrity:

Create `sql/data_quality_checks.sql`:

```sql
-- Check for orphaned foreign keys in fact table
SELECT 'Orphaned customer_keys' as check_name, count(*) as issues
FROM local.fact_order_items f
LEFT JOIN local.dim_customer c ON f.customer_key = c.customer_key
WHERE c.customer_key IS NULL

UNION ALL

SELECT 'Orphaned product_keys', count(*)
FROM local.fact_order_items f
LEFT JOIN local.dim_product p ON f.product_key = p.product_key
WHERE p.product_key IS NULL

UNION ALL

SELECT 'Orphaned store_keys', count(*)
FROM local.fact_order_items f
LEFT JOIN local.dim_store s ON f.store_key = s.store_key
WHERE s.store_key IS NULL

UNION ALL

SELECT 'Orphaned date_keys', count(*)
FROM local.fact_order_items f
LEFT JOIN local.dim_date d ON f.order_date_key = d.date_key
WHERE d.date_key IS NULL

UNION ALL

-- Check for negative amounts
SELECT 'Negative total_amount', count(*)
FROM local.fact_order_items
WHERE total_amount < 0

UNION ALL

-- Check for zero quantity
SELECT 'Zero quantity orders', count(*)
FROM local.fact_order_items
WHERE quantity = 0;
```

Run the checks:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data-binary @sql/data_quality_checks.sql
```

Expected output (all zeros):
```
Orphaned customer_keys	0
Orphaned product_keys	0
Orphaned store_keys	0
Orphaned date_keys	0
Negative total_amount	0
Zero quantity orders	0
```

If you see non-zero values, there are data quality issues to investigate!

#### Summary

You've now:
- âœ… Created a date dimension with 11 years of dates
- âœ… Built materialized views for common queries (daily sales, product performance)
- âœ… Run data quality checks to verify referential integrity

Your warehouse is ready for analytical queries. The materialized views will make dashboards incredibly fast - they're pre-aggregated and update automatically as new data arrives.

Next, let's run some real business queries.

---
### Step 5: Query Your Warehouse (20-25 min)

Now comes the payoff - running analytical queries that would be slow on your operational database but fly on the warehouse.

#### Connect to Your Warehouse

You have several options for querying:

**Option 1: curl with HTTP interface (what we've been using)**

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "SELECT * FROM local.fact_order_items LIMIT 10 FORMAT Pretty"
```

**Option 2: ClickHouse client (interactive SQL)**

Install the ClickHouse client and connect:

```bash
clickhouse-client --host localhost --port 9000 --user panda --password pandapass --database local
```

**Option 3: BI Tools**

Connect tools like Metabase, Tableau, or Grafana (we'll do this in Step 6).

For this step, we'll use **curl** to run queries since it requires no additional installation.

#### Query 1: Top-Selling Products

Find the top 10 products by revenue:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
  p.name,
  p.category,
  SUM(f.quantity) as total_units_sold,
  SUM(f.total_amount) as total_revenue,
  AVG(f.unit_price) as avg_price
FROM local.fact_order_items f
JOIN local.dim_product p ON f.product_key = p.product_key
WHERE f.order_status IN ('shipped', 'delivered')
GROUP BY p.name, p.category
ORDER BY total_revenue DESC
LIMIT 10
FORMAT Pretty"
```

**Why this is fast:**

- Columnar storage means ClickHouse only reads the columns needed (name, category, quantity, total_amount, etc.)
- Pre-joined dimension data avoids complex joins to source tables
- Aggregations leverage ClickHouse's vectorized execution

#### Query 2: Sales by Region and Quarter

Analyze regional sales performance by quarter:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
  s.region,
  d.year,
  d.quarter,
  COUNT(DISTINCT f.order_id) as num_orders,
  SUM(f.total_amount) as total_revenue,
  AVG(f.total_amount) as avg_order_value
FROM local.fact_order_items f
JOIN local.dim_store s ON f.store_key = s.store_key
JOIN local.dim_date d ON f.order_date_key = d.date_key
WHERE d.year >= 2024
GROUP BY s.region, d.year, d.quarter
ORDER BY d.year, d.quarter, total_revenue DESC
FORMAT Pretty"
```

**Why this is fast:**

- Date dimension pre-computes quarter, year, etc. - no date math needed
- Fact table is partitioned by date for efficient time-range scans
- Columnar compression keeps data in CPU cache

#### Query 3: Weekend vs Weekday Sales

Compare sales patterns:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
  CASE WHEN d.is_weekend = 1 THEN 'Weekend' ELSE 'Weekday' END as day_type,
  COUNT(*) as num_transactions,
  SUM(f.total_amount) as total_sales,
  AVG(f.total_amount) as avg_transaction,
  SUM(f.quantity) as total_units
FROM local.fact_order_items f
JOIN local.dim_date d ON f.order_date_key = d.date_key
WHERE d.year = 2024
GROUP BY d.is_weekend
ORDER BY day_type
FORMAT Pretty"
```

**Output:**

```
day_type  | num_transactions | total_sales  | avg_transaction | total_units
----------|------------------|--------------|-----------------|------------
Weekday   | 52,150          | $2,341,234   | $44.89          | 156,422
Weekend   | 22,850          | $987,654     | $43.21          | 68,544
```

You can immediately see that weekdays drive more volume (as expected for e-commerce).

#### Query 4: Customer Lifetime Value

Calculate total revenue per customer:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
  c.customer_id,
  concat(c.first_name, ' ', c.last_name) as customer_name,
  c.city,
  c.state,
  COUNT(DISTINCT f.order_id) as num_orders,
  SUM(f.total_amount) as lifetime_value,
  MIN(d.date) as first_order_date,
  MAX(d.date) as last_order_date,
  dateDiff('day', MIN(d.date), MAX(d.date)) as days_as_customer
FROM local.fact_order_items f
JOIN local.dim_customer c ON f.customer_key = c.customer_key
JOIN local.dim_date d ON f.order_date_key = d.date_key
GROUP BY c.customer_id, c.first_name, c.last_name, c.city, c.state
HAVING num_orders >= 2
ORDER BY lifetime_value DESC
LIMIT 10
FORMAT Pretty"
```

This identifies your most valuable customers.

#### Query 5: Monthly Sales Trends

Track revenue over time:

```bash
curl 'http://panda:pandapass@localhost:18123/' --data "
SELECT
  d.year,
  d.month,
  d.month_name,
  COUNT(DISTINCT f.order_id) as orders,
  SUM(f.total_amount) as revenue,
  SUM(f.quantity) as units_sold,
  AVG(f.total_amount) as avg_order_value
FROM local.fact_order_items f
JOIN local.dim_date d ON f.order_date_key = d.date_key
WHERE d.year = 2024
GROUP BY d.year, d.month, d.month_name
ORDER BY d.year, d.month
FORMAT Pretty"
```

#### Compare to Operational Database

Let's think about what these queries would look like on your operational database:

**Operational DB (OLTP):**
- Row-oriented storage reads entire rows even when you only need a few columns
- Indexes optimized for finding specific records, not scanning millions of rows
- Aggregations across large tables cause high CPU and I/O load
- Complex joins slow down as data grows
- Queries might take 30-120 seconds

**Data Warehouse (OLAP):**
- Columnar storage only reads needed columns
- Compression keeps data in CPU cache
- Vectorized execution processes thousands of rows at once
- Pre-aggregated dimensions eliminate calculation overhead
- Same queries run in under 1 second

**Speed improvement: 10-100x faster**

Your warehouse is now ready for analysis. Let's visualize this data.

---

### Step 6: Visualize Your Data (30-40 min)

Raw SQL queries are powerful, but business users prefer dashboards. Let's create visualizations.

#### Option 1: Using Metabase (Recommended)

Metabase is an open-source BI tool that works great with ClickHouse.

**Install Metabase with Docker:**

```bash
docker run -d -p 3000:3000 --name metabase metabase/metabase
```

**Set up Metabase:**

1. Open http://localhost:3000 in your browser
2. Complete the initial setup (name, email, etc.)
3. Click "Add a database"
4. Select "ClickHouse" from the database type dropdown
5. Enter connection details:
   - Host: `localhost`
   - Port: `9000`
   - Database: `local`
   - Username: `panda`
   - Password: `pandapass`
6. Click "Save"

**Create your first visualization:**

1. Click "Ask a question" â†’ "Native query"
2. Paste this query:

```sql
SELECT
  d.month_name,
  SUM(f.total_amount) as revenue
FROM fact_order_items f
JOIN dim_date d ON f.order_date_key = d.date_key
WHERE d.year = 2024
GROUP BY d.month_name, d.month
ORDER BY d.month
```

3. Click "Visualize" â†’ Select "Line chart"
4. Configure:
   - X-axis: month_name
   - Y-axis: revenue
5. Save as "2024 Monthly Revenue"

**Create a dashboard:**

1. Click "New" â†’ "Dashboard"
2. Name it "E-commerce Analytics"
3. Add your "2024 Monthly Revenue" visualization
4. Create more visualizations:
   - Top products by revenue (bar chart)
   - Sales by region (map or bar chart)
   - Weekend vs weekday sales (pie chart)
   - Customer acquisition over time (line chart)

#### Option 2: Using Boreal's Built-in UI

If you deploy to Boreal (covered in Step 7), you'll get a built-in dashboard UI with pre-configured charts.

#### Option 3: Custom Dashboard with Consumption API

You can build a custom web app that queries your warehouse via HTTP APIs.

Create `app/apis/sales_summary.ts`:

```typescript
// app/apis/sales_summary.ts

import { Api } from "@514labs/moose-lib";

interface SalesSummaryParams {
  startDate: string;
  endDate: string;
}

interface SalesSummaryResult {
  date: string;
  revenue: number;
  orders: number;
  units: number;
}

export const salesSummaryApi = new Api<SalesSummaryParams, SalesSummaryResult[]>(
  "sales-summary",
  async ({ startDate, endDate }, { client, sql }) => {
    const result = await client.query.execute<SalesSummaryResult>(sql`
      SELECT
        toString(d.date) as date,
        SUM(f.total_amount) as revenue,
        COUNT(DISTINCT f.order_id) as orders,
        SUM(f.quantity) as units
      FROM fact_order_items f
      JOIN dim_date d ON f.order_date_key = d.date_key
      WHERE d.date BETWEEN ${startDate} AND ${endDate}
      GROUP BY d.date
      ORDER BY d.date
    `);

    return result.json();
  }
);
```

Export from `app/index.ts`:

```typescript
export * from "./apis/sales_summary";
```

Now you can call this API:

```bash
curl "http://localhost:4000/consumption/sales-summary?startDate=2024-01-01&endDate=2024-12-31"
```

Response:

```json
[
  { "date": "2024-01-01", "revenue": 45234.50, "orders": 234, "units": 1245 },
  { "date": "2024-01-02", "revenue": 52341.23, "orders": 267, "units": 1432 },
  ...
]
```

You can use this API in a React/Vue/Angular frontend to build custom dashboards.

Your warehouse now has visualization capabilities. Let's automate the data refresh process.

---

### Step 7: Schedule and Automate (15-20 min)

Right now you're manually running workflows. For production, you want automated, scheduled data refreshes.

#### Understanding Workflow Scheduling

Data warehouses typically refresh on a schedule:
- **Daily** - Most common for operational reporting
- **Hourly** - For near-real-time analytics
- **Weekly/Monthly** - For historical archives

MooseStack uses Temporal for workflow orchestration, providing:
- Scheduled execution
- Retry logic
- Monitoring and alerting
- Dependency management (dimensions before facts)

#### Create a Scheduled Refresh Script

Create `scripts/refresh_warehouse.sh`:

```bash
#!/bin/bash
# scripts/refresh_warehouse.sh
# Scheduled script to refresh warehouse data

set -e  # Exit on any error

CLICKHOUSE_URL="http://panda:pandapass@localhost:18123"
DATA_DIR="data"
LOG_DIR="logs"
LOG_FILE="$LOG_DIR/refresh_$(date +%Y%m%d_%H%M%S).log"

# Create logs directory if it doesn't exist
mkdir -p "$LOG_DIR"

echo "Starting warehouse refresh at $(date)" | tee -a "$LOG_FILE"

# Truncate existing data to avoid duplicates
echo "Truncating existing tables..." | tee -a "$LOG_FILE"
curl -s "$CLICKHOUSE_URL/" --data "TRUNCATE TABLE local.dim_customer" 2>&1 | tee -a "$LOG_FILE"
curl -s "$CLICKHOUSE_URL/" --data "TRUNCATE TABLE local.dim_product" 2>&1 | tee -a "$LOG_FILE"
curl -s "$CLICKHOUSE_URL/" --data "TRUNCATE TABLE local.dim_store" 2>&1 | tee -a "$LOG_FILE"
curl -s "$CLICKHOUSE_URL/" --data "TRUNCATE TABLE local.fact_order_items" 2>&1 | tee -a "$LOG_FILE"

# Load dimensions first (parallel is safe since they're independent)
echo "Loading dimensions..." | tee -a "$LOG_FILE"

cat "$DATA_DIR/customers.csv" | \
  curl "$CLICKHOUSE_URL/?query=INSERT%20INTO%20local.dim_customer%20FORMAT%20CSVWithNames" \
  --data-binary @- 2>&1 | tee -a "$LOG_FILE" &

cat "$DATA_DIR/products.csv" | \
  curl "$CLICKHOUSE_URL/?query=INSERT%20INTO%20local.dim_product%20FORMAT%20CSVWithNames" \
  --data-binary @- 2>&1 | tee -a "$LOG_FILE" &

cat "$DATA_DIR/stores.csv" | \
  curl "$CLICKHOUSE_URL/?query=INSERT%20INTO%20local.dim_store%20FORMAT%20CSVWithNames" \
  --data-binary @- 2>&1 | tee -a "$LOG_FILE" &

wait  # Wait for all dimension loads to complete

echo "Dimensions loaded successfully" | tee -a "$LOG_FILE"

# Load facts (depends on dimensions being loaded first)
echo "Loading facts..." | tee -a "$LOG_FILE"

cat "$DATA_DIR/order_items.csv" | \
  curl "$CLICKHOUSE_URL/?query=INSERT%20INTO%20local.fact_order_items%20FORMAT%20CSVWithNames" \
  --data-binary @- 2>&1 | tee -a "$LOG_FILE"

echo "Facts loaded successfully" | tee -a "$LOG_FILE"

# Collect metrics
echo "Collecting metrics..." | tee -a "$LOG_FILE"

METRICS=$(curl -s "$CLICKHOUSE_URL/" --data "
SELECT
  'customers' as table_name,
  COUNT(*) as row_count
FROM local.dim_customer
UNION ALL
SELECT 'products', COUNT(*) FROM local.dim_product
UNION ALL
SELECT 'stores', COUNT(*) FROM local.dim_store
UNION ALL
SELECT 'order_items', COUNT(*) FROM local.fact_order_items
FORMAT Pretty
")

echo "$METRICS" | tee -a "$LOG_FILE"
echo "Refresh complete at $(date)" | tee -a "$LOG_FILE"
```

Make it executable:

```bash
chmod +x scripts/refresh_warehouse.sh
```

#### Configure Schedule with Cron

Add to your crontab to run daily at 2 AM:

```bash
# Edit crontab
crontab -e

# Add this line (adjust path as needed):
0 2 * * * cd /path/to/ecommerce-warehouse && ./scripts/refresh_warehouse.sh
```

**Common schedule patterns:**

- `0 2 * * *` - Daily at 2 AM
- `0 */4 * * *` - Every 4 hours
- `0 0 * * 0` - Weekly on Sunday at midnight
- `0 0 1 * *` - Monthly on the 1st at midnight

**For production**, use a task scheduler like:
- **cron** (Linux/macOS)
- **systemd timers** (Linux)
- **Airflow** (data pipeline orchestration)
- **Temporal** (MooseStack's built-in scheduler)

#### Test the Refresh Script

Run manually to test:

```bash
./scripts/refresh_warehouse.sh
```

Check the log output:

```bash
tail -f logs/refresh_*.log
```

You should see:

```
Starting warehouse refresh at Wed Jan 15 14:30:00 PST 2026
Loading dimensions...
Dimensions loaded successfully
Loading facts...
Facts loaded successfully
Collecting metrics...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ table_name  â”ƒ row_count â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ customers   â”‚ 5000      â”‚
â”‚ products    â”‚ 500       â”‚
â”‚ stores      â”‚ 50        â”‚
â”‚ order_items â”‚ 75000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Refresh complete at Wed Jan 15 14:31:15 PST 2026
```

#### Configure Data Quality Checks

Add data quality validations to your refresh script. Create `scripts/data_quality_checks.sh`:

```bash
#!/bin/bash
# scripts/data_quality_checks.sh
# Run data quality checks on the warehouse

CLICKHOUSE_URL="http://panda:pandapass@localhost:18123"

echo "Running data quality checks..."

# Check 1: No null customer keys in facts
NULL_CUSTOMERS=$(curl -s "$CLICKHOUSE_URL/" --data "
  SELECT COUNT(*) as cnt
  FROM local.fact_order_items
  WHERE customer_key = 0
  FORMAT TabSeparated
")

if [ "$NULL_CUSTOMERS" -eq 0 ]; then
  echo "âœ“ Check 1 passed: No null customer keys"
else
  echo "âœ— Check 1 failed: Found $NULL_CUSTOMERS null customer keys"
fi

# Check 2: Total amounts should be positive
NEGATIVE_AMOUNTS=$(curl -s "$CLICKHOUSE_URL/" --data "
  SELECT COUNT(*) as cnt
  FROM local.fact_order_items
  WHERE total_amount < 0
  FORMAT TabSeparated
")

if [ "$NEGATIVE_AMOUNTS" -eq 0 ]; then
  echo "âœ“ Check 2 passed: No negative amounts"
else
  echo "âœ— Check 2 failed: Found $NEGATIVE_AMOUNTS negative amounts"
fi

# Check 3: All dates are valid
INVALID_DATES=$(curl -s "$CLICKHOUSE_URL/" --data "
  SELECT COUNT(*) as cnt
  FROM local.fact_order_items f
  LEFT JOIN local.dim_date d ON f.order_date_key = d.date_key
  WHERE d.date_key IS NULL
  FORMAT TabSeparated
")

if [ "$INVALID_DATES" -eq 0 ]; then
  echo "âœ“ Check 3 passed: All dates are valid"
else
  echo "âœ— Check 3 failed: Found $INVALID_DATES invalid date references"
fi

echo "Data quality checks complete"
```

Run these checks as part of your refresh workflow or on a separate schedule.

#### Monitor Pipeline Health

MooseStack provides monitoring at http://localhost:8080 (Temporal UI).

Check:
- Workflow execution history
- Success/failure rates
- Execution duration trends
- Pending/running workflows

#### Review Logs

Check workflow logs:

```bash
moose logs workflow scheduled_refresh
```

Output shows execution history:

```
2024-03-15 02:00:00 | Starting scheduled warehouse refresh...
2024-03-15 02:01:23 | Dimensions loaded successfully
2024-03-15 02:02:45 | Facts loaded successfully
2024-03-15 02:02:47 | Refresh complete: { customers: 5000, ... }
```

Your warehouse is now fully automated!

---

### Congratulations!

You've built a complete data warehouse from scratch. Here's what you accomplished:

**What You Built:**

âœ… **Dimensional data model** with star schema (4 dimensions, 1 fact table)
âœ… **Multiple data sources** integrated (operational database, CSV files, sample data)
âœ… **Automated ETL pipelines** with incremental loading
âœ… **Fast analytical queries** (10-100x faster than operational database)
âœ… **Interactive dashboards** with Metabase
âœ… **Scheduled automation** with monitoring and alerting

**What Changed:**

**Before (Operational Database):**
- Analytics queries slow down production database
- Complex aggregations take 30-120 seconds
- Can't run historical trend analysis without impacting users
- Business teams wait hours for reports

**After (Data Warehouse):**
- Analytics isolated from production
- Same queries run in under 1 second
- Unlimited historical analysis
- Business teams get instant insights

**What You Can Do Now:**

- Query years of data instantly
- Build unlimited dashboards without performance impact
- Add new data sources without changing production systems
- Share data with BI tools, Python notebooks, and custom apps
- Scale to billions of rows with consistent performance

**Capabilities You Have:**

1. **Operational database extraction** with incremental loading
2. **Multiple data source consolidation** in one queryable system
3. **Columnar storage optimization** for analytical workloads
4. **Star schema modeling** for intuitive business queries
5. **Automated scheduling** with retry logic and monitoring
6. **REST APIs** for programmatic access
7. **Visualization** with industry-standard BI tools


---

## Part 3: How Warehouses Work

*This section explains the technical concepts behind what you built in Part 2.*

### OLTP vs OLAP: Two Different Workloads

**OLTP (Online Transaction Processing):**

What it does:
- Process individual transactions (orders, updates, deletes)
- Find specific records by ID or key
- Update small numbers of rows
- Return results in milliseconds

Examples:
- Customer places order (write a few rows)
- User logs in (lookup by email)
- Update inventory (find product, decrement count)
- Check order status (read by order ID)

Characteristics:
- Low latency (ms)
- High concurrency (1000s of transactions/sec)
- Small data per query (1-100 rows)
- Frequent writes

**OLAP (Online Analytical Processing):**

What it does:
- Aggregate large datasets (millions of rows)
- Calculate trends, totals, averages
- Complex joins across multiple tables
- Return summaries in seconds

Examples:
- Total sales by region last quarter (scan millions of orders)
- Top 100 customers by lifetime value (aggregate all orders)
- Month-over-month growth rate (compare time periods)
- Inventory turnover by category (calculate ratios across products)

Characteristics:
- Higher latency (seconds)
- Lower concurrency (10s of queries at once)
- Large data per query (millions of rows)
- Mostly reads, few writes

**Key difference in one sentence:** OLTP finds specific records quickly; OLAP processes millions of records to find patterns.

### Row Storage vs Column Storage

This is the fundamental difference that makes warehouses 10-100x faster for analytics.

**Row-oriented storage (OLTP databases):**

Data is stored as complete rows:

```
Record 1: [id=1, name="Alice", city="Seattle", amount=100, date="2024-03-01"]
Record 2: [id=2, name="Bob", city="Portland", amount=150, date="2024-03-02"]
Record 3: [id=3, name="Carol", city="Seattle", amount=200, date="2024-03-03"]
```

**On disk:**
```
Block 1: [Record 1 complete] [Record 2 complete] [Record 3 complete] ...
```

When you query "SELECT * FROM orders WHERE id = 2", the database:
1. Uses index to find the block containing record 2
2. Reads that block (fast! One disk seek)
3. Returns the complete record

**Perfect for OLTP** (finding individual records).

**Columnar storage (OLAP databases):**

Data is stored column-by-column:

```
IDs:     [1, 2, 3, ...]
Names:   ["Alice", "Bob", "Carol", ...]
Cities:  ["Seattle", "Portland", "Seattle", ...]
Amounts: [100, 150, 200, ...]
Dates:   ["2024-03-01", "2024-03-02", "2024-03-03", ...]
```

**On disk:**
```
Block 1: [All IDs from records 1-10000]
Block 2: [All names from records 1-10000]
Block 3: [All cities from records 1-10000]
Block 4: [All amounts from records 1-10000]
Block 5: [All dates from records 1-10000]
```

When you query "SELECT SUM(amount) FROM orders WHERE city='Seattle'", the database:
1. Reads city column (ignores ids, names, dates)
2. Reads amount column
3. Scans both in parallel
4. Sums amounts where city matches

**Perfect for OLAP** (aggregating across columns).

**Why columnar is 10-100x faster for analytics:**

1. **I/O reduction**: Only read columns you need (90% less data read from disk)
2. **Compression**: Similar values together compress better (Seattle, Seattle, Seattle = 3x references to one value)
3. **Cache efficiency**: CPU cache fits more values (contiguous memory layout)
4. **Vectorization**: Process thousands of values in one CPU instruction (SIMD)

**Example:**

Table: 1 million rows, 20 columns, each row 200 bytes = 200 MB total

Query: `SELECT AVG(amount) FROM sales WHERE region='West'`

**Row storage:**
- Read all 1M rows (200 MB)
- Extract region and amount columns
- Filter and calculate
- **I/O: 200 MB**

**Columnar storage:**
- Read region column (1M * 10 bytes = 10 MB, compressed to 2 MB)
- Read amount column (1M * 8 bytes = 8 MB, compressed to 4 MB)
- Filter and calculate
- **I/O: 6 MB**

**Improvement: 33x less I/O, proportionally faster**

**Compression benefits:**

Columnar data compresses extremely well:

- **Run-length encoding**: "Seattle" repeated 1000 times = store once + count
- **Dictionary encoding**: Map "Seattle"â†’1, "Portland"â†’2, then store integers
- **Delta encoding**: Store differences (prices: 100, 101, 99 â†’ 100, +1, -2)
- **Bit packing**: Pack values into minimal bits (0-255 needs only 8 bits)

Real-world compression ratios: 5-10x typical, 20-50x for sorted data.

**This means:**
- 1 TB of data stores in 100-200 GB
- Less disk I/O
- More data fits in memory
- Faster queries

### Star Schema: Organizing Your Warehouse

A **star schema** is the standard warehouse design. It consists of:

**Fact tables (center of the star):**
- Store measurable events (orders, clicks, transactions)
- Large (millions to billions of rows)
- Mostly numeric (quantities, amounts, counts)
- Foreign keys to dimensions

**Dimension tables (points of the star):**
- Store descriptive attributes (customers, products, dates)
- Smaller (thousands to millions of rows)
- Mostly text (names, categories, descriptions)
- Lookup tables for analysis

**ASCII diagram:**

```
              dim_customer
              (customer_key, name, city, ...)
                     |
                     |
dim_product ------ fact_orders ------ dim_date
(product_key,    (order_key,        (date_key,
 name,            customer_key,      date,
 category,        product_key,       quarter,
 ...)             date_key,          year,
                  store_key,         ...)
                  quantity,
                  amount,
                  ...)
                     |
                     |
              dim_store
              (store_key, name, region, ...)
```

**Why this design works:**

1. **Intuitive**: Business users understand "join fact to the dimensions you care about"
2. **Fast**: Dimensions are small (cached in memory), facts are large (columnar scanning)
3. **Flexible**: Add new dimensions without changing facts
4. **Performant**: Query planner has simple join tree

**Typical sizes:**

- Fact table: 100M - 10B rows, 100s of GB to TBs
- Dimension tables: 1K - 10M rows each, MBs to GBs
- Ratio: Facts are 100-1000x larger than dimensions

### Example Star Schema for E-commerce

**Fact table:** `fact_order_items`

Columns:
- `order_item_key` (primary key) - Surrogate key for this line item
- `order_id` - Business key from source system
- `line_number` - Line number within order
- `customer_key` (foreign key) - â†’ dim_customer
- `product_key` (foreign key) - â†’ dim_product
- `order_date_key` (foreign key) - â†’ dim_date
- `ship_date_key` (foreign key) - â†’ dim_date
- `store_key` (foreign key) - â†’ dim_store
- `quantity` (measure)
- `unit_price` (measure)
- `discount_amount` (measure)
- `tax_amount` (measure)
- `total_amount` (measure)
- `shipping_cost` (measure)

**Dimensions:**

`dim_customer`:
- `customer_key` (primary key)
- `customer_id` (business key)
- `email`, `first_name`, `last_name`
- `city`, `state`, `country`
- `customer_since`

`dim_product`:
- `product_key` (primary key)
- `product_id`, `sku` (business keys)
- `name`, `description`, `category`, `brand`
- `list_price`, `cost`

`dim_date`:
- `date_key` (primary key, format: YYYYMMDD)
- `date`, `year`, `quarter`, `month`, `week`
- `day_of_week`, `is_weekend`, `is_holiday`

`dim_store`:
- `store_key` (primary key)
- `store_id` (business key)
- `store_name`, `store_type`
- `city`, `state`, `region`

**Example query:**

"What were total sales by product category for each region last quarter?"

```sql
SELECT
  s.region,
  p.category,
  SUM(f.total_amount) as revenue,
  COUNT(DISTINCT f.order_id) as num_orders
FROM fact_order_items f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_store s ON f.store_key = s.store_key
JOIN dim_date d ON f.order_date_key = d.date_key
WHERE d.year = 2024 AND d.quarter = 1
GROUP BY s.region, p.category
ORDER BY revenue DESC;
```

**Why this query is fast:**

1. **Columnar scan**: Only reads region, category, total_amount columns
2. **Small dimensions**: product and store dimensions fit in memory
3. **Date filter**: Warehouse partitions fact table by date, only scans Q1 data
4. **Aggregation**: ClickHouse's vectorized execution processes thousands of rows at once
5. **Sorted data**: Fact table sorted by date_key, makes date filtering extremely fast

**Result: Sub-second query across millions of records**

### That's All You Need for Now

You now understand:

âœ… **OLTP vs OLAP** - Different workloads need different databases
âœ… **Columnar storage** - Why warehouses are 10-100x faster for analytics
âœ… **Compression** - How warehouses store 10x more data in less space
âœ… **Star schema** - How to organize warehouse data intuitively
âœ… **Performance** - Why these designs make queries so fast

**Next steps:**

Go back to Part 1 and build your warehouse if you haven't already. Now that you understand the concepts, the implementation will make more sense.

Want to build more advanced features? Check the [MooseStack documentation](https://docs.fiveonefour.com/moose) for:
- Slowly changing dimensions
- Materialized views
- Real-time streaming
- Machine learning feature stores
- Multi-tenant warehouses

**Happy building! ðŸš€**

---

## Next Steps

### Deploy to Production

Ready to deploy your warehouse to the cloud?

**Boreal** is the managed hosting platform for MooseStack. It handles:
- Infrastructure provisioning
- Scaling
- Backups
- Monitoring
- Security

Deploy with one command:

```bash
moose deploy boreal
```

Follow the prompts to create your Boreal account and provision production infrastructure.

### Expand Your Warehouse

**Add More Data Sources:**

- Connect additional operational databases (MySQL, SQL Server, etc.)
- Integrate third-party APIs (Stripe, Google Analytics, Salesforce)
- Add real-time event streaming (Kafka, Kinesis)
- Import historical archives (S3, Google Cloud Storage)

**Enhance Your Schema:**

- Add slowly changing dimensions (track historical changes)
- Create aggregate tables for common queries
- Add calculated metrics (profit margin, conversion rate)
- Implement role-playing dimensions (order date, ship date, delivery date)

**Improve Your Pipelines:**

- Set up incremental loading for all large tables
- Add Change Data Capture (CDC) for real-time sync
- Implement data quality checks and validation
- Add data lineage tracking
- Create alerting for pipeline failures

### Learn More

**Resources:**

- [MooseStack Documentation](https://docs.fiveonefour.com/moose) - Complete API reference
- [Boreal Platform Guide](https://docs.fiveonefour.com/boreal) - Deployment and operations
- [514 Registry](https://github.com/514-labs/registry) - Connector marketplace
- [Community Discord](https://discord.gg/moosestack) - Get help and share solutions

**Advanced Topics:**

- Materialized views for instant aggregations
- Consumption APIs for custom applications
- Security and access control
- Multi-tenant warehouses
- Real-time streaming analytics
- Machine learning feature stores

---


---

## Appendix: Common Issues

### Setup Issues

**Docker won't start:**

- **Mac/Windows**: Open Docker Desktop and ensure it's running
- **Linux**: Check service status with `sudo systemctl status docker`
- **Port conflicts**: If ports 18123, 9000, or 4000 are in use, configure different ports in `moose.config.toml`

**Can't connect to services:**

- Verify Docker containers are running: `docker ps`
- Check MooseStack dev server is running: Look for "Watching for changes..." in terminal
- Test ClickHouse: `curl http://localhost:18123/ping` should return `Ok.`

**Port conflicts:**

```bash
# Find process using port 4000
lsof -i :4000

# Kill process if needed
kill -9 <PID>
```

### Data Issues

**Connector fails:**

- **Database connection errors**:
  - Check credentials in `.env` file
  - Verify database is accessible from your machine
  - Test network connectivity: `psql -h localhost -U readonly_user -d ecommerce_production`
  - Check SSL settings if connection requires encryption

- **Incremental column not found**:
  - Verify `updated_at` column exists on all tables
  - If using different column name, update connector config
  - Add `updated_at` to source tables if missing:
    ```sql
    ALTER TABLE customers ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;
    CREATE TRIGGER update_customers_updated_at
      BEFORE UPDATE ON customers
      FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
    ```

**Data quality failures:**

- Check workflow logs: `moose logs workflow scheduled_refresh`
- Verify source data integrity in raw schemas
- Run data quality checks manually:
  ```sql
  SELECT COUNT(*) FROM fact_order_items WHERE customer_key IS NULL;
  SELECT COUNT(*) FROM fact_order_items WHERE total_amount < 0;
  ```

**Schema mismatches:**

- Raw data columns don't match model definitions
- Update models to match source schema, or
- Add transformation logic in workflows to map fields
- Check for type mismatches (string vs integer, etc.)

### Query Issues

**Slow queries:**

- Check if you're filtering by date - use date_key for best performance
- Verify dimensions are loaded (facts need dimension keys)
- Add indexes if querying by non-key columns frequently
- Consider materialized views for commonly-run aggregations

**No results:**

- Verify data loaded: `moose peek fact_order_items --limit 5`
- Check date ranges in WHERE clauses
- Confirm dimension keys exist: `SELECT COUNT(DISTINCT customer_key) FROM dim_customer`

**Connection errors:**

- ClickHouse client: Verify host, port, username, password
- Metabase: Ensure ClickHouse driver is installed
- Consumption APIs: Check HTTP server is running on port 4000


