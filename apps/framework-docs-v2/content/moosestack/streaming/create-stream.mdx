---
title: Create Streams
description: Define and configure Kafka/Redpanda topics that feed Moose Streams.
order: 1
category: streaming
---

import { Callout, LanguageTabs, LanguageTabContent } from "@/components/mdx";

# Declaring Streams

## Purpose

Use a `Stream` declaration to register a Kafka/Redpanda topic, specify its schema, and describe how events should be buffered before they flow into ClickHouse-backed OLAP tables. This page is a reference for the constructor, required fields, and supported options.

## Audience & prerequisites

- You are adding or updating streaming ingestion and already have a Moose project with the streaming feature flag enabled (`streaming_engine = true`).
- You understand the data model you want to publish (types, keys, retention requirements).
- You know whether the stream will sync directly to an OLAP table or act as an intermediate processing stage.

## Summary

<LanguageTabs>
  <LanguageTabContent value="typescript">

| Item | Details |
| --- | --- |
| Constructor | `new Stream<TSchema>(name, options)` |
| Typical options | `StreamOptions` such as `destination`, `retentionPeriod`, `parallelism`, `lifeCycle` |

  </LanguageTabContent>
  <LanguageTabContent value="python">

| Item | Details |
| --- | --- |
| Constructor | `Stream[Schema](name, StreamConfig(...))` |
| Typical options | `StreamConfig` keyword args such as `destination`, `retention_period`, `parallelism`, `life_cycle` |

  </LanguageTabContent>
</LanguageTabs>

Regardless of language, a Stream declaration produces a managed topic that MooseStack provisions, migrates, and exposes to APIs, transforms, or OLAP syncs.

## Syntax

<LanguageTabs>
  <LanguageTabContent value="typescript">
```ts filename="streams/user-events.ts" copy
import { Key, OlapTable, Stream } from "@514labs/moose-lib";

interface UserEvent {
  id: Key<string>;
  occurredAt: Date;
  action: string;
  payload?: Record<string, unknown>;
}

const userEventsTable = new OlapTable<UserEvent>("user_events");

export const userEventsStream = new Stream<UserEvent>("user_events", {
  destination: userEventsTable,
  retentionPeriod: 60 * 60 * 24 * 7, // 7 days (seconds)
  parallelism: 4,
});
```
  </LanguageTabContent>
  <LanguageTabContent value="python">
```py filename="streams/user_events.py" copy
from datetime import datetime
from typing import Optional, Dict, Any

from moose_lib import Key, OlapTable, Stream, StreamConfig
from pydantic import BaseModel

class UserEvent(BaseModel):
    id: Key[str]
    occurred_at: datetime
    action: str
    payload: Optional[Dict[str, Any]] = None

user_events_table = OlapTable[UserEvent]("user_events")

user_events_stream = Stream[UserEvent](
    "user_events",
    StreamConfig(
        destination=user_events_table,
        retention_period=60 * 60 * 24 * 7,
        parallelism=4,
    ),
)
```
  </LanguageTabContent>
</LanguageTabs>

## Parameters

### Constructor arguments

| Parameter | Required | Description |
| --- | --- | --- |
| `name: string` | Yes | Logical stream ID. Becomes the Kafka/Redpanda topic name and the handle used everywhere else in Moose. |
| `options: StreamOptions` | No (defaults shown below) | Controls how Moose provisions, syncs, and scales the topic. |

<LanguageTabs>
  <LanguageTabContent value="typescript">

| Option | Type | Required | Description |
| --- | --- | --- | --- |
| `destination` | `OlapTable<TSchema>` | No | When provided, Moose automatically syncs events from the stream into the specified ClickHouse table. |
| `retentionPeriod` | `number` (seconds) | No (default cluster value) | How long to keep events in the stream before purging. Useful when backfills or replays are required. |
| `parallelism` | `number` | No (default `1`) | Maximum concurrent workers Moose spins up for consumer/transform functions attached to the stream. |
| `lifeCycle` | `LifeCycle` enum | No (`LifeCycle.FULLY_MANAGED`) | Controls whether Moose can create, update, or delete the underlying topic. Use `EXTERNALLY_MANAGED` when the topic already exists. |

  </LanguageTabContent>
  <LanguageTabContent value="python">

| Option | Type | Required | Description |
| --- | --- | --- | --- |
| `destination` | `OlapTable[Schema]` | No | When provided, Moose automatically syncs events from the stream into the specified ClickHouse table. |
| `retention_period` | `int` (seconds) | No (default cluster value) | How long to keep events in the stream before purging. Useful when backfills or replays are required. |
| `parallelism` | `int` | No (default `1`) | Maximum concurrent workers Moose spins up for consumer/transform functions attached to the stream. |
| `life_cycle` | `LifeCycle` enum | No (`LifeCycle.FULLY_MANAGED`) | Controls whether Moose can create, update, or delete the underlying topic. Use `EXTERNALLY_MANAGED` when the topic already exists. |

  </LanguageTabContent>
</LanguageTabs>

<Callout type="info" title="Schema & type safety">
Stream schemas are inferred directly from the TypeScript interface or Python `BaseModel`. Changing the schema means Moose will reconcile the topic configuration and update downstream tables as part of the migration workflow.
</Callout>

## Behavior

- A `Stream` declaration is evaluated during `moose plan`/`moose apply`, so updates follow the same migration workflow as tables or APIs.
- Moose provisions the Kafka/Redpanda topic (unless `lifeCycle` disables management) and registers schema information so other components can rely on it.
- When `destination` is set, Moose continuously batches and writes records into the ClickHouse table with column alignment and batching tuned for OLAP workloads.
- Streams can act as sources or destinations for transform functions; the same configuration applies regardless of whether the stream receives data from an Ingest API, another stream, or an external producer.

## Examples

### 1. Stream fed by an Ingest API

<LanguageTabs>
  <LanguageTabContent value="typescript">
```ts filename="pipelines/session-events.ts" copy
import { IngestApi, OlapTable, Stream } from "@514labs/moose-lib";

interface SessionEvent {
  id: string;
  sessionId: string;
  occurredAt: Date;
}

const sessionEventsTable = new OlapTable<SessionEvent>("session_events");
const sessionEventsStream = new Stream<SessionEvent>("session_events", {
  destination: sessionEventsTable,
});

export const sessionEventsApi = new IngestApi<SessionEvent>("session_events", {
  destination: sessionEventsStream,
});
```
  </LanguageTabContent>
  <LanguageTabContent value="python">
```py filename="pipelines/session_events.py" copy
from datetime import datetime
from moose_lib import IngestApi, IngestConfig, OlapTable, Stream, StreamConfig
from pydantic import BaseModel

class SessionEvent(BaseModel):
    id: str
    session_id: str
    occurred_at: datetime

session_events_table = OlapTable[SessionEvent]("session_events")
session_events_stream = Stream[SessionEvent](
    "session_events",
    StreamConfig(destination=session_events_table),
)

session_events_api = IngestApi[SessionEvent](
    "session_events",
    IngestConfig(destination=session_events_stream),
)
```
  </LanguageTabContent>
</LanguageTabs>

### 2. Intermediate transformation stream

<LanguageTabs>
  <LanguageTabContent value="typescript">
```ts filename="streams/derived-events.ts" copy
import { Stream } from "@514labs/moose-lib";

interface RawEvent {
  id: string;
  payload: Record<string, unknown>;
}

interface DerivedEvent {
  id: string;
  flattenedPayload: string;
}

export const rawStream = new Stream<RawEvent>("raw_events");
export const derivedStream = new Stream<DerivedEvent>("derived_events");

rawStream.addTransform(derivedStream, (record) => ({
  id: record.id,
  flattenedPayload: JSON.stringify(record.payload),
}));
```
  </LanguageTabContent>
  <LanguageTabContent value="python">
```py filename="streams/derived_events.py" copy
import json

from moose_lib import Stream, StreamConfig
from pydantic import BaseModel

class RawEvent(BaseModel):
    id: str
    payload: dict

class DerivedEvent(BaseModel):
    id: str
    flattened_payload: str

raw_stream = Stream[RawEvent]("raw_events")
derived_stream = Stream[DerivedEvent](
    "derived_events",
    StreamConfig(parallelism=2),
)

raw_stream.add_transform(
    destination=derived_stream,
    transformation=lambda record: DerivedEvent(
        id=record.id,
        flattened_payload=json.dumps(record.payload),
    ),
)
```
  </LanguageTabContent>
</LanguageTabs>

## Related tasks

- [Publish data from your code](/moosestack/streaming/from-your-code)
- [Add consumer functions](/moosestack/streaming/consumer-functions)
- [Sync streams to OLAP tables](/moosestack/streaming/sync-to-table)
- [Run transformations between streams](/moosestack/streaming/transform-functions)
